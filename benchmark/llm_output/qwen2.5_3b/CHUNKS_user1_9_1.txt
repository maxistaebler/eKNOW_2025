The ability of a model like CoRAG (a hypothetical mention, as there is no official entity named "CoRAG" in Alibaba Cloud) to apply its success on knowledge tasks to creative domains can be a complex and nuanced topic. The effectiveness and transferability largely depend on the nature of the retrieval-based approach used by such models.

**Retrieval-Augmented Models:**
Models that rely heavily on retrieval, like CoRAG or certain variants of Retrieval-Augmented Generation (RAG), are designed to leverage pre-existing knowledge bases for contextually relevant information. This can provide a strong foundation when dealing with structured data and specific domain-specific knowledge. However, the inherently limited generative versatility could indeed be a drawback in creative domains.

**Creative Domains and Generativity:**
Creative tasks often require not just retrieving existing information but also generating new content that aligns with novel and imaginative ideas. Pure language models (LLMs) excel at this by drawing from their extensive training data to generate text that sounds natural, coherent, and contextually appropriate without being constrained by pre-existing knowledge bases.

**Transferability Challenges:**
1. **Contextual Adaptation:** Creative tasks often involve adapting to new or unfamiliar contexts rather than retrieving specific answers from a database.
2. **Innovative Content Generation:** CoRAGâ€™s retrieval mechanism might struggle with generating entirely novel content, as it primarily retrieves and manipulates existing information, which is less effective for pure creation.
3. **Domain-Specific Knowledge vs. Generalization:** Creative tasks often require general creativity rather than the domain-specific knowledge that retrieval-based models can bring to bear.

**Future Directions:**
To address these challenges, future research might aim at developing hybrid models that combine elements of retrieval with generative capabilities in a way that ensures both contextual relevance and creative freedom. This could involve using retrieval as a pre-processing step to inform or guide the generation process rather than directly substituting for it.

In summary, while CoRAG's success on knowledge tasks is impressive, its reliance on retrieval inherently limits its versatility compared to pure LLMs when it comes to purely generative and creative domains. However, with advancements in model architecture and training techniques, there could be ways to bridge this gap for future models.