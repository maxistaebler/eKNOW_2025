{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_output_structure() -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate a dictionary mapping folders in llm_output to their contained file paths.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary with folder names as keys and lists of file paths as values\n",
    "    \"\"\"\n",
    "    # Get the llm_output directory path\n",
    "    llm_output_dir = Path(\"../benchmark/llm_output\")\n",
    "    \n",
    "    # Initialize result dictionary\n",
    "    output_structure: Dict[str, List[str]] = {}\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for folder in llm_output_dir.iterdir():\n",
    "        if folder.is_dir():\n",
    "            # Get all files in the folder\n",
    "            files = [\n",
    "                str(file.relative_to(llm_output_dir))\n",
    "                for file in folder.glob(\"*\")\n",
    "                if file.is_file()\n",
    "            ]\n",
    "            # Add to dictionary with folder name as key\n",
    "            output_structure[folder.name] = sorted(files)\n",
    "            \n",
    "    return output_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "qwen2.5_7b:\n",
      "  - qwen2.5_7b/CHUNKS_admin_0_1.txt\n",
      "  - qwen2.5_7b/CHUNKS_admin_1_1.txt\n",
      "  - qwen2.5_7b/CHUNKS_admin_2_1.txt\n",
      "  - qwen2.5_7b/CHUNKS_admin_3_1.txt\n",
      "  - qwen2.5_7b/CHUNKS_admin_4_1.txt\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "llm_output_structure = get_llm_output_structure()\n",
    "for folder, files in llm_output_structure.items():\n",
    "    print(f\"\\n{folder}:\")\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f\"  - {file}\")\n",
    "    if len(files) > 5:\n",
    "        print(\"  ...\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of qwen2.5_7b/CHUNKS_admin_0_1.txt:\n",
      "----------------------------------------\n",
      "CoRAG (Contextual Rank-and-Generate) fundamentally differs from traditional Retrieval-Augmented Generation (RAG) architectures by addressing key limitations related to multi-hop reasoning. Specifically, CoRAG introduces a contextual ranking mechanism that enhances the handling of complex queries requiring multiple pieces of information.\n",
      "\n",
      "In traditional RAG architectures, the retrieval process often relies on keyword-based matching, which can struggle with queries that require linking distant or less explicit connections between documents. This is where CoRAG steps in by incorporating a more sophisticated ranking approach. According to the data provided, \"RankRAG is a method that unifies context ranki[ing] and generation\" [Data: Relationships (18)]. Through this mechanism, CoRAG can better identify relevant contexts across different documents, thereby facilitating multi-hop reasoning.\n",
      "\n",
      "Moreover, traditional RAG systems might face limitations in terms of retrieving the right information when dealing with sparse reward signals or when the necessary context is distributed sparsely within a large document collection. CoRAG addresses these issues by enhancing its retrieval capabilities to more effectively locate and integrate multiple sources of information needed for complex queries.\n",
      "\n",
      "For instance, traditional RAG systems might have difficulty in situations where the relevant information spans across several documents and requires combining different pieces of knowledge from those documents to provide a coherent answer. CoRAG's approach is designed to mitigate such limitations by providing a more nuanced understanding of the context necessary for accurate retrieval and generation [Data: Sources (0), Relationships (18)].\n",
      "\n",
      "In summary, CoRAG fundamentally differs from traditional RAG architectures by employing a contextual ranking mechanism that improves multi-hop reasoning capabilities. This enhancement addresses specific retrieval limitations faced by traditional systems, such as sparse reward signals and distributed information across documents, leading to more effective handling of complex queries.\n"
     ]
    }
   ],
   "source": [
    "def read_llm_response(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Read the contents of an LLM response file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Relative path to the file within llm_output directory\n",
    "        \n",
    "    Returns:\n",
    "        str: Contents of the file\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist\n",
    "    \"\"\"\n",
    "    # Construct full path\n",
    "    full_path = Path(\"../benchmark/llm_output\") / file_path\n",
    "    \n",
    "    try:\n",
    "        return full_path.read_text(encoding='utf-8')\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Response file not found: {full_path}\")\n",
    "\n",
    "# Get first file from first model folder as example\n",
    "first_model = next(iter(llm_output_structure))\n",
    "first_file = llm_output_structure[first_model][0]\n",
    "\n",
    "# Read and print contents\n",
    "print(f\"Contents of {first_file}:\")\n",
    "print(\"-\" * 40)\n",
    "print(read_llm_response(first_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_questions = []\n",
    "n_models = []\n",
    "n_texts = []\n",
    "n_access_levels = []\n",
    "n_response_types = []\n",
    "n_paths = []\n",
    "for k,v in llm_output_structure.items():\n",
    "    for text in v:\n",
    "        qid = text.split('/')[-1].split('_')[-2]\n",
    "        response_type = '_'.join(text.split('/')[-1].split('_')[:-3])\n",
    "        access_level = text.split('/')[-1].split('_')[-3:-2][0]\n",
    "        full_path = Path(\"../benchmark/llm_output\") / text\n",
    "        try:\n",
    "            txt = full_path.read_text(encoding='utf-8')\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Response file not found: {full_path}\")\n",
    "\n",
    "        n_questions.append(qid)\n",
    "        n_models.append(k)\n",
    "        n_texts.append(txt)\n",
    "        n_access_levels.append(access_level)\n",
    "        n_response_types.append(response_type)\n",
    "        n_paths.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model</th>\n",
       "      <th>access_level</th>\n",
       "      <th>response_type</th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>admin</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_7b/CHUNKS_admin_0_1.txt</td>\n",
       "      <td>CoRAG (Contextual Rank-and-Generate) fundament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>admin</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_7b/CHUNKS_admin_1_1.txt</td>\n",
       "      <td>Automated rejection sampling for generating in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>admin</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_7b/CHUNKS_admin_2_1.txt</td>\n",
       "      <td>The inconsistent performance across different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>admin</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_7b/CHUNKS_admin_3_1.txt</td>\n",
       "      <td>The performance discrepancy between CoRAG and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>admin</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_7b/CHUNKS_admin_4_1.txt</td>\n",
       "      <td>Based on the provided data, there are several ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_id       model access_level response_type  \\\n",
       "0           0  qwen2.5_7b        admin        CHUNKS   \n",
       "1           1  qwen2.5_7b        admin        CHUNKS   \n",
       "2           2  qwen2.5_7b        admin        CHUNKS   \n",
       "3           3  qwen2.5_7b        admin        CHUNKS   \n",
       "4           4  qwen2.5_7b        admin        CHUNKS   \n",
       "\n",
       "                              path  \\\n",
       "0  qwen2.5_7b/CHUNKS_admin_0_1.txt   \n",
       "1  qwen2.5_7b/CHUNKS_admin_1_1.txt   \n",
       "2  qwen2.5_7b/CHUNKS_admin_2_1.txt   \n",
       "3  qwen2.5_7b/CHUNKS_admin_3_1.txt   \n",
       "4  qwen2.5_7b/CHUNKS_admin_4_1.txt   \n",
       "\n",
       "                                                text  \n",
       "0  CoRAG (Contextual Rank-and-Generate) fundament...  \n",
       "1  Automated rejection sampling for generating in...  \n",
       "2  The inconsistent performance across different ...  \n",
       "3  The performance discrepancy between CoRAG and ...  \n",
       "4  Based on the provided data, there are several ...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([n_questions, n_models, n_access_levels, n_response_types, n_paths, n_texts]).T\n",
    "df.columns = ['question_id', 'model', 'access_level', 'response_type', 'path', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "import openai\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "class ResponseEvaluation(BaseModel):\n",
    "    \"\"\"Model for a single response evaluation.\"\"\"\n",
    "    model_name: str = Field(..., description=\"Name of the LLM model\")\n",
    "    rank: int = Field(..., description=\"Rank of the response (1 being best)\")\n",
    "    score: float = Field(..., ge=0, le=10, description=\"Score out of 10\")\n",
    "    reasoning: str = Field(..., description=\"Detailed explanation for the ranking\")\n",
    "\n",
    "class QuestionEvaluation(BaseModel):\n",
    "    \"\"\"Model for evaluating all responses to a single question.\"\"\"\n",
    "    question_id: str = Field(..., description=\"ID of the question (e.g., 'admin_1')\")\n",
    "    access_level: str = Field(..., description=\"Access level (admin/user)\")\n",
    "    question_type: str = Field(..., description=\"Type of response (CHUNKS/DOCUMENT_LEVEL/etc)\")\n",
    "    evaluations: List[ResponseEvaluation] = Field(..., description=\"List of response evaluations\")\n",
    "    meta_analysis: str = Field(..., description=\"Overall analysis of all responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate_responses_for_question(\n",
    "    question_id: str,\n",
    "    access_level: str,\n",
    "    response_type: str,\n",
    "    responses: Dict[str, str]\n",
    ") -> QuestionEvaluation:\n",
    "    \"\"\"\n",
    "    Evaluate all responses for a single question using GPT-4.\n",
    "    \n",
    "    Args:\n",
    "        question_id: Question identifier\n",
    "        access_level: Access level (admin/user)\n",
    "        response_type: Type of response (CHUNKS/DOCUMENT_LEVEL/etc)\n",
    "        responses: Dictionary mapping model names to their responses\n",
    "        \n",
    "    Returns:\n",
    "        QuestionEvaluation: Structured evaluation of all responses\n",
    "    \"\"\"\n",
    "    # Construct prompt for GPT-4\n",
    "    prompt = f\"\"\"You are an expert evaluator of LLM responses. Please analyze and rank the following responses to the same question.\n",
    "    Question ID: {question_id}\n",
    "    Access Level: {access_level}\n",
    "    Response Type: {response_type}\n",
    "\n",
    "    Responses to evaluate:\n",
    "    {'-' * 50}\n",
    "    \"\"\"\n",
    "    \n",
    "    for model_name, response in responses.items():\n",
    "        prompt += f\"\\n{model_name}:\\n{response}\\n{'-' * 50}\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\\nPlease evaluate each response based on:\n",
    "    1. Accuracy and factual correctness\n",
    "    2. Completeness of the answer\n",
    "    3. Clarity and coherence\n",
    "    4. Relevance to the question\n",
    "    5. Proper use of available context\n",
    "\n",
    "    Provide a ranking from best to worst, with scores (0-10) and detailed explanations.\n",
    "    Format your response in a structured way that can be parsed into the following JSON schema:\n",
    "    {\n",
    "        \"evaluations\": [\n",
    "            {\n",
    "                \"model_name\": \"model name\",\n",
    "                \"rank\": rank number,\n",
    "                \"score\": score (0-10),\n",
    "                \"reasoning\": \"detailed explanation\"\n",
    "            }\n",
    "        ],\n",
    "        \"meta_analysis\": \"overall analysis of patterns and differences between responses\"\n",
    "    }\"\"\"\n",
    "\n",
    "    # Get evaluation from GPT-4\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator of LLM responses.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" }\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    evaluation_data = response.choices[0].message.content\n",
    "    \n",
    "    # Create QuestionEvaluation object\n",
    "    # return QuestionEvaluation(\n",
    "    #     question_id=question_id,\n",
    "    #     access_level=access_level,\n",
    "    #     question_type=response_type,\n",
    "    #     **evaluation_data\n",
    "    # )\n",
    "    return evaluation_data\n",
    "\n",
    "def evaluate_all_questions(q_n_a: pd.DataFrame) -> List[QuestionEvaluation]:\n",
    "    \"\"\"\n",
    "    Evaluate all questions across different models and response types.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_df: DataFrame containing response embeddings and metadata\n",
    "        \n",
    "    Returns:\n",
    "        List[QuestionEvaluation]: List of evaluations for all questions\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    results = {}\n",
    "    \n",
    "    # Group responses by question ID, access level, and response type\n",
    "    # embeddings_df['question_id'] = embeddings_df['filename'].apply(\n",
    "    #     lambda x: '_'.join(x.split('_')[1:3])\n",
    "    # )\n",
    "    # embeddings_df['access_level'] = embeddings_df['filename'].apply(\n",
    "    #     lambda x: x.split('_')[1]\n",
    "    # )\n",
    "    # embeddings_df['response_type'] = embeddings_df['filename'].apply(\n",
    "    #     lambda x: x.split('_')[0]\n",
    "    # )\n",
    "\n",
    "    # ROWs: 'question_id', 'model', 'access_level', 'response_type', 'text'\n",
    "    \n",
    "    # Process each unique combination\n",
    "    for (qid, access, rtype), group in df.groupby(['question_id', 'access_level', 'response_type']):\n",
    "        # Get responses for each model\n",
    "        responses = {}\n",
    "        for _, row in group.iterrows():\n",
    "            # response_text = read_llm_response(f\"{row['model']}/{row['path']}\")\n",
    "            responses[row['model']] = row['text']\n",
    "            \n",
    "        # Get evaluation\n",
    "        # print(responses)\n",
    "        # print(row)\n",
    "        evaluation = evaluate_responses_for_question(qid, access, rtype, responses)\n",
    "        try:\n",
    "            eval_dict = json.loads(evaluation)\n",
    "            # evaluations_dicts.append(eval_dict)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "        \n",
    "        eval_dict['params'] = {'qid': qid, 'access': access, 'rtype': rtype}\n",
    "        results[qid] = eval_dict\n",
    "        # evaluations.append(evaluation)\n",
    "        \n",
    "        # Save individual evaluation\n",
    "        # output_path = Path(f\"../benchmark/output/evaluations/{qid}_{rtype}.json\")\n",
    "        # output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # output_path.write_text(evaluation.model_dump_json(indent=2))\n",
    "    \n",
    "    return results # evaluations\n",
    "\n",
    "# Run evaluations\n",
    "evaluations = evaluate_all_questions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>This response provides a comprehensive and wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>The response accurately discusses the differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>While this response discusses important aspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>This response includes a thoughtful reflection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>The analysis presented is somewhat vague and l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model_name  rank  score  \\\n",
       "0  mistral-small_24b     1     10   \n",
       "1         qwen2.5_7b     2      9   \n",
       "2         qwen2.5_3b     3      8   \n",
       "3   deepseek-r1_1.5b     4      7   \n",
       "4    deepseek-r1_32b     5      6   \n",
       "\n",
       "                                           reasoning  \n",
       "0  This response provides a comprehensive and wel...  \n",
       "1  The response accurately discusses the differen...  \n",
       "2  While this response discusses important aspect...  \n",
       "3  This response includes a thoughtful reflection...  \n",
       "4  The analysis presented is somewhat vague and l...  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(evaluations['0']['evaluations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The evaluation reveals that the top responses excel in providing a structured, accurate, and detailed exploration of how CoRAG differs from traditional RAG architectures. Clearly outlining key differences and limitations with suitable examples enhances their clarity and relevance. Lower-ranked responses, while still addressing the topic, often lack the depth of explanation or structured coherence needed to fully satisfy the prompt.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations['0']['meta_analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': '0', 'access': 'admin', 'rtype': 'CHUNKS'}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations['0']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary by model:\n",
      "                  score             rank\n",
      "                   mean   std count mean\n",
      "model_name                              \n",
      "deepseek-r1_1.5b    5.4  0.84    10  4.3\n",
      "deepseek-r1_32b     6.2  1.55    10  3.7\n",
      "mistral-small_24b   9.1  0.57    10  1.1\n",
      "qwen2.5_3b          5.8  1.23    10  3.9\n",
      "qwen2.5_7b          8.0  0.67    10  2.0\n"
     ]
    }
   ],
   "source": [
    "def merge_evaluation_dataframes(evaluations: Dict, num_questions: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge evaluation DataFrames from multiple questions into one DataFrame with question IDs.\n",
    "    \n",
    "    Args:\n",
    "        evaluations: Dictionary containing evaluation results\n",
    "        num_questions: Number of questions to process\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with question IDs\n",
    "    \"\"\"\n",
    "    # List to store individual DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Process each question's evaluations\n",
    "    for i in range(num_questions):\n",
    "        try:\n",
    "            # Get the evaluations for this question\n",
    "            question_eval = evaluations[str(i)]\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(question_eval['evaluations'])\n",
    "            \n",
    "            # Add question metadata\n",
    "            df['question_id'] = i\n",
    "            df['access_level'] = question_eval['params']['access']\n",
    "            df['response_type'] = question_eval['params']['rtype']\n",
    "            \n",
    "            # Add to list\n",
    "            dfs.append(df)\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: No evaluations found for question {i}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns to put metadata first\n",
    "    column_order = ['question_id', 'access_level', 'response_type', 'model_name', 'rank', 'score', 'reasoning']\n",
    "    combined_df = combined_df[column_order]\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Create combined DataFrame\n",
    "combined_evaluations = merge_evaluation_dataframes(evaluations)\n",
    "\n",
    "# Display sample and summary\n",
    "# print(\"\\nSample of combined evaluations:\")\n",
    "# print(combined_evaluations.head())\n",
    "\n",
    "print(\"\\nSummary by model:\")\n",
    "summary = combined_evaluations.groupby('model_name').agg({\n",
    "    'score': ['mean', 'std', 'count'],\n",
    "    'rank': 'mean'\n",
    "}).round(2)\n",
    "print(summary)\n",
    "\n",
    "# Save to CSV\n",
    "combined_evaluations.to_csv('./results/combined_evaluations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>access_level</th>\n",
       "      <th>response_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>user1</td>\n",
       "      <td>KG_ONLY</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>This response is the most accurate and complet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>user1</td>\n",
       "      <td>KG_ONLY</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>This response provides a good overview of CoRA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>user1</td>\n",
       "      <td>KG_ONLY</td>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>This response effectively covers the differenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>user1</td>\n",
       "      <td>KG_ONLY</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>This response attempts to explain the differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>user1</td>\n",
       "      <td>KG_ONLY</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>This response raises several valid points rega...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id access_level response_type         model_name  rank  score  \\\n",
       "0            0        user1       KG_ONLY  mistral-small_24b     1     10   \n",
       "1            0        user1       KG_ONLY         qwen2.5_7b     2      8   \n",
       "2            0        user1       KG_ONLY         qwen2.5_3b     3      7   \n",
       "3            0        user1       KG_ONLY   deepseek-r1_1.5b     4      6   \n",
       "4            0        user1       KG_ONLY    deepseek-r1_32b     5      5   \n",
       "\n",
       "                                           reasoning  \n",
       "0  This response is the most accurate and complet...  \n",
       "1  This response provides a good overview of CoRA...  \n",
       "2  This response effectively covers the differenc...  \n",
       "3  This response attempts to explain the differen...  \n",
       "4  This response raises several valid points rega...  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_evaluations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Shape: (400, 386)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>filename</th>\n",
       "      <th>dim_0</th>\n",
       "      <th>dim_1</th>\n",
       "      <th>dim_2</th>\n",
       "      <th>dim_3</th>\n",
       "      <th>dim_4</th>\n",
       "      <th>dim_5</th>\n",
       "      <th>dim_6</th>\n",
       "      <th>dim_7</th>\n",
       "      <th>...</th>\n",
       "      <th>dim_374</th>\n",
       "      <th>dim_375</th>\n",
       "      <th>dim_376</th>\n",
       "      <th>dim_377</th>\n",
       "      <th>dim_378</th>\n",
       "      <th>dim_379</th>\n",
       "      <th>dim_380</th>\n",
       "      <th>dim_381</th>\n",
       "      <th>dim_382</th>\n",
       "      <th>dim_383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>CHUNKS_admin_0_1.txt</td>\n",
       "      <td>-0.075566</td>\n",
       "      <td>0.119131</td>\n",
       "      <td>-0.036106</td>\n",
       "      <td>-0.095838</td>\n",
       "      <td>-0.049041</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>-0.127176</td>\n",
       "      <td>0.092244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113079</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.111015</td>\n",
       "      <td>-0.082428</td>\n",
       "      <td>0.148435</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>-0.129441</td>\n",
       "      <td>0.107625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>CHUNKS_admin_1_1.txt</td>\n",
       "      <td>-0.008544</td>\n",
       "      <td>-0.101055</td>\n",
       "      <td>-0.130910</td>\n",
       "      <td>-0.054039</td>\n",
       "      <td>0.110466</td>\n",
       "      <td>-0.068998</td>\n",
       "      <td>-0.068796</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184230</td>\n",
       "      <td>0.196043</td>\n",
       "      <td>0.064503</td>\n",
       "      <td>-0.083653</td>\n",
       "      <td>-0.010190</td>\n",
       "      <td>0.024736</td>\n",
       "      <td>0.163483</td>\n",
       "      <td>0.089139</td>\n",
       "      <td>-0.094404</td>\n",
       "      <td>-0.092834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>CHUNKS_admin_2_1.txt</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>-0.199228</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>0.219409</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>-0.117323</td>\n",
       "      <td>-0.105174</td>\n",
       "      <td>0.076461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116590</td>\n",
       "      <td>0.017078</td>\n",
       "      <td>-0.158439</td>\n",
       "      <td>-0.036856</td>\n",
       "      <td>-0.027145</td>\n",
       "      <td>0.094577</td>\n",
       "      <td>-0.118003</td>\n",
       "      <td>-0.053782</td>\n",
       "      <td>-0.151496</td>\n",
       "      <td>0.009665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>CHUNKS_admin_3_1.txt</td>\n",
       "      <td>0.070269</td>\n",
       "      <td>0.026260</td>\n",
       "      <td>-0.095843</td>\n",
       "      <td>0.056919</td>\n",
       "      <td>0.190134</td>\n",
       "      <td>-0.027714</td>\n",
       "      <td>-0.125104</td>\n",
       "      <td>0.045174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>0.170429</td>\n",
       "      <td>0.162762</td>\n",
       "      <td>-0.050768</td>\n",
       "      <td>-0.128579</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>-0.079618</td>\n",
       "      <td>0.123970</td>\n",
       "      <td>-0.122808</td>\n",
       "      <td>-0.058790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>CHUNKS_admin_4_1.txt</td>\n",
       "      <td>0.073119</td>\n",
       "      <td>-0.056663</td>\n",
       "      <td>-0.111264</td>\n",
       "      <td>-0.087591</td>\n",
       "      <td>0.004742</td>\n",
       "      <td>-0.318034</td>\n",
       "      <td>-0.320232</td>\n",
       "      <td>-0.027044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121301</td>\n",
       "      <td>0.161043</td>\n",
       "      <td>-0.056585</td>\n",
       "      <td>0.054711</td>\n",
       "      <td>0.090903</td>\n",
       "      <td>0.069179</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>-0.135994</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>-0.005580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        model              filename     dim_0     dim_1     dim_2     dim_3  \\\n",
       "0  qwen2.5_7b  CHUNKS_admin_0_1.txt -0.075566  0.119131 -0.036106 -0.095838   \n",
       "1  qwen2.5_7b  CHUNKS_admin_1_1.txt -0.008544 -0.101055 -0.130910 -0.054039   \n",
       "2  qwen2.5_7b  CHUNKS_admin_2_1.txt  0.025271 -0.199228  0.005019  0.219409   \n",
       "3  qwen2.5_7b  CHUNKS_admin_3_1.txt  0.070269  0.026260 -0.095843  0.056919   \n",
       "4  qwen2.5_7b  CHUNKS_admin_4_1.txt  0.073119 -0.056663 -0.111264 -0.087591   \n",
       "\n",
       "      dim_4     dim_5     dim_6     dim_7  ...   dim_374   dim_375   dim_376  \\\n",
       "0 -0.049041  0.003664 -0.127176  0.092244  ... -0.113079  0.008026  0.111015   \n",
       "1  0.110466 -0.068998 -0.068796  0.039880  ...  0.184230  0.196043  0.064503   \n",
       "2  0.082700 -0.117323 -0.105174  0.076461  ...  0.116590  0.017078 -0.158439   \n",
       "3  0.190134 -0.027714 -0.125104  0.045174  ...  0.095337  0.170429  0.162762   \n",
       "4  0.004742 -0.318034 -0.320232 -0.027044  ...  0.121301  0.161043 -0.056585   \n",
       "\n",
       "    dim_377   dim_378   dim_379   dim_380   dim_381   dim_382   dim_383  \n",
       "0 -0.082428  0.148435 -0.000946  0.015064  0.055559 -0.129441  0.107625  \n",
       "1 -0.083653 -0.010190  0.024736  0.163483  0.089139 -0.094404 -0.092834  \n",
       "2 -0.036856 -0.027145  0.094577 -0.118003 -0.053782 -0.151496  0.009665  \n",
       "3 -0.050768 -0.128579  0.012660 -0.079618  0.123970 -0.122808 -0.058790  \n",
       "4  0.054711  0.090903  0.069179  0.044614 -0.135994  0.010154 -0.005580  \n",
       "\n",
       "[5 rows x 386 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "\n",
    "def get_embeddings(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get embeddings for text using ollama's all-minilm model.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to embed\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Embedding vector\n",
    "    \"\"\"\n",
    "    response = ollama.embeddings(\n",
    "        model='all-minilm:33m',\n",
    "        prompt=text\n",
    "    )\n",
    "    return response['embedding']\n",
    "\n",
    "def create_embeddings_dataframe() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing embeddings for all LLM responses along with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns for model name, filename, and embedding values\n",
    "    \"\"\"\n",
    "    # Get file structure\n",
    "    output_structure = get_llm_output_structure()\n",
    "    \n",
    "    # Lists to store data\n",
    "    data = []\n",
    "    \n",
    "    # Process each file\n",
    "    for model_name, files in output_structure.items():\n",
    "        for file_name in files:\n",
    "            try:\n",
    "                # Read content\n",
    "                content = read_llm_response(file_name)\n",
    "                \n",
    "                # Get embeddings\n",
    "                embedding = get_embeddings(content)\n",
    "                \n",
    "                # Store data\n",
    "                data.append({\n",
    "                    'model': model_name,\n",
    "                    'filename': file_name.split('/')[-1],  # Just the filename without path\n",
    "                    'embedding': embedding\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {str(e)}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Expand embedding column into separate columns\n",
    "    embedding_df = pd.DataFrame(df['embedding'].tolist(), \n",
    "                              columns=[f'dim_{i}' for i in range(len(df['embedding'].iloc[0]))])\n",
    "    \n",
    "    # Combine metadata with embeddings\n",
    "    result_df = pd.concat([\n",
    "        df[['model', 'filename']],\n",
    "        embedding_df\n",
    "    ], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Create and display the DataFrame\n",
    "embeddings_df = create_embeddings_dataframe()\n",
    "print(\"\\nDataFrame Shape:\", embeddings_df.shape)\n",
    "# print(\"\\nFirst few rows:\")\n",
    "# print(embeddings_df.head())\n",
    "\n",
    "# Optionally save to parquet for later use\n",
    "embeddings_df.to_parquet('./results/response_embeddings.parquet')\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_reduced_dim_plots(df: pd.DataFrame, method: str = 'pca', n_components: int = 2) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create dimensionality reduction plots using either PCA or t-SNE.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing embeddings and metadata\n",
    "        method: Reduction method ('pca' or 'tsne')\n",
    "        n_components: Number of components for reduction\n",
    "        \n",
    "    Returns:\n",
    "        go.Figure: Interactive plotly figure\n",
    "    \"\"\"\n",
    "    # Get embedding columns\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('dim_')]\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        reduced_data = reducer.fit_transform(df[embedding_cols])\n",
    "        explained_var = reducer.explained_variance_ratio_\n",
    "        title = f'PCA Visualization (Explained variance: {sum(explained_var):.2%})'\n",
    "    else:  # t-SNE\n",
    "        reducer = TSNE(n_components=n_components, random_state=42)\n",
    "        reduced_data = reducer.fit_transform(df[embedding_cols])\n",
    "        title = 'T-SNE Visualization'\n",
    "    \n",
    "    # Create DataFrame with reduced dimensions\n",
    "    plot_df = pd.DataFrame(\n",
    "        reduced_data,\n",
    "        columns=[f'Component_{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Add metadata\n",
    "    plot_df['model'] = df['model']\n",
    "    plot_df['filename'] = df['filename']\n",
    "    \n",
    "    # Extract type from filename (CHUNKS, DOCUMENT_LEVEL, etc.)\n",
    "    plot_df['response_type'] = plot_df['filename'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    # Create hover text\n",
    "    plot_df['hover_text'] = plot_df.apply(\n",
    "        lambda row: f\"Model: {row['model']}<br>File: {row['filename']}<br>Type: {row['response_type']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig = px.scatter(\n",
    "        plot_df,\n",
    "        x='Component_1',\n",
    "        y='Component_2',\n",
    "        color='model',\n",
    "        symbol='response_type',\n",
    "        hover_data=['filename'],\n",
    "        title=title,\n",
    "        labels={\n",
    "            'Component_1': f'{method.upper()} Component 1',\n",
    "            'Component_2': f'{method.upper()} Component 2'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        template='plotly_white',\n",
    "        legend=dict(\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create both visualizations\n",
    "pca_fig = create_reduced_dim_plots(embeddings_df, method='pca')\n",
    "tsne_fig = create_reduced_dim_plots(embeddings_df, method='tsne')\n",
    "\n",
    "# Display figures\n",
    "# pca_fig.show()\n",
    "# tsne_fig.show()\n",
    "\n",
    "# Optionally save the figures\n",
    "pca_fig.write_html(\"./results/pca_visualization.html\")\n",
    "tsne_fig.write_html(\"./results/tsne_visualization.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response depended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reduced_dim_plots_by_type(df: pd.DataFrame, method: str = 'pca', n_components: int = 2) -> Dict[str, go.Figure]:\n",
    "    \"\"\"\n",
    "    Create separate dimensionality reduction plots for each response type.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing embeddings and metadata\n",
    "        method: Reduction method ('pca' or 'tsne')\n",
    "        n_components: Number of components for reduction\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, go.Figure]: Dictionary mapping response types to their figures\n",
    "    \"\"\"\n",
    "    # Get embedding columns\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('dim_')]\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "        reduced_data = reducer.fit_transform(df[embedding_cols])\n",
    "        explained_var = reducer.explained_variance_ratio_\n",
    "        title_prefix = f'PCA Visualization (Explained variance: {sum(explained_var):.2%})'\n",
    "    else:  # t-SNE\n",
    "        reducer = TSNE(n_components=n_components, random_state=42)\n",
    "        reduced_data = reducer.fit_transform(df[embedding_cols])\n",
    "        title_prefix = 'T-SNE Visualization'\n",
    "    \n",
    "    # Create DataFrame with reduced dimensions\n",
    "    plot_df = pd.DataFrame(\n",
    "        reduced_data,\n",
    "        columns=[f'Component_{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Add metadata\n",
    "    plot_df['model'] = df['model']\n",
    "    plot_df['filename'] = df['filename']\n",
    "    plot_df['response_type'] = df['filename'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    # Create hover text\n",
    "    plot_df['hover_text'] = plot_df.apply(\n",
    "        lambda row: f\"Model: {row['model']}<br>File: {row['filename']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create separate figures for each response type\n",
    "    figures = {}\n",
    "    for response_type in plot_df['response_type'].unique():\n",
    "        type_df = plot_df[plot_df['response_type'] == response_type]\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            type_df,\n",
    "            x='Component_1',\n",
    "            y='Component_2',\n",
    "            color='model',\n",
    "            hover_data=['filename'],\n",
    "            title=f\"{title_prefix} - {response_type}\",\n",
    "            labels={\n",
    "                'Component_1': f'{method.upper()} Component 1',\n",
    "                'Component_2': f'{method.upper()} Component 2'\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=600,\n",
    "            width=800,\n",
    "            template='plotly_white',\n",
    "            legend=dict(\n",
    "                yanchor=\"top\",\n",
    "                y=0.99,\n",
    "                xanchor=\"left\",\n",
    "                x=0.01\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        figures[response_type] = fig\n",
    "    \n",
    "    return figures\n",
    "\n",
    "# Create visualizations for both PCA and t-SNE\n",
    "pca_figs = create_reduced_dim_plots_by_type(embeddings_df, method='pca')\n",
    "tsne_figs = create_reduced_dim_plots_by_type(embeddings_df, method='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all figures\n",
    "# print(\"\\nPCA Visualizations:\")\n",
    "# for response_type, fig in pca_figs.items():\n",
    "#     print(f\"\\n{response_type}:\")\n",
    "#     fig.show()\n",
    "    \n",
    "# print(\"\\nT-SNE Visualizations:\")\n",
    "# for response_type, fig in tsne_figs.items():\n",
    "#     print(f\"\\n{response_type}:\")\n",
    "#     fig.show()\n",
    "\n",
    "# Save all figures\n",
    "for response_type, fig in pca_figs.items():\n",
    "    fig.write_html(f\"./results/pca_visualization_{response_type.lower()}.html\")\n",
    "    \n",
    "for response_type, fig in tsne_figs.items():\n",
    "    fig.write_html(f\"./results/tsne_visualization_{response_type.lower()}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Matrix Shape: (400, 400)\n",
      "\n",
      "Top 20 Most Similar Pairs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response1</th>\n",
       "      <th>response2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77918</th>\n",
       "      <td>deepseek-r1_32b - CHUNKS_user1_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b - DOCUMENT_LEVEL_admin_8_1.txt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77938</th>\n",
       "      <td>deepseek-r1_32b - CHUNKS_user1_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b - FULL_admin_8_1.txt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77958</th>\n",
       "      <td>deepseek-r1_32b - CHUNKS_user1_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b - KG_ONLY_admin_8_1.txt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78493</th>\n",
       "      <td>deepseek-r1_32b - DOCUMENT_LEVEL_admin_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b - FULL_admin_8_1.txt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78513</th>\n",
       "      <td>deepseek-r1_32b - DOCUMENT_LEVEL_admin_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b - KG_ONLY_admin_8_1.txt</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response1  \\\n",
       "77918          deepseek-r1_32b - CHUNKS_user1_8_1.txt   \n",
       "77938          deepseek-r1_32b - CHUNKS_user1_8_1.txt   \n",
       "77958          deepseek-r1_32b - CHUNKS_user1_8_1.txt   \n",
       "78493  deepseek-r1_32b - DOCUMENT_LEVEL_admin_8_1.txt   \n",
       "78513  deepseek-r1_32b - DOCUMENT_LEVEL_admin_8_1.txt   \n",
       "\n",
       "                                            response2  similarity  \n",
       "77918  deepseek-r1_32b - DOCUMENT_LEVEL_admin_8_1.txt         1.0  \n",
       "77938            deepseek-r1_32b - FULL_admin_8_1.txt         1.0  \n",
       "77958         deepseek-r1_32b - KG_ONLY_admin_8_1.txt         1.0  \n",
       "78493            deepseek-r1_32b - FULL_admin_8_1.txt         1.0  \n",
       "78513         deepseek-r1_32b - KG_ONLY_admin_8_1.txt         1.0  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def create_similarity_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing cosine similarities between all embeddings.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing embeddings and metadata\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with cosine similarities and metadata\n",
    "    \"\"\"\n",
    "    # Get embedding columns\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('dim_')]\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    similarities = cosine_similarity(df[embedding_cols])\n",
    "    \n",
    "    # Create identifiers for each row\n",
    "    identifiers = df.apply(\n",
    "        lambda row: f\"{row['model']} - {row['filename']}\", \n",
    "        axis=1\n",
    "    ).values\n",
    "    \n",
    "    # Create similarity DataFrame with identifiers\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarities,\n",
    "        index=identifiers,\n",
    "        columns=identifiers\n",
    "    )\n",
    "    \n",
    "    return similarity_df\n",
    "\n",
    "# Create similarity matrix\n",
    "similarity_df = create_similarity_matrix(embeddings_df)\n",
    "\n",
    "# Display sample of the matrix\n",
    "print(\"\\nSimilarity Matrix Shape:\", similarity_df.shape)\n",
    "# print(\"\\nSample of Similarity Matrix:\")\n",
    "# print(similarity_df.iloc[:5, :5])\n",
    "\n",
    "# Save to parquet\n",
    "similarity_df.to_parquet('./results/response_similarities.parquet')\n",
    "\n",
    "# Optional: Find most similar pairs (excluding self-similarity)\n",
    "def get_top_similarities(sim_df: pd.DataFrame, n: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the top N most similar pairs of responses.\n",
    "    \n",
    "    Args:\n",
    "        sim_df: Similarity matrix DataFrame\n",
    "        n: Number of top pairs to return\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with top similar pairs\n",
    "    \"\"\"\n",
    "    # Create pairs DataFrame\n",
    "    pairs = []\n",
    "    for i in range(len(sim_df)):\n",
    "        for j in range(i + 1, len(sim_df)):  # Only upper triangle to avoid duplicates\n",
    "            pairs.append({\n",
    "                'response1': sim_df.index[i],\n",
    "                'response2': sim_df.index[j],\n",
    "                'similarity': sim_df.iloc[i, j]\n",
    "            })\n",
    "    \n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    return pairs_df.nlargest(n, 'similarity')\n",
    "\n",
    "# Get top similar pairs\n",
    "top_pairs = get_top_similarities(similarity_df, n=20)\n",
    "print(\"\\nTop 20 Most Similar Pairs:\")\n",
    "top_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of transformed DataFrame: (79800, 5)\n",
      "\n",
      "Sample of highest similarity pairs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_1_model</th>\n",
       "      <th>response_1_filename</th>\n",
       "      <th>response_2_model</th>\n",
       "      <th>response_2_filename</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77958</th>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>CHUNKS_user1_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>KG_ONLY_admin_8_1.txt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77938</th>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>CHUNKS_user1_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>FULL_admin_8_1.txt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78493</th>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>FULL_admin_8_1.txt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79323</th>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>FULL_admin_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>KG_ONLY_admin_8_1.txt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77918</th>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>CHUNKS_user1_8_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_8_1.txt</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36304</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_4_1.txt</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_9_1.txt</td>\n",
       "      <td>-0.104718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44689</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>FULL_user1_4_1.txt</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_9_1.txt</td>\n",
       "      <td>-0.105722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71369</th>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_9_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>DOCUMENT_LEVEL_user1_4_1.txt</td>\n",
       "      <td>-0.111688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71349</th>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_9_1.txt</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>CHUNKS_user1_4_1.txt</td>\n",
       "      <td>-0.113390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41994</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>FULL_admin_4_1.txt</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>DOCUMENT_LEVEL_admin_9_1.txt</td>\n",
       "      <td>-0.118050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        response_1_model           response_1_filename  response_2_model  \\\n",
       "77958    deepseek-r1_32b          CHUNKS_user1_8_1.txt   deepseek-r1_32b   \n",
       "77938    deepseek-r1_32b          CHUNKS_user1_8_1.txt   deepseek-r1_32b   \n",
       "78493    deepseek-r1_32b  DOCUMENT_LEVEL_admin_8_1.txt   deepseek-r1_32b   \n",
       "79323    deepseek-r1_32b            FULL_admin_8_1.txt   deepseek-r1_32b   \n",
       "77918    deepseek-r1_32b          CHUNKS_user1_8_1.txt   deepseek-r1_32b   \n",
       "...                  ...                           ...               ...   \n",
       "36304  mistral-small_24b  DOCUMENT_LEVEL_admin_4_1.txt  deepseek-r1_1.5b   \n",
       "44689  mistral-small_24b            FULL_user1_4_1.txt  deepseek-r1_1.5b   \n",
       "71369   deepseek-r1_1.5b  DOCUMENT_LEVEL_admin_9_1.txt   deepseek-r1_32b   \n",
       "71349   deepseek-r1_1.5b  DOCUMENT_LEVEL_admin_9_1.txt   deepseek-r1_32b   \n",
       "41994  mistral-small_24b            FULL_admin_4_1.txt  deepseek-r1_1.5b   \n",
       "\n",
       "                response_2_filename  similarity  \n",
       "77958         KG_ONLY_admin_8_1.txt    1.000000  \n",
       "77938            FULL_admin_8_1.txt    1.000000  \n",
       "78493            FULL_admin_8_1.txt    1.000000  \n",
       "79323         KG_ONLY_admin_8_1.txt    1.000000  \n",
       "77918  DOCUMENT_LEVEL_admin_8_1.txt    1.000000  \n",
       "...                             ...         ...  \n",
       "36304  DOCUMENT_LEVEL_admin_9_1.txt   -0.104718  \n",
       "44689  DOCUMENT_LEVEL_admin_9_1.txt   -0.105722  \n",
       "71369  DOCUMENT_LEVEL_user1_4_1.txt   -0.111688  \n",
       "71349          CHUNKS_user1_4_1.txt   -0.113390  \n",
       "41994  DOCUMENT_LEVEL_admin_9_1.txt   -0.118050  \n",
       "\n",
       "[79800 rows x 5 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_similarity_pairs_df(similarity_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform similarity matrix into a long-format DataFrame with paired responses.\n",
    "    \n",
    "    Args:\n",
    "        similarity_df: DataFrame containing similarity matrix\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Long-format DataFrame with columns for response pairs and their similarity\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the data\n",
    "    pairs = []\n",
    "    \n",
    "    # Iterate through upper triangle of matrix (excluding diagonal)\n",
    "    for i in range(len(similarity_df)):\n",
    "        for j in range(i + 1, len(similarity_df)):\n",
    "            # Get response identifiers\n",
    "            response1 = similarity_df.index[i]\n",
    "            response2 = similarity_df.columns[j]\n",
    "            \n",
    "            # Split identifiers into model and filename\n",
    "            response1_model, response1_filename = response1.split(\" - \", 1)\n",
    "            response2_model, response2_filename = response2.split(\" - \", 1)\n",
    "            \n",
    "            # Get similarity score\n",
    "            similarity = similarity_df.iloc[i, j]\n",
    "            \n",
    "            # Add to pairs list\n",
    "            pairs.append({\n",
    "                'response_1_model': response1_model,\n",
    "                'response_1_filename': response1_filename,\n",
    "                'response_2_model': response2_model,\n",
    "                'response_2_filename': response2_filename,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from pairs\n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    \n",
    "    # Sort by similarity in descending order\n",
    "    pairs_df = pairs_df.sort_values('similarity', ascending=False)\n",
    "    \n",
    "    return pairs_df\n",
    "\n",
    "# Create the transformed DataFrame\n",
    "similarity_pairs_df = create_similarity_pairs_df(similarity_df)\n",
    "\n",
    "# Display sample and save\n",
    "print(\"\\nShape of transformed DataFrame:\", similarity_pairs_df.shape)\n",
    "print(\"\\nSample of highest similarity pairs:\")\n",
    "# print(similarity_pairs_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "similarity_pairs_df.to_csv('./results/response_similarity_pairs.csv', index=False)\n",
    "similarity_pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Model Consistency Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Self-Consistency:\n",
      "                     mean     std\n",
      "response_1_model                 \n",
      "deepseek-r1_1.5b   0.3150  0.1793\n",
      "deepseek-r1_32b    0.3404  0.2116\n",
      "mistral-small_24b  0.3323  0.2014\n",
      "qwen2.5_3b         0.3372  0.1766\n",
      "qwen2.5_7b         0.3594  0.1810\n",
      "\n",
      "Cross-Model Agreement:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_1_model</th>\n",
       "      <th>response_2_model</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.3207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>0.3314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.3213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.3329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>0.3424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>0.3516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    response_1_model   response_2_model  similarity\n",
       "0   deepseek-r1_1.5b    deepseek-r1_32b      0.3237\n",
       "1  mistral-small_24b   deepseek-r1_1.5b      0.3207\n",
       "2  mistral-small_24b    deepseek-r1_32b      0.3354\n",
       "3  mistral-small_24b         qwen2.5_3b      0.3314\n",
       "4         qwen2.5_3b   deepseek-r1_1.5b      0.3213\n",
       "5         qwen2.5_3b    deepseek-r1_32b      0.3321\n",
       "6         qwen2.5_7b   deepseek-r1_1.5b      0.3329\n",
       "7         qwen2.5_7b    deepseek-r1_32b      0.3440\n",
       "8         qwen2.5_7b  mistral-small_24b      0.3424\n",
       "9         qwen2.5_7b         qwen2.5_3b      0.3516"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_model_consistency(similarity_pairs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze how consistent each model's responses are with itself and other models.\n",
    "    \n",
    "    Args:\n",
    "        similarity_pairs_df: DataFrame with similarity pairs\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Statistics about model consistency\n",
    "    \"\"\"\n",
    "    # Self-consistency (how similar are responses from the same model)\n",
    "    self_consistency = similarity_pairs_df[\n",
    "        similarity_pairs_df['response_1_model'] == similarity_pairs_df['response_2_model']\n",
    "    ].groupby('response_1_model')['similarity'].agg(['mean', 'std']).round(4)\n",
    "    \n",
    "    # Cross-model agreement\n",
    "    cross_model = similarity_pairs_df[\n",
    "        similarity_pairs_df['response_1_model'] != similarity_pairs_df['response_2_model']\n",
    "    ].groupby(['response_1_model', 'response_2_model'])['similarity'].mean().round(4)\n",
    "    \n",
    "    return self_consistency, cross_model\n",
    "\n",
    "# Get consistency metrics\n",
    "self_consistency, cross_model = analyze_model_consistency(similarity_pairs_df)\n",
    "print(\"\\nModel Self-Consistency:\")\n",
    "print(self_consistency)\n",
    "print(\"\\nCross-Model Agreement:\")\n",
    "cross_model.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Type Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response Type Analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_1_type</th>\n",
       "      <th>response_2_type</th>\n",
       "      <th>response_1_model</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.3347</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3286</td>\n",
       "      <td>0.1938</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>0.3438</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>0.3424</td>\n",
       "      <td>0.1817</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>CHUNKS</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>0.3507</td>\n",
       "      <td>0.1831</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>KG</td>\n",
       "      <td>KG</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>0.1896</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>KG</td>\n",
       "      <td>KG</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>KG</td>\n",
       "      <td>KG</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>0.3385</td>\n",
       "      <td>0.1820</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>KG</td>\n",
       "      <td>KG</td>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>0.3204</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>KG</td>\n",
       "      <td>KG</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>0.3586</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   response_1_type response_2_type   response_1_model    mean     std  count\n",
       "0           CHUNKS          CHUNKS   deepseek-r1_1.5b  0.3347  0.1890    590\n",
       "1           CHUNKS          CHUNKS    deepseek-r1_32b  0.3286  0.1938    190\n",
       "2           CHUNKS          CHUNKS  mistral-small_24b  0.3438  0.1876   1390\n",
       "3           CHUNKS          CHUNKS         qwen2.5_3b  0.3424  0.1817    990\n",
       "4           CHUNKS          CHUNKS         qwen2.5_7b  0.3507  0.1831   1790\n",
       "..             ...             ...                ...     ...     ...    ...\n",
       "69              KG              KG   deepseek-r1_1.5b  0.3106  0.1896    590\n",
       "70              KG              KG    deepseek-r1_32b  0.3039  0.2045    190\n",
       "71              KG              KG  mistral-small_24b  0.3385  0.1820   1390\n",
       "72              KG              KG         qwen2.5_3b  0.3204  0.1673    990\n",
       "73              KG              KG         qwen2.5_7b  0.3586  0.1784   1790\n",
       "\n",
       "[74 rows x 6 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_response_types(similarity_pairs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze how different response types (CHUNKS, DOCUMENT_LEVEL, etc.) compare.\n",
    "    \n",
    "    Args:\n",
    "        similarity_pairs_df: DataFrame with similarity pairs\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Statistics about response type consistency\n",
    "    \"\"\"\n",
    "    # Extract response types\n",
    "    similarity_pairs_df['response_1_type'] = similarity_pairs_df['response_1_filename'].apply(\n",
    "        lambda x: x.split('_')[0]\n",
    "    )\n",
    "    similarity_pairs_df['response_2_type'] = similarity_pairs_df['response_2_filename'].apply(\n",
    "        lambda x: x.split('_')[0]\n",
    "    )\n",
    "    \n",
    "    # Analyze consistency within and between response types\n",
    "    type_analysis = similarity_pairs_df.groupby(\n",
    "        ['response_1_type', 'response_2_type', 'response_1_model']\n",
    "    )['similarity'].agg(['mean', 'std', 'count']).round(4)\n",
    "    \n",
    "    return type_analysis\n",
    "\n",
    "# Get response type analysis\n",
    "type_analysis = analyze_response_types(similarity_pairs_df)\n",
    "print(\"\\nResponse Type Analysis:\")\n",
    "type_analysis.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Level Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question-Level Analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_1</th>\n",
       "      <th>response_1_model</th>\n",
       "      <th>response_2_model</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEVEL_admin</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LEVEL_admin</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEVEL_admin</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEVEL_admin</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.3051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LEVEL_admin</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.3368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>user1_9</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>deepseek-r1_1.5b</td>\n",
       "      <td>0.8276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>user1_9</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>deepseek-r1_32b</td>\n",
       "      <td>0.8747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>user1_9</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>mistral-small_24b</td>\n",
       "      <td>0.8329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>user1_9</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>qwen2.5_3b</td>\n",
       "      <td>0.7939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>user1_9</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>qwen2.5_7b</td>\n",
       "      <td>0.8642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      question_1   response_1_model   response_2_model  similarity\n",
       "0    LEVEL_admin   deepseek-r1_1.5b   deepseek-r1_1.5b      0.2607\n",
       "1    LEVEL_admin   deepseek-r1_1.5b    deepseek-r1_32b      0.3230\n",
       "2    LEVEL_admin    deepseek-r1_32b    deepseek-r1_32b      0.3077\n",
       "3    LEVEL_admin  mistral-small_24b   deepseek-r1_1.5b      0.3051\n",
       "4    LEVEL_admin  mistral-small_24b    deepseek-r1_32b      0.3368\n",
       "..           ...                ...                ...         ...\n",
       "355      user1_9         qwen2.5_7b   deepseek-r1_1.5b      0.8276\n",
       "356      user1_9         qwen2.5_7b    deepseek-r1_32b      0.8747\n",
       "357      user1_9         qwen2.5_7b  mistral-small_24b      0.8329\n",
       "358      user1_9         qwen2.5_7b         qwen2.5_3b      0.7939\n",
       "359      user1_9         qwen2.5_7b         qwen2.5_7b      0.8642\n",
       "\n",
       "[360 rows x 4 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_question_responses(similarity_pairs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare how different models answered the same questions.\n",
    "    \n",
    "    Args:\n",
    "        similarity_pairs_df: DataFrame with similarity pairs\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Question-level analysis\n",
    "    \"\"\"\n",
    "    # Extract question numbers\n",
    "    similarity_pairs_df['question_1'] = similarity_pairs_df['response_1_filename'].apply(\n",
    "        lambda x: '_'.join(x.split('_')[1:3])  # Gets e.g., 'admin_1'\n",
    "    )\n",
    "    similarity_pairs_df['question_2'] = similarity_pairs_df['response_2_filename'].apply(\n",
    "        lambda x: '_'.join(x.split('_')[1:3])\n",
    "    )\n",
    "    \n",
    "    # Analyze same-question responses across models\n",
    "    question_analysis = similarity_pairs_df[\n",
    "        similarity_pairs_df['question_1'] == similarity_pairs_df['question_2']\n",
    "    ].groupby(['question_1', 'response_1_model', 'response_2_model'])['similarity'].mean().round(4)\n",
    "    \n",
    "    return question_analysis\n",
    "\n",
    "# Get question-level analysis\n",
    "question_analysis = analyze_question_responses(similarity_pairs_df)\n",
    "print(\"\\nQuestion-Level Analysis:\")\n",
    "question_analysis.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_comparisons(similarity_pairs_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create heatmap visualizations of model similarities.\n",
    "    \"\"\"\n",
    "    # Create model similarity matrix\n",
    "    model_similarities = similarity_pairs_df.groupby(\n",
    "        ['response_1_model', 'response_2_model']\n",
    "    )['similarity'].mean().unstack()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        model_similarities, \n",
    "        annot=True, \n",
    "        cmap='magma',\n",
    "        center=0,\n",
    "        fmt='.3f'\n",
    "    )\n",
    "    plt.title('Average Response Similarity Between Models')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./results/model_similarities_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "# Create visualization\n",
    "plot_model_comparisons(similarity_pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
