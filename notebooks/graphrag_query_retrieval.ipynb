{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    \"How does CoRAG fundamentally differ from traditional RAG architectures in handling multi-hop reasoning, and what specific retrieval limitations does it address?\",\n",
    "    \"Could the reliance on automated rejection sampling for intermediate chain generation introduce confirmation bias, and how does this compare to human-annotated reasoning chains?\",\n",
    "    \"Given the 10+ point EM score improvements on HotpotQA/2WikiMultihopQA, what explains the inconsistent performance on Bamboogle where Search-o1-32B outperforms CoRAG?\",\n",
    "    \"Why does CoRAG underperform on FEVER compared to Atlas-11B despite superior results elsewhere? Does this suggest task-specific limitations in fact verification?\",\n",
    "    \"The paper claims a log-linear relationship between token consumption and performance - what safeguards prevent exponential compute costs in real-world deployments?\",\n",
    "    \"If CoRAG's gains persist across retriever qualities (E5-base vs E5-large), does this imply the framework compensates for weak retrievers rather than synergizing with strong ones?\",\n",
    "    \"Why does iterative rejection sampling improve 2WikiMultihopQA but degrade other datasets? Does this indicate diminishing returns in chain quality?\",\n",
    "    \"The early stopping mechanism reduces tokens by 38% but lowers EM scores - how would you dynamically optimize this trade-off for production systems?\",\n",
    "    \"Given the increased compute requirements (128k tokens for best performance), what environmental impact assessments were conducted for large-scale deployment?\",\n",
    "    \"Could CoRAG's success on knowledge tasks transfer to creative domains, or does its reliance on retrieval inherently limit generative versatility compared to pure LLMs?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphRAG Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.embedding import OpenAIEmbedding\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.question_gen.local_gen import LocalQuestionGen\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../ragtest/output/\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "ENTITY_TABLE = \"create_final_nodes\"\n",
    "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "RELATIONSHIP_TABLE = \"create_final_relationships\"\n",
    "COVARIATE_TABLE = \"create_final_covariates\"\n",
    "TEXT_UNIT_TABLE = \"create_final_text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity count: 1653\n"
     ]
    }
   ],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)\n",
    "\n",
    "# load description embeddings to an in-memory lancedb vectorstore\n",
    "# to connect to a remote db, specify url and port values.\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "print(f\"Entity count: {len(entity_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship count: 919\n"
     ]
    }
   ],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)\n",
    "\n",
    "print(f\"Relationship count: {len(relationship_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report records: 131\n"
     ]
    }
   ],
   "source": [
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
    "\n",
    "print(f\"Report records: {len(report_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text unit records: 53\n"
     ]
    }
   ],
   "source": [
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "print(f\"Text unit records: {len(text_unit_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = \"gpt-4o-mini\" # os.environ[\"GRAPHRAG_LLM_MODEL\"]\n",
    "embedding_model = 'text-embedding-3-small' # os.environ[\"GRAPHRAG_EMBEDDING_MODEL\"]\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    # api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "text_embedder = OpenAIEmbedding(\n",
    "    # api_key=api_key,\n",
    "    api_base=None,\n",
    "    api_type=OpenaiApiType.OpenAI,\n",
    "    model=embedding_model,\n",
    "    deployment_name=embedding_model,\n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    # if you did not run covariates during indexing, set this to None\n",
    "    covariates=None,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_unit_prop: proportion of context window dedicated to related text units\n",
    "# community_prop: proportion of context window dedicated to community reports.\n",
    "# The remaining proportion is dedicated to entities and relationships. Sum of text_unit_prop and community_prop should be <= 1\n",
    "# conversation_history_max_turns: maximum number of turns to include in the conversation history.\n",
    "# conversation_history_user_turns_only: if True, only include user queries in the conversation history.\n",
    "# top_k_mapped_entities: number of related entities to retrieve from the entity description embedding store.\n",
    "# top_k_relationships: control the number of out-of-network relationships to pull into the context window.\n",
    "# include_entity_rank: if True, include the entity rank in the entity table in the context window. Default entity rank = node degree.\n",
    "# include_relationship_weight: if True, include the relationship weight in the context window.\n",
    "# include_community_rank: if True, include the community rank in the context window.\n",
    "# return_candidate_context: if True, return a set of dataframes containing all candidate entity/relationship/covariate records that\n",
    "# could be relevant. Note that not all of these records will be included in the context window. The \"in_context\" column in these\n",
    "# dataframes indicates whether the record is included in the context window.\n",
    "# max_tokens: maximum number of tokens to use for the context window.\n",
    "\n",
    "\n",
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,\n",
    "    \"top_k_relationships\": 10,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
    "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "}\n",
    "\n",
    "llm_params = {\n",
    "    \"max_tokens\": 2_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500)\n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = LocalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    llm_params=llm_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await search_engine.asearch(QUESTIONS[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73\n",
      "f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3\n",
      "f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3\n"
     ]
    }
   ],
   "source": [
    "chunks_document_mapping = {}\n",
    "chunks_document_mapping_r = {}\n",
    "entity_document_mapping = {}\n",
    "entity_document_mapping_r = {}\n",
    "relationship_document_mapping = {}\n",
    "relationship_document_mapping_r = {}\n",
    "\n",
    "entities_df = pd.read_parquet('../ragtest/output/create_final_entities.parquet')\n",
    "entity_mapper = {}\n",
    "for idx, row in entity_df.iterrows():\n",
    "    entity_mapper[row['id']] = row['human_readable_id']\n",
    "\n",
    "relationship_mapper = {}\n",
    "for idx, row in relationship_df.iterrows():\n",
    "    relationship_mapper[row['id']] = row['human_readable_id']\n",
    "\n",
    "\n",
    "for idx, row in text_unit_df.iterrows():\n",
    "    chunks_document_mapping[row['human_readable_id']] = row['document_ids'][0]\n",
    "    for e in row['entity_ids']:\n",
    "        entity_document_mapping[entity_mapper[e]] = row['document_ids'][0]\n",
    "    for r in row['relationship_ids']:\n",
    "        relationship_document_mapping[relationship_mapper[r]] = row['document_ids'][0]\n",
    "\n",
    "\n",
    "print(chunks_document_mapping[1])\n",
    "print(entity_document_mapping[641])\n",
    "print(relationship_document_mapping[638])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_access_mapping():\n",
    "    \"\"\"Create a mock document access mapping with fictional user and document IDs.\"\"\"\n",
    "    return {\n",
    "        \"a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73\": [\"user1\", \"admin\"],\n",
    "        \"f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3\": [\"admin\"],\n",
    "    }\n",
    "\n",
    "def filter_response_by_access(response_dict, access_level, user_id=None):\n",
    "    \"\"\"\n",
    "    Filter the response dictionary based on access level and user permissions.\n",
    "    \n",
    "    Args:\n",
    "        response_dict (dict): Original response containing all information\n",
    "        access_level (str): One of 'KG_ONLY', 'CHUNKS', 'FULL', 'DOCUMENT_LEVEL'\n",
    "        user_id (str): Required for DOCUMENT_LEVEL access checking\n",
    "    \n",
    "    Returns:\n",
    "        dict: Filtered response based on access level\n",
    "    \"\"\"\n",
    "    if access_level == \"KG_ONLY\":\n",
    "        return {\n",
    "            \"relationships\": response_dict[\"relationships\"],\n",
    "            \"entities\": response_dict[\"entities\"]\n",
    "        }\n",
    "    \n",
    "    elif access_level == \"CHUNKS\":\n",
    "        return {\n",
    "            \"relationships\": response_dict[\"relationships\"],\n",
    "            \"entities\": response_dict[\"entities\"],\n",
    "            \"sources\": response_dict[\"sources\"]\n",
    "        }\n",
    "    \n",
    "    elif access_level == \"FULL\":\n",
    "        return response_dict\n",
    "    \n",
    "    elif access_level == \"DOCUMENT_LEVEL\":\n",
    "        if not user_id:\n",
    "            raise ValueError(\"user_id is required for DOCUMENT_LEVEL access\")\n",
    "        \n",
    "        doc_access = create_document_access_mapping()\n",
    "        accessible_docs = {doc_id for doc_id, users in doc_access.items() \n",
    "                         if user_id in users}\n",
    "        print(accessible_docs)\n",
    "        \n",
    "        filtered_response = {}\n",
    "        \n",
    "        # Filter entities\n",
    "        if \"entities\" in response_dict:\n",
    "            filtered_response[\"entities\"] = [\n",
    "                entity for _, entity in response_dict[\"entities\"].iterrows()\n",
    "                if entity_document_mapping.get(int(entity[\"id\"])) in accessible_docs\n",
    "            ]\n",
    "        \n",
    "        # Filter relationships\n",
    "        if \"relationships\" in response_dict:\n",
    "            filtered_response[\"relationships\"] = [\n",
    "                rel for _, rel in response_dict[\"relationships\"].iterrows()\n",
    "                if relationship_document_mapping.get(int(rel[\"id\"])) in accessible_docs\n",
    "            ]\n",
    "        \n",
    "        # Filter sources\n",
    "        if \"sources\" in response_dict:\n",
    "            filtered_response[\"sources\"] = [\n",
    "                source for _, source in response_dict[\"sources\"].iterrows()\n",
    "                if chunks_document_mapping.get(int(source[\"id\"])) in accessible_docs\n",
    "            ]\n",
    "        \n",
    "        # Include reports if present and user has access\n",
    "        if \"reports\" in response_dict:\n",
    "            filtered_response[\"reports\"] = response_dict[\"reports\"]\n",
    "        \n",
    "        return filtered_response\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid access level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example calls:\n",
    "kg_only_result = filter_response_by_access(result.context_data, \"KG_ONLY\")\n",
    "chunks_result = filter_response_by_access(result.context_data, \"CHUNKS\")\n",
    "full_result = filter_response_by_access(result.context_data, \"FULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73'}\n",
      "User user1:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6 entries, 4 to 18\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   id                       6 non-null      object\n",
      " 1   entity                   6 non-null      object\n",
      " 2   description              6 non-null      object\n",
      " 3   number of relationships  6 non-null      object\n",
      " 4   in_context               6 non-null      bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 246.0+ bytes\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20 entries, 3 to 56\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           20 non-null     object\n",
      " 1   source       20 non-null     object\n",
      " 2   target       20 non-null     object\n",
      " 3   description  20 non-null     object\n",
      " 4   weight       20 non-null     object\n",
      " 5   links        20 non-null     object\n",
      " 6   in_context   20 non-null     bool  \n",
      "dtypes: bool(1), object(6)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, 1 to 1\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      1 non-null      object\n",
      " 1   text    1 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 24.0+ bytes\n",
      "None\n",
      "\n",
      "##########\n",
      "{'a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73', 'f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3'}\n",
      "User admin:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20 entries, 0 to 19\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   id                       20 non-null     object\n",
      " 1   entity                   20 non-null     object\n",
      " 2   description              20 non-null     object\n",
      " 3   number of relationships  20 non-null     object\n",
      " 4   in_context               20 non-null     bool  \n",
      "dtypes: bool(1), object(4)\n",
      "memory usage: 820.0+ bytes\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 73 entries, 0 to 72\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           73 non-null     object\n",
      " 1   source       73 non-null     object\n",
      " 2   target       73 non-null     object\n",
      " 3   description  73 non-null     object\n",
      " 4   weight       73 non-null     object\n",
      " 5   links        73 non-null     object\n",
      " 6   in_context   73 non-null     bool  \n",
      "dtypes: bool(1), object(6)\n",
      "memory usage: 4.1+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4 entries, 0 to 3\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      4 non-null      object\n",
      " 1   text    4 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 96.0+ bytes\n",
      "None\n",
      "\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "for u in [\"user1\", \"admin\"]:\n",
    "    document_level_result = filter_response_by_access(result.context_data, \"DOCUMENT_LEVEL\", user_id=u)\n",
    "    print(f\"User {u}:\")\n",
    "    print(pd.DataFrame(document_level_result['entities']).info())\n",
    "    print(pd.DataFrame(document_level_result['relationships']).info())\n",
    "    print(pd.DataFrame(document_level_result['sources']).info())\n",
    "    print()\n",
    "    print('#'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "def generate_search_prompt(context_data: str, response_type: str = \"multiple paragraphs\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt using the local search system prompt template\n",
    "    \n",
    "    Args:\n",
    "        context_data: The context data to include in the prompt\n",
    "        response_type: The desired response format/length\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted prompt\n",
    "    \"\"\"\n",
    "    # Read template file\n",
    "    template_path = Path(\"../ragtest/prompts/local_search_system_prompt.txt\")\n",
    "    with open(template_path, \"r\") as f:\n",
    "        template = f.read()\n",
    "    \n",
    "    # Replace variables\n",
    "    prompt = template.replace(\"{response_type}\", response_type)\n",
    "    prompt = prompt.replace(\"{context_data}\", context_data)\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_response(model, prompt, question):\n",
    "    # Use Ollama to test the prompt\n",
    "    response = ollama.chat(\n",
    "        model=model,  # or your preferred model\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }, {\n",
    "            'role': 'user',\n",
    "            'content': question\n",
    "        }]\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: deepseek-r1:latest - Access: KG_ONLY - User: user1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out whether CoRAG's success on knowledge tasks can be transferred to creative domains. The user is asking if the techniques used in CoRAG for knowledge tasks are applicable elsewhere, maybe even in areas like creativity.\n",
      "\n",
      "First, let me recall what CoRAG does. From the data provided, it seems that CoRAG stands for something, maybe an acronym? Looking at the 'entities' table, I see mentions of RAG (Retrieval-Augmented Generation), LLMs (Large Language Models), and other techniques like knowledge engines and factual knowledge. So CoRAG is likely a method that enhances generation by using retrieval techniques to augment text.\n",
      "\n",
      "Now, in the 'relationships' table, there are records linking CoRAG, RAG, LongRAG, and others with various concepts like retrieval-augmented generation (RAG), multi-task learning, and knowledge bases. It seems CoRAG is a specific approach within these broader categories.\n",
      "\n",
      "The user wants to know if this method can be applied beyond knowledge-intensive tasks into creative domains. Creative fields might require more than just factual recall; they often need originality, style, or innovation, which are not typically outputs of LLMs when relying solely on retrieval.\n",
      "\n",
      "Looking at the data, CoRAG's success is tied to how well it can retrieve and augment information for generation. However, creative tasks like writing poetry or generating art might require different inputs—like specific artistic styles or unique ideas that aren't just retrieved from a knowledge base.\n",
      "\n",
      "In the relationships table, there are mentions of models like LLaMA 2 which show creativity but don't use retrieval in the same way CoRAG does. So maybe mixing retrieval with pure generation could blend both aspects?\n",
      "\n",
      "But wait, is this feasible? Retrieving information and then generating creative content might lead to a smoother flow because the model can first gather relevant data and then transform it creatively. However, the user's question points out that RAG's reliance on retrieval inherently limits generative versatility compared to pure LLMs.\n",
      "\n",
      "Looking at the 'points' table in the relationships section, record 12 mentions that KILT is a benchmark for knowledge-intensive tasks but doesn't directly talk about creativity. Record 5 says LongRAG posits that RAG performance can be improved with more resources, suggesting scalability rather than versatility.\n",
      "\n",
      "So maybe CoRAG's approach isn't inherently bad for creativity, but its current applications are focused on tasks where retrieval is key, like answering factual questions or solving complex problems. To apply it to creative domains, we'd need new techniques that combine retrieval with generation in a way that allows for originality and style preservation.\n",
      "\n",
      "In the general knowledge base section (record 1), CoRAG's success isn't directly linked to creativity but could potentially support creative work by providing accurate starting points or inspiration. However, without specific mechanisms to handle originality, it might not fully enable creative tasks on its own.\n",
      "\n",
      "The conclusion is that while CoRAG's techniques have transferable parts—like retrieval and generation—it might need additional elements to effectively move into creative domains. The reliance on existing retrieval structures doesn't inherently limit creativity but may require new approaches or augmentations beyond the current setup.\n",
      "</think>\n",
      "\n",
      "The success of CoRAG, which relies on retrieval-augmented generation (RAG), in knowledge-intensive tasks does not inherently limit its potential for application in creative domains. CoRAG's ability to retrieve and generate text can serve as a foundational component for creativity by providing structured yet flexible content sources. However, it is essential to note that pure LLMs, such as LLaMA 2, demonstrate creativity without relying solely on retrieval techniques.\n",
      "\n",
      "CoRAG's architecture combines retrieval with generation, which could potentially blend both aspects needed for creative tasks like writing poetry or generating art. The reliance on retrieval might not inherently limit generative versatility but may require new mechanisms to handle originality and style preservation. Thus, while CoRAG's current applications are focused on knowledge-intensive tasks, it could be adapted into creative domains with the addition of new techniques that enhance its originality capabilities.\n",
      "\n",
      "In summary, CoRAG's techniques offer a promising foundation for creativity by integrating retrieval and generation. However, further development is needed to fully enable creative tasks, potentially involving augmentations beyond existing retrieval structures.\n",
      "##########\n",
      "Model: deepseek-r1:latest - Access: KG_ONLY - User: admin\n",
      "relationships: 73\n",
      "entities: 20\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out whether CoRAG's success in knowledge tasks can be applied to creative domains. The user is asking if the techniques used by CoRAG can transfer from areas like answering factual questions to creating art or stories.\n",
      "\n",
      "First, I'll look at what CoRAG does. From the data tables, it seems CoRAG uses retrieval-augmented generation, which means it combines existing knowledge with new inputs to generate responses. This is good for tasks where you need specific facts because it can pull relevant information from a database.\n",
      "\n",
      "But when it comes to creativity, things are different. Creativity often requires generating something novel and unique, not just recalling or combining known facts. RAG models like CoRAG might be okay at summarizing or explaining concepts, but they probably struggle with creating new ideas because their responses are constrained by the information they have access to.\n",
      "\n",
      "I remember reading that when evaluating AGI (Artificial General Intelligence), tasks require more than just pattern recognition; they need creativity and originality. CoRAG's reliance on existing data could limit its ability to come up with something truly novel in creative fields. Instead, models like LLMs might be better suited because they can generate text without strict knowledge constraints.\n",
      "\n",
      "Also, the data shows that CoRAG is used for tasks where domain knowledge is important, but it doesn't mention much about creativity. In contrast, pure LLMs don't have the same retrieval mechanisms and might rely more on their training data to produce outputs.\n",
      "\n",
      "So, putting this together, I think CoRAG's structure isn't ideal for creative domains because of its reliance on pre-existing information. It works well where you need accurate factual answers but probably can't create something entirely new or original.\n",
      "</think>\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "CoRAG (Retrieval-Augmented Generation) is designed primarily for tasks that require leveraging existing knowledge to generate responses, such as answering questions or providing explanations. Its reliance on retrieval mechanisms makes it highly effective in domains where accuracy and factual recall are crucial, like knowledge-intensive tasks. However, its structure inherently limits its ability to engage in creative processes. \n",
      "\n",
      "When considering CoRAG's applicability to creative domains, several factors come into play:\n",
      "\n",
      "1. **Creativity and Novelty**: CoRAG excels at combining relevant information with new inputs but struggles with generating something fundamentally novel or creative. This limitation is common across many AI systems designed for structured tasks.\n",
      "\n",
      "2. **Model Architecture**: Pure LLMs (Language Models) often lack the explicit mechanisms that constrain creativity, allowing them to generate text more freely without strict knowledge limitations. CoRAG's reliance on pre-existing data might hinder its capacity to create truly original content.\n",
      "\n",
      "3. **Evaluation Metrics for Creativity**: Assessing creative outputs is inherently subjective and context-dependent, making it challenging to define clear benchmarks. Most evaluations of generative models focus on coherence, relevance, or ability to follow instructions rather than creativity.\n",
      "\n",
      "4. **Generalization Across Tasks**: The paper discusses CoRAG's applications in specific knowledge domains but does not provide evidence of its effectiveness in creative tasks. This suggests that while CoRAG is powerful for certain tasks, it may not be the best fit for domains requiring originality and innovation.\n",
      "\n",
      "In conclusion, CoRAG's success in knowledge tasks is due to its ability to handle structured retrieval and generation. However, its reliance on existing information makes it less suitable for creative domains compared to pure LLMs, which are designed without such constraints. This distinction implies that while CoRAG excels where knowledge accuracy is key, it may not be the optimal choice for tasks requiring creativity and originality.\n",
      "\n",
      "--- \n",
      "\n",
      "This answer provides a structured analysis of CoRAG's capabilities and limitations in both knowledge-intensive and creative domains, supported by insights from the provided data tables and general understanding of AI models.\n",
      "##########\n",
      "Model: deepseek-r1:latest - Access: CHUNKS - User: user1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether CoRAG's success in knowledge tasks can be transferred to creative domains. The user is also concerned about if its reliance on retrieval limits its generative versatility compared to pure LLMs. Hmm.\n",
      "\n",
      "First, I need to understand what CoRAG is. From the data tables, it looks like CoRAG stands for Retrieval-Augmented Generation. It's a technique that combines retrieval mechanisms with generation models. So, in knowledge tasks, where factual information is key, CoRAG has shown success by retrieving relevant information and augmenting it with AI-generated text.\n",
      "\n",
      "But now, moving to creative domains. Creativity often requires more than just retrieval; it needs the ability to generate novel ideas or combine concepts in unique ways. Retrieval alone might not be enough because creativity involves original thought processes that aren't just based on existing data. So, CoRAG's reliance on retrieval could limit its generative versatility.\n",
      "\n",
      "Looking at the sources provided: there are mentions of RAG and LLMs. For example, Source 0 says that in RAG domains like next-token prediction, there's an objective within a multilingual model to predict subsequent tokens. Then, Source 1 talks about Chain-of-Retrieval Augmented Generation by Lia... which suggests that retrieval is integrated into the generation process.\n",
      "\n",
      "But when it comes to creativity, maybe pure LLMs have an edge because they're designed for generating new content without relying on external data sources as much. They can handle more varied and complex tasks because their training allows them to create from scratch rather than augmenting existing knowledge.\n",
      "\n",
      "I also recall that in Source 15, Long-tail factual knowledge refers to less common or niche information which might not be well-handled by CoRAG if it's too reliant on retrieval. However, for knowledge tasks, where data is key, CoRAG has its uses.\n",
      "\n",
      "Putting this together, while CoRAG excels in structured knowledge tasks through retrieval and augmentation, it might struggle with the nuanced, original creation needed in creative domains. Pure LLMs could offer more generative versatility because they're trained on a broader range of text and can handle diverse outputs without relying on pre-retrieved data.\n",
      "\n",
      "I should also consider that this is speculative based on existing data. The dataset doesn't provide direct comparisons between CoRAG's creativity capabilities and pure LLMs, but from the information given, it seems like retrieval could be limiting for creative tasks.\n",
      "</think>\n",
      "\n",
      "Based on the analysis of CoRAG's capabilities within knowledge-intensive tasks and its potential limitations in creative domains, here is a structured response:\n",
      "\n",
      "### Understanding CoRAG\n",
      "CoRAG (Retrieval-Augmented Generation) integrates retrieval mechanisms with generation models to enhance performance in knowledge tasks. It retrieves relevant information and augments it with AI-generated text, making it effective for structured tasks that require factual accuracy.\n",
      "\n",
      "### Creativity and Retrieval Limitations\n",
      "Creativity often demands the ability to generate novel ideas or combine concepts uniquely. CoRAG's reliance on retrieval may limit its generative versatility because creativity typically involves original thought processes beyond mere data augmentation. Retrieval mechanisms are optimized for retrieving existing information rather than fostering originality.\n",
      "\n",
      "### Comparison with Pure LLMs\n",
      "Pure Large Language Models (LLMs) excel in creative tasks due to their training on diverse text and ability to generate novel outputs without relying solely on pre-retrieved data. Their design allows them to handle complex, varied, and original content creation more effectively than CoRAG's structured retrieval-augmented approach.\n",
      "\n",
      "### Conclusion\n",
      "While CoRAG is effective for knowledge tasks through retrieval and augmentation, its reliance on this mechanism may hinder its generative versatility in creative domains compared to pure LLMs. The integration of retrieval might be insufficient for tasks requiring originality and nuanced creativity, suggesting that CoRAG's strengths lie more in structured, factual contexts rather than unstructured creative outputs.\n",
      "\n",
      "This conclusion is drawn from the provided dataset, which highlights CoRAG's effectiveness in specific knowledge areas but does not provide direct evidence on its application in creative domains.\n",
      "##########\n",
      "Model: deepseek-r1:latest - Access: CHUNKS - User: admin\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out whether CoRAG can transfer its success from knowledge tasks to creative domains, considering that it relies on retrieval which might limit its generative versatility compared to pure LLMs. Let's start by breaking down the information given.\n",
      "\n",
      "First, looking at the 'entities' table, CoRAG is associated with entities like RAG, LLM, and entities such as RETRIEVAL-AUGMENTED GENERATION (RAG), LONG-RAG, and various models like CORAG-8B. The descriptions for these entries suggest that CoRAG is a model designed for specific tasks, mainly retrieval-augmented generation.\n",
      "\n",
      "In the 'sources' table, there's mention of next-token prediction objectives within a multitask learning framework in dataset 43, which suggests that CoRAG has been trained on multiple tasks. There are also references to chain-of-thought methodologies and potential applications in rejection sampling and test-time decision-making from dataset 51. These might indicate that while CoRAG is effective in certain domains, its application beyond knowledge-intensive areas could be limited.\n",
      "\n",
      "Looking at the 'points' table, each record seems to outline different applications and relationships. For instance, CoRAG's application spans areas like response generation, IRCoT, multi-hop question answering, etc. However, none of these seem directly related to creativity or creative domains. This suggests that while CoRAG is powerful in retrieval tasks, its adaptability into generative or creative processes might be constrained.\n",
      "\n",
      "Now, considering the comparison with pure LLMs: models like GPT are known for their generative capabilities without explicit retrieval mechanisms. They can create content more flexibly because they don't rely on external data sources beyond their own training. CoRAG's reliance on retrieval could act as a constraint when it comes to generating entirely new, creative outputs that require the model to create from scratch rather than retrieve and augment.\n",
      "\n",
      "However, there are exceptions mentioned in the 'points' table where pure LLMs aren't suitable—like long-tail factual knowledge (record 15) or Federated Domain-specific Knowledge Transfer (record 18). This indicates that while generative creativity might be an area where CoRAG could excel if it can integrate more flexible generation methods alongside retrieval.\n",
      "\n",
      "Additionally, models like LLaMA and ChatGPT are noted to have limitations in certain domains due to their reliance on prompt engineering rather than pure language understanding. So, CoRAG's approach, which combines retrieval with generative tasks, might offer a balance but could still be limited compared to pure LLMs in terms of creative output.\n",
      "\n",
      "In summary, while CoRAG has shown success in knowledge-intensive areas through its retrieval-augmented capabilities, transferring this success into creative domains may face limitations due to its reliance on existing data sources. Pure LLMs might offer more generative flexibility, but advancements like enhanced retrieval mechanisms could mitigate some of these limitations.\n",
      "</think>\n",
      "\n",
      "**CoRAG's Application Across Knowledge and Creativity Domains**\n",
      "\n",
      "CoRAG, standing for Retrieval-Augmented Generation (RAG), has demonstrated significant success in knowledge-intensive tasks by integrating retrieval mechanisms with generative capabilities. Its architecture is designed to augment text generation through retrieved information from external sources, making it highly effective in areas requiring factual accuracy and contextual relevance.\n",
      "\n",
      "**Knowledge Intensive Tasks: Strengths**\n",
      "\n",
      "CoRAG's proficiency lies in its ability to combine retrieval with generative processes, as evidenced by its applications across various knowledge domains:\n",
      "\n",
      "1. **Text Generation**: CoRAG excels in generating coherent and contextually accurate text based on retrieved information.\n",
      "2. **Question Answering Systems (QAS)**: By leveraging chain-of-thought methodologies, CoRAG improves the accuracy of QAS through enhanced retrieval mechanisms.\n",
      "3. **Creative Limitations**: While its strengths are evident in knowledge domains, transitioning to creative tasks presents challenges.\n",
      "\n",
      "**Challenges in Creative Domains**\n",
      "\n",
      "The reliance on retrieval can be a limitation when it comes to creativity:\n",
      "\n",
      "1. **Generative Versatility**: Pure Large Language Models (LLMs) like GPT offer more generative flexibility due to their inherent ability to create content without explicit data sources, unlike CoRAG's structured approach.\n",
      "2. **Content Creation**: Creative processes often require originality and unrestricted generation, which can be constrained by retrieval mechanisms that depend on pre-existing datasets.\n",
      "\n",
      "**Potential Mitigation Strategies**\n",
      "\n",
      "1. **Hybrid Models**: Enhancing CoRAG through advancements in hybrid models could bridge the gap between retrieval and generative creativity.\n",
      "2. **Creative Augmentation**: Integrating more flexible generation methods alongside retrieval mechanisms might allow CoRAG to approach creative tasks with greater diversity, provided prompt engineering is enhanced.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "While CoRAG's success in knowledge domains is commendable, its reliance on retrieval may inherently limit its adaptability to purely creative processes compared to pure LLMs. However, with potential advancements and hybrid model developments, CoRAG could bridge these gaps, offering a balanced approach that combines retrieval strengths with enhanced generative capabilities for diverse applications.\n",
      "##########\n",
      "Model: deepseek-r1:latest - Access: FULL - User: user1\n",
      "reports: 1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "<think>\n",
      "Okay, I need to figure out whether CoRAG's success in knowledge tasks can be transferred to creative domains, and if not, how does its reliance on retrieval limit its generative capabilities compared to pure LLMs. \n",
      "\n",
      "First, I'll look at what CoRAG is. From the data tables, CoRAG seems to be a model that combines retrieval with generation. It's used in tasks where knowledge is needed, like answering factual questions.\n",
      "\n",
      "Looking at the sources, Record 0 from dataset next-token prediction talks about how models can predict the next token within multiple steps. That might relate to generative capabilities, but I'm not sure yet. \n",
      "\n",
      "Record 42 says CoRAG-8B outperforms pure LLMs in knowledge tasks. So that's a point for CoRAG having good knowledge transfer.\n",
      "\n",
      "But then, Record 51 mentions retrieval-enhanced models are used in both rejection sampling and test-time decoding, which could affect creativity because they might be too guided by existing data instead of generating new ideas.\n",
      "\n",
      "In the sources about generative abilities (dataset Sources), Record 43 talks about next-token prediction objectives, suggesting that models can generate text step by step. CoRAG probably benefits from this but might struggle in creative areas where deviation is needed.\n",
      "\n",
      "Putting it together: CoRAG does well in knowledge tasks because of its retrieval component, but this reliance on existing data might limit its creativity since it's not generating entirely new ideas but rather augmenting retrieved information.\n",
      "</think>\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "CoRAG (Retrieval-enhanced Augmented Generation) demonstrates significant success in knowledge-intensive tasks, as evidenced by Record 42 where CoRAG-8B outperformed pure LLMs. Its ability to leverage retrieval mechanisms provides it with strong performance in domains requiring factual or structured information processing. However, this reliance on retrieving existing data inherently constrains its generative versatility compared to pure Large Language Models (LLMs). \n",
      "\n",
      "While pure LLMs can generate text more freely and creatively by exploring new ideas without being bound by stored data, CoRAG's augmentation of retrieval processes might limit its capacity for truly innovative thinking. This is supported by Record 51, which highlights how retrieval-enhanced models are often used in structured or rejection sampling contexts, suggesting a bias towards generating content within predefined data boundaries.\n",
      "\n",
      "In summary, while CoRAG excels in knowledge tasks through effective retrieval and generation integration, it may lack the creative flexibility of pure LLMs due to its reliance on existing data for generating new text.\n",
      "##########\n",
      "Model: deepseek-r1:latest - Access: FULL - User: admin\n",
      "reports: 1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "<think>\n",
      "Okay, so I need to figure out whether CoRAG's success in knowledge tasks can be transferred to creative domains. The user is asking if CoRAG's strengths might make it versatile enough for creativity beyond just retrieval.\n",
      "\n",
      "First, I remember that CoRAG uses a hybrid approach combining generation and retrieval models. It's been successful in tasks like question answering where context matters. But creative tasks require something more—like generating ideas or art from scratch without existing data.\n",
      "\n",
      "Looking at the data tables provided, CoRAG has 23 relationships, which suggests it's well-connected but not as specialized as some pure LLMs. The knowledge graph is rich, so maybe that helps in generating related concepts for creativity. There are mentions of IRCoT and bootstrapping techniques, which might aid in expanding creative ideas.\n",
      "\n",
      "However, CoRAG does rely on existing data, so it's limited by the information it has access to. For creative tasks like poetry or music composition, where originality is key, relying solely on data might not cut it. Pure LLMs without prior knowledge can sometimes create more novel outputs because they don't depend on external data.\n",
      "\n",
      "So, while CoRAG can provide a good foundation for creativity by leveraging its knowledge graph and generation capabilities, the reliance on retrieval might limit its ability to generate truly original creative content. To enhance this, combining CoRAG with other techniques that encourage exploration could help expand its creative potential.\n",
      "</think>\n",
      "\n",
      "# Transfer of CoRAG's Success to Creative Domains: A Comparative Analysis\n",
      "\n",
      "The question at hand is whether CoRAG’s success in knowledge-intensive tasks can be meaningfully extended into the creative domains. This involves exploring both the strengths and limitations of CoRAG when applied outside its traditional purview, such as in tasks requiring originality or novel outputs.\n",
      "\n",
      "## Theoretical Foundations: Understanding CoRAG's Architecture\n",
      "\n",
      "CoRAG (Retrieval-enhanced Augmented Generation) integrates retrieval mechanisms with generative models to enhance performance in knowledge-based tasks. Its architecture combines the strengths of large language models (LLMs) and deep retrieval systems, creating a hybrid model that excels in structured reasoning and factual querying.\n",
      "\n",
      "The success of CoRAG is evident in its ability to combine retrieval and generation effectively. For instance, IRCoT, a variant discussed in the data, employs chain-of-thought methodology, which could theoretically aid creative processes by allowing iterative refinement based on retrieved information. However, this approach may inherently limit generative versatility compared to pure LLMs.\n",
      "\n",
      "## Transferability to Creative Domains\n",
      "\n",
      "Creative domains often require more than mere factual retrieval; they demand originality, innovation, and the ability to generate content without explicit guidance. While CoRAG’s hybrid architecture could potentially aid creativity by providing a rich knowledge graph and structured generation pathways, its reliance on existing data may impose limitations.\n",
      "\n",
      "Consider the case of iterative refinement in creative processes: IRCoT’s chain-of-thought methodology could allow for generative iterations based on retrieved information, creating a feedback loop that might enhance creativity. However, this depends heavily on the quality and diversity of the underlying knowledge base.\n",
      "\n",
      "## Broader Implications\n",
      "\n",
      "The discussion raises questions about the role of pure LLMs in creative processes. While they may lack specific domain knowledge, their ability to generate from scratch could sometimes lead to novel outputs without prior exposure to specific data. This suggests that a hybrid model like CoRAG might offer a balanced approach for creativity, combining structured retrieval with generative flexibility.\n",
      "\n",
      "Moreover, the reliance on pre-existing knowledge graphs in CoRAG introduces potential biases and limitations. Creative processes often benefit from unconstrained exploration of possibilities, which may not always be feasible with a data-dependent model.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "In conclusion, while CoRAG’s success in knowledge tasks suggests promising hybrid capabilities, its transferability to creative domains is contingent upon addressing the reliance on pre-existing knowledge graphs and exploring methods to enhance generative versatility. A combination of retrieval-enhanced models like CoRAG with techniques that foster originality and exploration could potentially expand its creative potential.\n",
      "\n",
      "In summary, CoRAG's strengths in structured reasoning and generation make it a valuable tool for creativity when complemented by appropriate methodologies. However, its limitations as a sole-reliance model on existing data should be acknowledged, suggesting that hybrid approaches combining retrieval with pure generative capabilities may hold greater promise.\n",
      "##########\n",
      "{'a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73'}\n",
      "Model: deepseek-r1:latest - Access: DOCUMENT_LEVEL - User: user1\n",
      "entities: 6\n",
      "relationships: 20\n",
      "sources: 1\n",
      "reports: 1\n",
      "\n",
      "<think>\n",
      "Okay, so I'm trying to understand whether Corag's success in knowledge tasks can be transferred to creative domains. The user also asked about how Corag's reliance on retrieval might limit its generative versatility compared to pure LLMs. \n",
      "\n",
      "First, I need to clarify what Corag is. From my understanding, Corag is a large language model built by DAMO Academy. It's designed with knowledge graphs and retrieval mechanisms, which means it can access vast amounts of structured data in addition to handling text.\n",
      "\n",
      "Now, the user is asking if Corag's success on tasks like answering questions or providing information (which are knowledge-intensive) can also help in creative areas like writing poetry or composing music. I think about how LLMs work differently from traditional retrieval systems. Pure LLMs generate text based solely on patterns learned from vast amounts of text data, without necessarily retrieving specific factual information unless it's encoded into the model.\n",
      "\n",
      "Corag, since it uses a retrieval system, might have an advantage in quickly accessing relevant facts or concepts for knowledge tasks. However, creativity involves more than just recalling facts; it requires originality and perhaps combining ideas in novel ways. Pure LLMs might struggle with creating truly creative content because they don't inherently understand the context or the nuances required for creativity.\n",
      "\n",
      "So, Corag's reliance on retrieval could limit its generative versatility because while it can efficiently find relevant information to answer questions or explain concepts, it lacks the ability to generate entirely new ideas without some form of structured knowledge input. On the other hand, pure LLMs might be better at generating creative outputs but could lack accuracy and relevance in specific knowledge domains unless they're fine-tuned for that purpose.\n",
      "\n",
      "I also think about how Corag can be fine-tuned with specific knowledge graphs or datasets to improve its creativity. If someone adds domain-specific information, maybe it can start to generate more relevant creative content. But without that training, it's probably not as versatile in creative tasks compared to a pure LLM.\n",
      "\n",
      "Another point is the balance between retrieval and generation. Corag might excel where structured data is involved but needs assistance or input to create truly original works. Pure LLMs, on the other hand, are more about free-form text generation without external data sources.\n",
      "\n",
      "So, putting it all together, while Corag's strengths in knowledge tasks could theoretically support some creative domains if used with the right context, its reliance on retrieval inherently limits its generative versatility compared to pure LLMs that don't rely on such systems. To bridge this gap, Corag might need additional training or integration of domain-specific knowledge.\n",
      "</think>\n",
      "\n",
      "Corag's potential for application in creative domains is influenced by its combination of knowledge retrieval and generation capabilities. Here's a structured summary of the analysis:\n",
      "\n",
      "1. **Corag's Architecture**: Corag, an advanced LLM developed by DAMO Academy, integrates retrieval mechanisms alongside its generative abilities. This allows it to quickly access relevant information from structured data.\n",
      "\n",
      "2. **Strengths in Knowledge Tasks**: Corag excels in knowledge-intensive tasks due to its ability to retrieve and utilize structured data efficiently. This makes it effective for answering questions, providing explanations, and handling factual inquiries.\n",
      "\n",
      "3. **Limitations in Creativity**: The reliance on retrieval systems inherently limits Corag's generative versatility. While it can generate text based on patterns learned from data, it lacks the inherent creativity required for tasks like writing poetry or composing music without external input or context.\n",
      "\n",
      "4. **Comparison with Pure LLMs**: Pure LLMs excel in generating creative content because they learn from vast amounts of text data alone, without structured retrieval systems. However, this can lead to less relevant outputs in specific knowledge domains unless fine-tuned.\n",
      "\n",
      "5. **Potential for Creativity with Context**: Corag's capabilities could be enhanced by adding domain-specific information or context. Fine-tuning the model with structured datasets might improve its creativity within those domains.\n",
      "\n",
      "6. **Conclusion**: While Corag's strengths in knowledge tasks provide a foundation that could support creative domains, its reliance on retrieval inherently limits its ability to generate entirely original content without additional input. Pure LLMs, by contrast, offer more raw creative potential but require specific training for knowledge-related tasks.\n",
      "\n",
      "In summary, Corag can potentially aid creativity when provided with relevant context and domain-specific data, though it may need additional training or integration of structured knowledge to match the generative versatility of pure LLMs.\n",
      "##########\n",
      "{'a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73', 'f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3'}\n",
      "Model: deepseek-r1:latest - Access: DOCUMENT_LEVEL - User: admin\n",
      "entities: 20\n",
      "relationships: 73\n",
      "sources: 4\n",
      "reports: 1\n",
      "\n",
      "<think>\n",
      "Okay, so I'm trying to figure out whether Corag's success in knowledge-based tasks can carry over into creative domains. The user is asking if the way Corag works, which relies a lot on retrieval from existing data, might be limiting its ability to generate new creative content compared to pure large language models (LLMs).\n",
      "\n",
      "First, I need to understand what Corag does. From what I know, Corag uses a hybrid approach where it combines retrieval with reasoning and generation. It's designed for tasks like answering questions by pulling relevant information from vast databases but also doing some reasoning or generating text based on that info.\n",
      "\n",
      "Now, in knowledge tasks, Corag probably excels because it can quickly find the right data and apply reasoning to provide accurate answers. But when it comes to creativity, things might be different. Pure LLMs are trained solely on generating text without relying on external data sources, so they have a lot of creative freedom from their training data.\n",
      "\n",
      "I wonder if Corag's reliance on retrieval could limit its generative capabilities. Since it retrieves information and then applies reasoning or generation based on that, maybe it lacks the raw creativity that comes from being trained to generate text without external input. Pure LLMs don't depend on databases; they rely solely on their training data which can be a diverse set of creative texts.\n",
      "\n",
      "On the other hand, Corag might handle certain aspects better because of its structured approach—like needing context or having to pull specific facts before generating something. That could actually help in some creative tasks where context is important, like writing a story with specific elements pulled from a database and then expanding on them creatively.\n",
      "\n",
      "But wait, if the user is asking whether Corag's success in knowledge tasks can transfer to creativity, I'm not sure it would be as strong because the two are quite different. Knowledge tasks require accuracy and relevant information, while creative tasks need originality and flexibility beyond pre-existing data.\n",
      "\n",
      "So, does Corag's method inherently limit its generative versatility compared to pure LLMs? Maybe yes, but maybe there are ways around it. Perhaps combining retrieval with generation in Corag could still allow for some creativity as long as the system is designed to use the retrieved information flexibly and apply creative processes on top of that.\n",
      "\n",
      "I should also consider how Corag handles reasoning versus generation. If it's good at both, perhaps it can support more creative tasks by not being constrained by just retrieval alone. But if it's mainly relying on retrieval for everything, then maybe its creativity is more limited compared to pure LLMs which don't rely on external data.\n",
      "\n",
      "In summary, my initial thought is that Corag's reliance on retrieval might indeed limit its generative versatility in creative domains because pure LLMs are trained solely on generating text without external data sources. However, with some innovative approaches, maybe Corag can enhance its creativity by combining retrieval with other methods.\n",
      "</think>\n",
      "\n",
      "Corag's hybrid approach, which combines retrieval, reasoning, and generation, excels in knowledge tasks where accuracy and relevant information retrieval are crucial. While this strength is beneficial for structured and factual tasks, it may inherently limit generative versatility in creative domains compared to pure large language models (LLMs). Pure LLMs, trained solely on generating text without external data sources, offer a higher degree of creative freedom due to their training on diverse, creative texts.\n",
      "\n",
      "However, Corag's system could potentially enhance creativity by integrating retrieval with other methods that allow for flexibility and creative processes. This approach might leverage the structured information retrieval while still supporting some level of originality in tasks where context is important. Thus, while Corag may not match pure LLMs' generative capabilities out-of-the-box, innovative design could mitigate this limitation.\n",
      "##########\n",
      "Model: phi3.5:latest - Access: KG_ONLY - User: user1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "\n",
      "Cross-domain application of language models like CoRAG (which stands for Crossing Domain Generalist) involves assessing the model's ability to transfer knowledge learned in one domain, such as factual information retrieval or question answering within a specific area (\"knowledge tasks\"), and apply it effectively across different but related domains. The success transference of skills from structured data scenarios (where CoRAG models typically excel) into unstructured creative areas depends on several factors:\n",
      "\n",
      "1. **Model Design** – Traditional retrieval-based systems like CoRa are designed to find and use existing information effectively, which may not lend itself well directly to generative tasks that require creating new content from little or no reference material (common in pure LLMs). If a knowledge model has been specifically optimized for creativity alongside factual accuracy, it might perform better.\n",
      "\n",
      "2. **Degree of Specialization** – Knowledge models are often specialized and may not be inherently good at generative tasks if they have primarily learned patterns from structured data sources or focused on specific types of knowledge extraction without branching into creativity training components like divergent thinking, metapthyical understanding, narratives building, etc.\n",
      "\n",
      "3. **Transfer Learning** – For transferring skills across domains successfully, a model would need to generalize well beyond its original purpose and learn features that are universally applicable—a task for which deep learning models with sufficient capacity (and appropriate training) can achieve impressive results when combined with techniques like domain adaptation or multi-tasking.\n",
      "\n",
      "4. **Fine-tuning Capabilities** – Even if a knowledge model has fundamental capabilities, fine-tuning it on creative datasets could vastly improve its generativity without necessarily becoming as powerful in structured retrieval tasks anymore (and vice versa). This is something domain specialists or multimodal models often do.\n",
      "\n",
      "5. **Contextual Understanding** – Creativity frequently involves understanding context, emotions, nuances of language and societal norms—complex aspects that pure generative LLMs are trained on with massive datasets to capture well-rounded linguistic patterns but may not grasp as effectively from knowledge extraction tasks alone without specific training.\n",
      "\n",
      "6. **Flexibility in Output Generation** – Generativeness involves more than merely selecting the right facts; it requires synthesizing ideas, constructing stories or poems, inventive metaphors—tasks where CoRAG's retrieval foundation might not be sufficient by itself without additional mechanisms for creativity.\n",
      "   \n",
      "In summary, while a knowledge-driven model like CoRa has strong potential in generating factually accurate content within its domain of expertise due to effective information extraction and recall capabilities, these abilities do not inherently translate into generative tasks requiring novel creation or artistic flair without additional creativity training. To perform well on both kinds of tasks—knowledge retrieval as well as generation in a more freeform style commonly seen with LLMs (Large Language Models) —a system would likely require specialized components for each, potentially integrated within the same model architecture or through separate systems that communicate effectively to provide complementary capabilities.\n",
      "##########\n",
      "Model: phi3.5:latest - Access: KG_ONLY - User: admin\n",
      "relationships: 73\n",
      "entities: 20\n",
      "\n",
      "The performance of models like CORe-Augmented Retrieval (CoRa) in various tasks can depend heavily on their architecture and the way they've been trained. CoRAG or similar retrieval-augmented generation systems are typically designed to leverage a large corpus of knowledge by efficiently fetching relevant information during generative processes, which is quite effective for factoid questions where specific details need to be accurately retrieved from existing data sources (e.g., databases).\n",
      "\n",
      "When it comes to transferring this success into creative domains such as artistic writing or music composition—where the generation isn't just about retrieving facts but also creating something novel and expressive with fewer constraints on factual accuracy:\n",
      "\n",
      "1. **Knowledge Retrieval-Augmented Systems (CoRAGs):** If CoRa systems are primarily built for knowledge tasks, they may perform well when the output requires specific details or context retrieval but might struggle in creative areas where novel ideas and expressions—rather than predefined facts—are essential. They could potentially provide inspiration by fetching examples of past works that match a desired style or theme if there's an extensive knowledge base to learn from, though this still differs significantly from pure generative models which can create without direct reference material for guidance.\n",
      "\n",
      "2. **Limitations Due to Retrieval Focus:** CoRAG systems might inherently limit their creativity since they are focused on retrieval and may not have the innate capability of generating entirely new ideas or content that has no precedent in training data, which is often required for truly innovative outputs like original poems or art.\n",
      "\n",
      "3. **Comparison with Pure LLMs:** Compared to pure Large Language Models (LLMs), CoRAG might lag creatively since these models can generate text without needing a retrieval step, and instead have vast internal representations that they train on diverse datasets allowing them more freedom in the generative process.\n",
      "\n",
      "4. **Hybrid Approaches:** Some systems attempt to bridge this gap by combining knowledge-driven approaches with pure generation techniques (such as controlled or prompted creativity). These models use retrieval for inspiration and context while also engaging their learned patterns of language production, potentially leading to more balanced performances in both domains.\n",
      "\n",
      "5. **Training Adjustments:** To facilitate transfer across different tasks, CoRAG systems can be fine-tuned on diverse datasets or trained with specific objectives that encourage creativity—this might involve using specialized prompt engineering and additional training to develop the model's generative capabilities further without losing its strong knowledge retrieval abilities.\n",
      "\n",
      "In conclusion, while a system like CoRAG may excel in tasks where factual accuracy is paramount due to their designed strength in information extraction from large datasets or corpora, creativity often requires models that can simulate abstract connections and novel compositions beyond existing data points alone—qualities more characteristic of pure generative language systems. Hybrid approaches might be necessary for a CoRAG-like system if it is intended not just to excel at knowledge tasks but also in domains requiring higher degrees of original creativity, though this could involve complex training regimes and design considerations that balance retrieval with open generation capabilities.\n",
      "##########\n",
      "Model: phi3.5:latest - Access: CHUNKS - User: user1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "Cross-domain application of models like CORe50 (CORE Knowledge Representation and Generalization) generally pertaining to knowledge tasks involves understanding the structure within which these systems were trained. They are often optimized for specific types of data processing, such as answering questions or performing fact retrieval from a large corpus based on entity recognition and relationship mapping between entities.\n",
      "\n",
      "CORe50 is not an LLM (Large Language Model) per se but rather seems to refer to knowledge-driven models designed with structured databases in mind for efficient information extraction, reasoning, and possibly basic creative tasks like generating summaries or explanations based on predefined templates within the scope of its training.\n",
      "\n",
      "Transferring CORe50' extrinsic capabilities (or any model optimized primarily for knowledge retrieval) to purely generative domains where novel text creation without explicit information can be a challenge:\n",
      "\n",
      "1. **Knowledge Transfer**: If your goal is related and falls within the range of its training, there may exist some level of transferable success because it has access to structured factual databases that could inform content generation tasks like storytelling or creative writing guided by facts (e.g., historical dates).\n",
      "   \n",
      "2. **Creativity Limitation**: However, CORe50's core strength lies in knowledge extraction rather than generative creation without specific prompting—so when it comes to original fiction generation, world-building beyond its stored databases or abstract creative thinking that requires analogical reasoning and out-of-context synthesis of ideas, the limitations become quite significant.\n",
      "   \n",
      "3. **Leveraging Retrieval for Creativity**: One way CORe50' might contribute generatively is through a process called \"museomatching\" or creative prompt engineering where external information retrieval informs content generation—selectively combining structured knowledge with open-ended language model capabilities, if such integration were attempted.\n",
      "   \n",
      "4. **Generation Flexibility**: Pure LLM's (like GPT models) have been shown to generate text more freely due their large context windows and less rigid training on factual databases; they excel at creating diverse sentences within a given topic, which could lead them generally further than CORe50 in generative tasks.\n",
      "   \n",
      "In conclusion, while there might be niches where knowledge extraction from models like CORe50 can inform or inspire creative outputs—particularly when such content requires factual accuracy and contextually rich grounding (e.g., educational material generation) — the intrinsic architectural bias towards retrieval over generative creation means that they are likely to be outperformed by pure LLM's in more broad, open-ended creativity tasks without significant modification or augmentation of their capabilities with additional language model components designed for high versatility.\n",
      "##########\n",
      "Model: phi3.5:latest - Access: CHUNKS - User: admin\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "CoRAG (Cross-domain Retrieval and Generation) systems are typically designed for specific types of problems that require both the ability to retrieve relevant information from large databases/knowledge bases, as well as generate new content based on this retrieved knowledge. Pure Language Models (LMs), such as GPT or BERT in their generative forms, rely heavily on patterns learned during training and may not have direct access to structured external data sources for retrieval purposes but excel at generating text that is coherent with human language styles due to extensive pre-training.\n",
      "\n",
      "The ability of CoRAG systems to transfer success from knowledge tasks back into creative domains depends significantly on several factors:\n",
      "\n",
      "1. **Domain Adaptation**: The extent and quality of the training data used for domain adaptation can strongly influence a system's performance in both retrieval and generation, especially when crossing between different types of content (e.g., factual information to fiction writing). If CoRAG systems are trained with diverse datasets spanning creative domains alongside knowledge tasks, they may have better transfer learning capabilities into those areas compared to pure LLMs that might not be as broadly exposed in their training data and hence could lack relevant references or styles.\n",
      "\n",
      "2. **Flexibility of the Architecture**: CoRAG architectures are typically built with retrieval mechanisms (e.g., using external databases, APIs), which may constrain them when compared to pure generative LLMs that rely solely on learned patterns from a text corpus for creativity and generation tasks without explicit knowledge base lookups or real-time factual data integration during the writing process.\n",
      "\n",
      "3. **Task Design**: The way CoRAG systems are tasked can determine their applicability to creative domains—if it's set up as generative, then they could create new content using retrieved information effectively; if retrieval is all there is (e.g., QA), the transfer might be limited primarily to answering questions or summarizing known facts rather than generating novel narratives.\n",
      "\n",
      "4. **Hybrid Approaches**: Some advanced AI models are being developed as hybrids, combining generative approaches with mechanisms for external data retrieval and integration (similarly inspired by CoRAG systems). These can potentially excel in creativity-based tasks if they efficiently merge the deep knowledge extraction of Retrieval Models like dactyl or T5 with extensive pretraining from LLMs.\n",
      "\n",
      "In summary, while pure LLMs may lack explicit retrieval capabilities and might rely on their vast exposure to text for generality in creative domains, CoRAG systems could excel when the context requires both factual accuracy provided by retrieved knowledge alongside narrative creation or other forms of content generation—if they've been suitably trained. The success transfer heavily relies not only on system architecture but also training data and task specifics designed for cross-domain performance excellence that can bridge between structured external information systems, like databases with creative outputs generated by a language model endowed with both retrieval capability (like CoRAG) or hybrid designs.\n",
      "\n",
      "Thus, there's no definitive answer without considering the specific design and training of each system in question—CoRAs can potentially transfer to generative tasks if designed appropriately; however, their success might be limited compared with pure LLMs that are tailored for content generation from a pre-trained perspective. It is an area where ongoing research could unveil more nuanced understandings and technological advancements in the near future.\n",
      "##########\n",
      "Model: phi3.5:latest - Access: FULL - User: user1\n",
      "reports: 1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "CoRaG (Content Retrieval and Generation) systems are designed for a specific type of task that involves both content retrieval from databases/archives and generation based upon this retrieved information. The success of such models on knowledge tasks often depends on the nature of their training data, architectures, and objectives.\n",
      "\n",
      "Transfer to Creative Domains:\n",
      "- **Possible Limitations**: CoRaG systems may have limitations when it comes to highly creative domains because they are traditionally more structured in retrieving factual information rather than generating novel content from scratch or inventing new ideas (hallmarks of pure LLMs). Their generative capabilities might be constrained by the scope and nature of their training data.\n",
      "- **Potential Use**: However, with sufficient fine-dictional tuning that includes creative elements in its dataset, CoRaG could potentially assist or contribute to tasks within certain artistic domains – for example as a tool to generate specific styles of writing based on historical texts it has learned from.\n",
      "  \n",
      "Generativity vs Retrieval: Pure LLMs (Large Language Models):\n",
      "- **Pure Generative Capabilities**: Purely generative large language models are trained end-to-end for generation without explicit retrieval stages, which can sometimes make them more versatile in creative tasks. They might be better at producing content that doesn't rely heavily on external databases and instead generates from learned patterns across a broad dataset encompassing many styles of writing or imagery creation.\n",
      "  \n",
      "- **Fine Tuning for Creativity**: Fine tuned LLMs can potentially reach high levels of creativity by incorporating specific datasets that represent the desired artistic style, genre, etc., but they may still not match CoRaG's structured knowledge retrieval aspect in domains where factual accuracy and source-based generation are key.\n",
      "  \n",
      "Transferability between Models:\n",
      "To transfer skills from a model like CoRaG to generative tasks typically associated with pure LLMs, one would likely need significant adaptation or hybrid approaches that combine the strength of each system while mitigating their weaknesses. For example, incorporating retrieval capabilities into an end-to-end generation framework might lead to novel applications where both factual information and generative creativity are required (e.g., writing historical fiction).\n",
      "  \n",
      "In summary: While pure LLMs generally have a more flexible approach that's conducive for broadly diverse tasks, including those in the realm of artistic creation, CoRaG systems might excel at specific types where structured retrieval and generation are beneficial. For transfer to creative domains particularly effective hybrid models or methodologies may be necessary rather than direct transference from one system type’s strengths alone.\n",
      "\n",
      "To fully understand the potential of such a transition, empirical research would need to assess how CoRaG systems can adapt and perform in tasks where generativity is crucial while leveraging their robust retrieval faculties – essentially creating new paradigms for creative AI that merge factual rigor with imaginative production.\n",
      "##########\n",
      "Model: phi3.5:latest - Access: FULL - User: admin\n",
      "reports: 1\n",
      "relationships: 73\n",
      "entities: 20\n",
      "sources: 4\n",
      "\n",
      "CoRAG (Cross-Modal Retrieval and Generation) systems are specialized for understanding the relationships between different modalities of data—such as textual information paired with images, videos, or audio. They excel at retrieving relevant examples from large datasets to inform a generation task based on this contextually rich input.\n",
      "\n",
      "On one hand, CoRaG's strength in cross-modal knowledge could be beneficial for creative domains because they often require understanding the nuanced relationships between different types of stimuli—like how visual art might inspire poetry or music composition:\n",
      "\n",
      "1. Inspiration Detection - By recognizing patterns across modalities, CoRAG systems can help identify sources of inspiration within vast datasets more effectively than traditional models that operate on a single modality. For instance, in discovering similar painting styles for digital art projects based on visual input alone might be limited without cross-modal insights provided by such systems.\n",
      "   \n",
      "2. Contextual Generation - CoRAG can provide relevant context when generating creative content which helps maintain the relevance and coherence of generated material within a multi-modality framework, potentially leading to richer outputs that leverage diverse data sources (e.g., text prompted by an image).\n",
      "   \n",
      "3. Improved Personalization - Since CoRAG systems can better understand user contexts or preferences across different modalities, they could tailor creative recommendations more closely aligned with individual tastes when compared to conventional models that do not make such cross-modal connections.\n",
      "\n",
      "On the other hand, there are some potential limitations in transferring knowledge from CoRaG systems directly into generative tasks:\n",
      "\n",
      "1. Retrieval Bias - Since these models rely heavily on retrieval of examples for generation, they could suffer if their training data lacks diversity or contains biases present across different modalities which might limit the breadth and originality in creative domains that require novelty-driven output (e.g., creating entirely new styles rather than emulating existing ones).\n",
      "   \n",
      "2. Generativeness vs Retrieval - CoRaG systems are not inherently generative models but rely on retrieval to produce outputs, which may make them less flexible in generating content without significant modifications or combining with pure LLMs (Large Language Models) that excel at text generation alone. For example, they might struggle when tasked with creating unique narratives purely from a dataset of images and texts if their core functionality does not generate new data but rather relies on the existing examples for synthesis.\n",
      "   \n",
      "3. Domain Limitations - Creative domains often demand an understanding beyond factual knowledge (e.g., abstract expressionism requires deep emotional intelligence), which CoRAG systems, being retrieval-based and trained across different modalities without necessarily comprehending these subtleties, might not capture as effectively compared to models specifically designed for creative generative tasks like pure LLMs or transformer architectures that are tailored towards generating novel content.\n",
      "   \n",
      "4. Technical Constraints - There may also be technical challenges in seamlessly integrating CoRaG's cross-modal retrieval with the continuous flow of generation, as creative processes often require iterative refinement and real-time interaction between different data modalities that pure generative LLM models are more attuned to handle.\n",
      "   \n",
      "To fully leverage a system like CoRAG for creative domains while mitigating its limitations, one might consider hybrid approaches where it is combined with or adapted by systems designed specifically around generation—think along the lines of using pure generative LLMs as an input source and supplementing them with cross-modal retrieval capabilities to inform content creation.\n",
      "   \n",
      "In conclusion, while CoRAG's multi-modality knowledge could certainly transfer some advantages into creative domains by providing richer contextual understanding for inspiration detection or personalized generation tasks, its success might be limited compared to pure LLMs due in part to the inherent retrieval bias and lack of generativeness. A hybrid approach that merges CoRAG's strength with a strong text-generation capability could potentially offer an optimal balance for creative applications requiring both cross-modal understanding and novel content generation.\n",
      "##########\n",
      "{'a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73'}\n",
      "Model: phi3.5:latest - Access: DOCUMENT_LEVEL - User: user1\n",
      "entities: 6\n",
      "relationships: 20\n",
      "sources: 1\n",
      "reports: 1\n",
      "\n",
      "CoRA (Cognitive Retrieval Agent) systems are designed with a focus on leveraging existing information from databases and other sources for reasoning about problems. They often excel in tasks that require factual knowledge, understanding context, or making logical inferences based upon retrieved data. However, CoRAs' performance characteristics may differ significantly when applied to creative domains compared to pure Large Language Models (LLMs) like GPT-3 or BERT:\n",
      "\n",
      "1. **Data Dependence and Retrieval Focus**: Since CoRAs rely on retrieving information rather than generating new content from internal patterns, they might not have the same level of versatility in creative tasks as pure LLMs which generate text based solely on training data without direct external input during generation.\n",
      "   \n",
      "2. **Knowledge and Context Understanding**: While CoRAs can often excel at leveraging factual knowledge, they might struggle with understanding the nuances of context or abstract reasoning required for creativity that goes beyond mere information retrieval. Pure LLMs are typically more generative but may lack specific depth in domain-specific facts without significant fine-dicting and prompt engineering by humans.\n",
      "   \n",
      "3. **Transferability**: CoRA's success on knowledge tasks might not seamlessly translate to creativity because the latter often requires a form of abstraction, innovation, or novel conceptual blending that can extend beyond retrieval capabilities alone. Pure LLMs are trained in such ways and may naturally integrate diverse concepts into new forms—a crucial aspect for many artistic tasks like storytelling, painting descriptions, poetry creation, etc.\n",
      "   \n",
      "4. **Generative Flexibility**: Generational models can produce a wide variety of outputs that closely mimic human creativity because they learn patterns from vast amounts of data including styles and structures not present in their training databases explicitly—something CoRAs are less equipped for without additional mechanisms to handle generativeness.\n",
      "   \n",
      "5. **Adaptability**: Pure LLMs can adapt based on new prompting scenarios due to the nature of deep learning, whereas a retrieval-based system like CoRA would require an update or reprogramming with relevant external data sources for similar tasks in creative domains—limiting their flexibility and timeliness.\n",
      "   \n",
      "6. **Interactivity**: The interaction between humans (or other systems) can drive the generativity of pure LLMs, leading to iterative refinements that are more characteristic of a human artist’s process than what retrieval agents typically offer which is largely static output based on available data points until updated with new inputs.\n",
      "   \n",
      "7. **Hybrid Approaches**: There have been efforts in AI research towards hybrids, combining the strengths of both generative and knowledge-based systems to handle creativity while leveraging facts when relevant—potentially providing a bridge between pure LLM capabilities and CoRA's information retrieval advantages.\n",
      "   \n",
      "In summary, although not impossible for specialized cases where factual accuracy or specific domain expertise is required (e.g., historical fiction writing), the inherent design of CoRAs with their focus on data-driven answers makes them less naturally suited to broad creative tasks that require generativity and abstract reasoning—areas where pure LLMs generally excel due to their internal pattern recognition capabilities absent any retrieval dependency.\n",
      "##########\n",
      "{'a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73', 'f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3'}\n",
      "Model: phi3.5:latest - Access: DOCUMENT_LEVEL - User: admin\n",
      "entities: 20\n",
      "relationships: 73\n",
      "sources: 4\n",
      "reports: 1\n",
      "\n",
      "CoRAGE (Creativity and Retrieval for Generative Task Execution) is a hypothetical system that combines aspects of both knowledge-grounded reasoning models like Factual Contrastive Language Learning with the creative generation found in language models. If CoRaG were to exist, its success on traditional factoid (knowledge retrieval and understanding tasks such as question answering) would not necessarily guarantee equivalent performance or transferability into purely generative domains that require high degrees of originality and abstraction thinking—common in artistic creativity like writing poetry, music composition, painting creation, etc.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1. **Domain Specific Training**: LLMs are trained on large datasets containing diverse examples from their respective fields (e.g., books for text generation). If CoRaG or similar models were to perform well in creative tasks that require understanding nuances and styles of a domain, they would need extensive training data specific not just within fact-based knowledge but also artistic expression patterns—far beyond standard datasets used by conventional language models.\n",
      "\n",
      "2. **Inherent Structures**: The architecture behind LLMs is specifically designed to mimic human text generation through deep learning techniques and has the capability of generating coherent, contextually relevant responses with minimal supervision after being fine-thy tuned on specific corpora (text data). This inherently generative nature allows for creativity within certain bounds. However, CoRaG may rely more heavily upon retrieving existing knowledge rather than spontaneous generation processes that pure LLMs excel at due to their extensive training in generating text based solely on prompted patterns and contextual cues without the need for fact-specific databases or external references during runtime.\n",
      "\n",
      "3. **Generative Flexibility**: Pure Language Models (LLM), which might not have heavy reliance on prior knowledge, can often explore more abstract connections between concepts due to their generative nature and extensive contextual understanding from training data that covers a wider array of topics than fact-based systems typically focus.\n",
      "\n",
      "4. **Task Complexity**: Creative tasks are inherently complex with less clear metrics for success compared to structured knowledge retrieval, where facts can be definitively correct or incorrect based on evidence (e.g., historical dates). In creativity assessment often relies more subjective criteria like emotional resonance, novelty, and aesthetic value—which pure LLMs do not intrinsiceallly optimize for given their design purpose to generate textually relevant outputs rather than create art or music from scratch.\n",
      "\n",
      "5. **Transferability**: The transfer of success between domains often requires the model's architecture is adept at both understanding and generating based on context (as LLM does), alongside a robust mechanism to draw upon external knowledge when needed—something fact-focused models are proficient in, but not necessarily generative creativity.\n",
      "\n",
      "In summary, while CoRaG or any retrieval system may perform admirably within structured domains that rely on known facts and rules (like a legal database), its design could limit it when faced with the need for original creation—characteristic of pure LLMs which excel in mimicking human-style generative creativity without predefined knowledge. To bridge this gap, hybrid models or entirely new architectures may be required that can handle both structured reasoning and abstract generation effectively — a challenging feat yet an ongoing area of research within AI systems development today.\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "for m in ['deepseek-r1:latest', 'phi3.5:latest']:\n",
    "    for access in [\"KG_ONLY\", \"CHUNKS\", \"FULL\", \"DOCUMENT_LEVEL\"]:\n",
    "        for u in [\"user1\", \"admin\"]:\n",
    "            document_level_result = filter_response_by_access(result.context_data, access, user_id=u)\n",
    "            # Generate prompt\n",
    "            prompt = generate_search_prompt(\n",
    "                context_data=str(document_level_result),\n",
    "                response_type=\"multiple paragraphs\"\n",
    "            )\n",
    "            response = create_llm_response(m, prompt, QUESTIONS[9])\n",
    "            print(f\"Model: {m} - Access: {access} - User: {u}\")\n",
    "            for k in document_level_result.keys():\n",
    "                print(f\"{k}: {len(pd.DataFrame(document_level_result[k]))}\")\n",
    "            print()\n",
    "            print(response)\n",
    "            print('#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['relationships', 'entities'])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_level_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import instructor\n",
    "from instructor import OpenAISchema\n",
    "import openai\n",
    "import chromadb\n",
    "import ollama\n",
    "\n",
    "entities = [\n",
    "    \"PERSON\",\n",
    "    \"ORGANIZATION\",\n",
    "    \"GEO\",\n",
    "    \"EVENT\",\n",
    "    \"MODEL\",\n",
    "    \"TECHNIQUE\",\n",
    "    \"DATASET\",\n",
    "    \"METRIC\",\n",
    "    \"BENCHMARK\",\n",
    "    \"DOMAIN\",\n",
    "    \"FRAMEWORK\",\n",
    "    \"ALGORITHM\",\n",
    "    \"PARAMETER\",\n",
    "    \"OPTIMIZER\",\n",
    "    \"LOSS_FUNCTION\",\n",
    "    \"EMBEDDING\",\n",
    "    \"RETRIEVER\",\n",
    "    \"TUNING_METHOD\"\n",
    "]\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client = instructor.patch(openai.OpenAI())\n",
    "\n",
    "class EntityMatch(OpenAISchema):\n",
    "    \"\"\"Represents a matched entity from the query\"\"\"\n",
    "    query_text: str = Field(description=\"The exact text segment from the query that matches an entity\")\n",
    "    entity_type: str = Field(description=f\"The type of entity ({entities})\")\n",
    "    confidence: float = Field(description=\"Confidence score between 0 and 1\", ge=0, le=1)\n",
    "\n",
    "# Define the query and system prompt\n",
    "system_prompt = \"\"\"You are an expert entity matching system. Extract mentions of predefined entities from user queries.\n",
    "\n",
    "            PREDEFINED ENTITIES:\n",
    "            {entities}\n",
    "\n",
    "            RULES:\n",
    "            1. Only match entities from the provided list - do not suggest new entities\n",
    "            2. Match variations and synonyms of entity names if they clearly refer to the same concept\n",
    "            3. Assign confidence scores based on how certain the match is:\n",
    "            - 1.0: Exact match\n",
    "            - 0.7-0.9: Clear reference but different wording\n",
    "            - 0.5-0.7: Possible match with some ambiguity\n",
    "            4. If no entities are found, return an empty list\n",
    "\"\"\".format(\n",
    "    entities=entities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CORAG', 'MODEL', 0.9)\n",
      "('RAG', 'MODEL', 0.9)\n",
      "('MULTI-HOP REASONING', 'TECHNIQUE', 0.7)\n",
      "('RETRIEVAL LIMITATIONS', 'PARAMETER', 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Make the API call\n",
    "res = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # Make sure to use a valid model name\n",
    "    response_model=List[EntityMatch],\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Extract entities from this query: {QUESTIONS[0]}\"}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for entity in res:\n",
    "    print((entity.query_text.upper(), entity.entity_type, entity.confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded collection (knowledge_graph)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "COLLECTION_NAME = 'knowledge_graph'\n",
    "\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=COLLECTION_NAME)\n",
    "    print(f'Loaded collection ({COLLECTION_NAME})')\n",
    "except:\n",
    "    print('No Collection Found')\n",
    "\n",
    "query_embedding = ollama.embed(model='all-minilm:33m', input=QUESTIONS[0])\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding.embeddings,\n",
    "    n_results=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default flashrank model for language en\n",
      "Default Model: ms-marco-MiniLM-L-12-v2\n",
      "Loading FlashRankRanker model ms-marco-MiniLM-L-12-v2 (this message can be suppressed by setting verbose=0)\n",
      "Loading model FlashRank model ms-marco-MiniLM-L-12-v2...\n"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "ranker = Reranker('flashrank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 3 - CoRAG utilizes retrieval chains to enhance its multi-hop question answering capabilities\n",
      "Rank: 4 - CoRAG improves upon RAG by dynamically reformulating queries for better retrieval\n",
      "Rank: 2 - CoRAG is particularly effective in the domain of multi-hop question answering, where it retrieves and reasons over multiple information sources.\n",
      "Rank: 8 - CoRAG-8B utilizes retrieval chains to enhance its performance in multi-hop question answering tasks\n",
      "Rank: 11 - CoRAG is a specific implementation of the retrieval-augmented generation framework, improving upon traditional methods.\n",
      "Rank: 9 - RAG and CoRAG are related entities in the field of retrieval-augmented generation. The CoRAG framework outlines essential components for generating retrieval chains and training models within the RAG paradigm. CoRAG serves as an enhanced version of RAG, specifically designed to improve the retrieval process by integrating a chain-of-retrieval mechanism. This advancement allows for more efficient and effective information retrieval, thereby enhancing the overall performance of retrieval-augmented generation tasks.\n",
      "Rank: 1 - CoRAG is evaluated on the NQ dataset, showcasing its capabilities in multi-hop reasoning tasks.\n",
      "Rank: 0 - CoRAG is tested on the 2WikiMultihopQA dataset to evaluate its multi-hop reasoning capabilities\n",
      "Rank: 10 - CoRAG is designed to enhance the capabilities of large language models by teaching them iterative retrieval and reasoning\n",
      "Rank: 12 - This community centers around the CoRAG-8B model and its interactions with various multi-hop question answering datasets, including 2WikiMultihopQA and Bamboogle. The relationships among these entities highlight the importance of model evaluation and performance in complex reasoning tasks.\n",
      "Rank: 6 - The community centers around the development and evaluation of advanced models for multi-hop question answering, particularly focusing on CoRAG and its associated datasets and methodologies. Key entities include Microsoft Corporation, Renmin University of China, and various datasets that facilitate the assessment of multi-hop reasoning capabilities.\n",
      "Rank: 5 - The community centers around CoRAG, a sophisticated model for multi-hop question answering, developed in collaboration with Microsoft Corporation and Renmin University of China. Key entities include various decoding strategies and metrics that enhance the model's performance and efficiency, indicating a strong interconnection among them.\n",
      "Rank: 14 - The community centers around CoRAG, which employs various decoding strategies such as token consumption, greedy decoding, and best-of-N sampling to enhance its performance in generating answers. The relationships among these entities highlight their interdependence in optimizing model efficiency and retrieval quality.\n",
      "Rank: 7 - CoRAG is assessed using the Bamboogle dataset to evaluate its performance in complex reasoning tasks\n",
      "Rank: 13 - The CoRAG Development Community consists of key entities involved in the creation and enhancement of the CoRAG model, including Microsoft Corporation and Renmin University of China. These entities collaborate on various aspects of multi-hop question answering, utilizing advanced techniques such as multi-task learning and tree search to improve the model's performance.\n"
     ]
    }
   ],
   "source": [
    "rr_results = ranker.rank(query=QUESTIONS[0], docs=results['documents'][0], doc_ids=[x for x in range(len(results['documents'][0]))])\n",
    "for r in rr_results.results:\n",
    "    print(f\"Rank: {r.doc_id} - {r.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>Large Language Models and Their Ecosystem</td>\n",
       "      <td># Large Language Models and Their Ecosystem\\n\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      title  \\\n",
       "0  20  Large Language Models and Their Ecosystem   \n",
       "\n",
       "                                             content  \n",
       "0  # Large Language Models and Their Ecosystem\\n\\...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --> Communities\n",
    "result.context_data['reports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>links</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>633</td>\n",
       "      <td>CORAG</td>\n",
       "      <td>RETRIEVAL-AUGMENTED GENERATION</td>\n",
       "      <td>CoRAG is a specific implementation of the retr...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>638</td>\n",
       "      <td>RAG</td>\n",
       "      <td>CORAG</td>\n",
       "      <td>RAG and CoRAG are related entities in the fiel...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>636</td>\n",
       "      <td>RAG</td>\n",
       "      <td>LONGRAG</td>\n",
       "      <td>LongRAG posits that RAG performance can be imp...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id source                          target  \\\n",
       "0  633  CORAG  RETRIEVAL-AUGMENTED GENERATION   \n",
       "1  638    RAG                           CORAG   \n",
       "2  636    RAG                         LONGRAG   \n",
       "\n",
       "                                         description weight links  in_context  \n",
       "0  CoRAG is a specific implementation of the retr...    9.0     1        True  \n",
       "1  RAG and CoRAG are related entities in the fiel...    9.0     1        True  \n",
       "2  LongRAG posits that RAG performance can be imp...    6.0     1        True  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search By id\n",
    "result.context_data['relationships'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity</th>\n",
       "      <th>description</th>\n",
       "      <th>number of relationships</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641</td>\n",
       "      <td>CORAG</td>\n",
       "      <td>CoRAG is a sophisticated model designed for mu...</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>660</td>\n",
       "      <td>RAG</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a doma...</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>RETRIEVAL-AUGMENTED GENERATION</td>\n",
       "      <td>Retrieval-Augmented Generation (RAG) is a soph...</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                          entity  \\\n",
       "0  641                           CORAG   \n",
       "1  660                             RAG   \n",
       "2   37  RETRIEVAL-AUGMENTED GENERATION   \n",
       "\n",
       "                                         description number of relationships  \\\n",
       "0  CoRAG is a sophisticated model designed for mu...                      23   \n",
       "1  Retrieval-Augmented Generation (RAG) is a doma...                       8   \n",
       "2  Retrieval-Augmented Generation (RAG) is a soph...                      40   \n",
       "\n",
       "   in_context  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entities\n",
    "result.context_data['entities'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>next-token prediction objective within a mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>Chain-of-Retrieval Augmented Generation\\n\\nLia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>for both rejection sampling and test-time dec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0  43   next-token prediction objective within a mult...\n",
       "1  41  Chain-of-Retrieval Augmented Generation\\n\\nLia...\n",
       "2  51   for both rejection sampling and test-time dec..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --> Chunks + 1\n",
    "result.context_data['sources'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>combined_degree</th>\n",
       "      <th>text_unit_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d9742a40-1414-4e30-9d1b-2cfc070a972e</td>\n",
       "      <td>0</td>\n",
       "      <td>VENKATESH BALAVADHANI PARTHASARATHY</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>Venkatesh Balavadhani Parthasarathy is affilia...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[9ed73c3fa03f83895a135beea0b9167417e1259afd8d6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4860f12a-0b4f-49d1-a968-11e066ac2721</td>\n",
       "      <td>1</td>\n",
       "      <td>AHTSHAM ZAFAR</td>\n",
       "      <td>CEADAR</td>\n",
       "      <td>Ahtsham Zafar is affiliated with CeADAR, contr...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[9ed73c3fa03f83895a135beea0b9167417e1259afd8d6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  \\\n",
       "0  d9742a40-1414-4e30-9d1b-2cfc070a972e                  0   \n",
       "1  4860f12a-0b4f-49d1-a968-11e066ac2721                  1   \n",
       "\n",
       "                                source  target  \\\n",
       "0  VENKATESH BALAVADHANI PARTHASARATHY  CEADAR   \n",
       "1                        AHTSHAM ZAFAR  CEADAR   \n",
       "\n",
       "                                         description  weight  combined_degree  \\\n",
       "0  Venkatesh Balavadhani Parthasarathy is affilia...     8.0                6   \n",
       "1  Ahtsham Zafar is affiliated with CeADAR, contr...     8.0                6   \n",
       "\n",
       "                                       text_unit_ids  \n",
       "0  [9ed73c3fa03f83895a135beea0b9167417e1259afd8d6...  \n",
       "1  [9ed73c3fa03f83895a135beea0b9167417e1259afd8d6...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relationship_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arangodb_queries import *\n",
    "\n",
    "# Initialize the ArangoDB client\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")\n",
    "\n",
    "# Connect to \"_system\" database as root user\n",
    "sys_db = client.db(\"_system\", username=\"root\", password=\"root\")\n",
    "\n",
    "# Create a new database if it doesn't exist\n",
    "if not sys_db.has_database(\"knowledge_graph\"):\n",
    "    sys_db.create_database(\"knowledge_graph\")\n",
    "\n",
    "# Connect to the knowledge_graph database\n",
    "db = client.db(\"knowledge_graph\", username=\"root\", password=\"root\")\n",
    "\n",
    "# Get the community with human_readable_id \"20\"\n",
    "communities = get_communities_by_human_readable_ids([20, 21, 22], db)\n",
    "relationships = get_relationships_by_human_readable_ids([633, 638], db)\n",
    "entities = get_entities_by_human_readable_ids([1,2,3], db)\n",
    "chunks = get_chunks_by_human_readable_ids([1,2,3], db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'human_readable_id': 20, 'id': '6c9c1211-2f9f-436f-9234-19f153a4f320'}, {'human_readable_id': 21, 'id': '32432a3c-8507-4581-83c1-3cec0b8c61bd'}, {'human_readable_id': 22, 'id': '06795090-5cef-408c-9c28-384add4a608c'}]\n",
      "[{'human_readable_id': 633, 'id': '77faed0e-660a-4384-9470-cdb94378c437'}, {'human_readable_id': 638, 'id': '210ab9e3-1d6b-4424-b033-ada92ace9e40'}]\n",
      "[{'human_readable_id': 1, 'id': 'b131aa10-5aae-4723-8915-61ae01d3bddf'}, {'human_readable_id': 2, 'id': 'b06d9c88-a62b-4117-97e3-4aa390f13fce'}, {'human_readable_id': 3, 'id': '00a23f02-7c93-41fc-a0b9-9302b4f4d417'}]\n",
      "[{'human_readable_id': 1, 'id': '9ed73c3fa03f83895a135beea0b9167417e1259afd8d6a2995037542ed2b04b6c11d37acdfa4bd0c280f586919f89c51a6bab22295153fd5f74f0faaccffd271'}, {'human_readable_id': 2, 'id': 'adb58fb2ac4dc2e636420a23765cd5f718dd4f88e00555d7aa84533fc852b568bd9b80d652ce717022631ce0aeb9d9b625d6d8020e55b7eb40d912c43dc13616'}, {'human_readable_id': 3, 'id': '759dea32dd8bddec032bdfe47d91384a521198df10389687cc5b0da299074cfd1fff3f77339d66350d9526343f1e8ade6a97454329ddb15cc98b41cfd9bbb2fd'}]\n"
     ]
    }
   ],
   "source": [
    "print(communities)\n",
    "print(relationships)\n",
    "print(entities)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
