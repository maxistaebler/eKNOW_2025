{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../src')\n",
    "sys.path.insert(1, '../prompts')\n",
    "sys.path.insert(1, '../input')\n",
    "\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import json\n",
    "import pandas as pd\n",
    "from arango import ArangoClient\n",
    "from arango import DocumentInsertError\n",
    "import chromadb\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF Files (optinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianstaebler/code/PHD/RESEARCH_PAPER/eKNOW_2025/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PdfProcessor import PdfProcessor\n",
    "# For text output\n",
    "processor = PdfProcessor(output_dir=\"../input/txt\")\n",
    "processor.process_pdfs(output_format=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_parquet('../benchmark/output/create_final_entities.parquet')\n",
    "relationships = pd.read_parquet('../benchmark/output/create_final_relationships.parquet')\n",
    "community_reports = pd.read_parquet('../benchmark/output/create_final_community_reports.parquet')\n",
    "communities = pd.read_parquet('../benchmark/output/create_final_communities.parquet')\n",
    "text_units = pd.read_parquet('../benchmark/output/create_final_text_units.parquet')\n",
    "documents = pd.read_parquet('../benchmark/output/create_final_documents.parquet')\n",
    "# nodes = pd.read_parquet('../ragtest/output/create_final_nodes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>human_readable_id</th>\n",
       "      <th>community</th>\n",
       "      <th>parent</th>\n",
       "      <th>level</th>\n",
       "      <th>title</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>relationship_ids</th>\n",
       "      <th>text_unit_ids</th>\n",
       "      <th>period</th>\n",
       "      <th>...</th>\n",
       "      <th>level_reports</th>\n",
       "      <th>title_reports</th>\n",
       "      <th>summary</th>\n",
       "      <th>full_content</th>\n",
       "      <th>rank</th>\n",
       "      <th>rank_explanation</th>\n",
       "      <th>findings</th>\n",
       "      <th>full_content_json</th>\n",
       "      <th>period_reports</th>\n",
       "      <th>size_reports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63c7cb5c-d7d6-44e3-ad30-5bfc94fa5ed7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Community 0</td>\n",
       "      <td>[db22651f-2b32-4135-b485-c25a7f724304, 12b4db9...</td>\n",
       "      <td>[1ff51675-7d63-411e-956e-13c691786300, 23fa4e5...</td>\n",
       "      <td>[6089c9eecfbecf037f199213619a90f7cbe8351d933f7...</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hugging Face and NeurIPS Community</td>\n",
       "      <td>The community is centered around Hugging Face,...</td>\n",
       "      <td># Hugging Face and NeurIPS Community\\n\\nThe co...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'Hugging Face is a key player...</td>\n",
       "      <td>{\\n    \"title\": \"Hugging Face and NeurIPS Comm...</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c55fc2c-7565-4c65-a802-81e5c69487c0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Community 1</td>\n",
       "      <td>[5557962d-42e4-46db-a3d0-d77ce972dac3, dc2ad48...</td>\n",
       "      <td>[1ccb53c6-2e14-49a8-ab0b-66f4d8964177, 1f67be3...</td>\n",
       "      <td>[3a7cfdfe2513989b5be63087d2f81ce884640a0f191aa...</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>AI, CO₂ Emissions, and Nuclear Energy</td>\n",
       "      <td>This community encompasses the interplay betwe...</td>\n",
       "      <td># AI, CO₂ Emissions, and Nuclear Energy\\n\\nThi...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>The impact severity rating is high due to the ...</td>\n",
       "      <td>[{'explanation': 'Artificial Intelligence (AI)...</td>\n",
       "      <td>{\\n    \"title\": \"AI, CO₂ Emissions, and Nuclea...</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  human_readable_id  community  parent  \\\n",
       "0  63c7cb5c-d7d6-44e3-ad30-5bfc94fa5ed7                  0          0      -1   \n",
       "1  3c55fc2c-7565-4c65-a802-81e5c69487c0                  1          1      -1   \n",
       "\n",
       "   level        title                                         entity_ids  \\\n",
       "0      0  Community 0  [db22651f-2b32-4135-b485-c25a7f724304, 12b4db9...   \n",
       "1      0  Community 1  [5557962d-42e4-46db-a3d0-d77ce972dac3, dc2ad48...   \n",
       "\n",
       "                                    relationship_ids  \\\n",
       "0  [1ff51675-7d63-411e-956e-13c691786300, 23fa4e5...   \n",
       "1  [1ccb53c6-2e14-49a8-ab0b-66f4d8964177, 1f67be3...   \n",
       "\n",
       "                                       text_unit_ids      period  ...  \\\n",
       "0  [6089c9eecfbecf037f199213619a90f7cbe8351d933f7...  2025-01-30  ...   \n",
       "1  [3a7cfdfe2513989b5be63087d2f81ce884640a0f191aa...  2025-01-30  ...   \n",
       "\n",
       "   level_reports                          title_reports  \\\n",
       "0              0     Hugging Face and NeurIPS Community   \n",
       "1              0  AI, CO₂ Emissions, and Nuclear Energy   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The community is centered around Hugging Face,...   \n",
       "1  This community encompasses the interplay betwe...   \n",
       "\n",
       "                                        full_content  rank  \\\n",
       "0  # Hugging Face and NeurIPS Community\\n\\nThe co...   8.5   \n",
       "1  # AI, CO₂ Emissions, and Nuclear Energy\\n\\nThi...   7.5   \n",
       "\n",
       "                                    rank_explanation  \\\n",
       "0  The impact severity rating is high due to the ...   \n",
       "1  The impact severity rating is high due to the ...   \n",
       "\n",
       "                                            findings  \\\n",
       "0  [{'explanation': 'Hugging Face is a key player...   \n",
       "1  [{'explanation': 'Artificial Intelligence (AI)...   \n",
       "\n",
       "                                   full_content_json  period_reports  \\\n",
       "0  {\\n    \"title\": \"Hugging Face and NeurIPS Comm...      2025-01-30   \n",
       "1  {\\n    \"title\": \"AI, CO₂ Emissions, and Nuclea...      2025-01-30   \n",
       "\n",
       "  size_reports  \n",
       "0           22  \n",
       "1           34  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_merged = pd.merge(\n",
    "    communities, \n",
    "    community_reports, \n",
    "    on='community',           # Column to merge on\n",
    "    how='outer',             # Keep all rows from both dataframes\n",
    "    suffixes=('', '_reports'), # Add suffixes to distinguish duplicate column names\n",
    ")\n",
    "\n",
    "communities_merged.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ArangoDB client\n",
    "client = ArangoClient(hosts=\"http://localhost:8529\")\n",
    "\n",
    "# Connect to \"_system\" database as root user\n",
    "sys_db = client.db(\"_system\", username=\"root\", password=\"root\")\n",
    "\n",
    "# Create a new database if it doesn't exist\n",
    "if not sys_db.has_database(\"benchmark\"):\n",
    "    sys_db.create_database(\"benchmark\")\n",
    "\n",
    "# Connect to the knowledge_graph database\n",
    "db = client.db(\"benchmark\", username=\"root\", password=\"root\")\n",
    "\n",
    "# Create document collections if they don't exist\n",
    "for collection_name in [\"chunks\", \"entities\", \"documents\", \"communities\"]:\n",
    "    if not db.has_collection(collection_name):\n",
    "        db.create_collection(collection_name)\n",
    "\n",
    "# Create edge collections if they don't exist\n",
    "for edge_collection in [\"relationships\", \"chunk_contains\", \"community_contains\", \"document_contains\"]:\n",
    "    if not db.has_collection(edge_collection):\n",
    "        db.create_collection(edge_collection, edge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create document with custom _key\n",
    "def create_update_document(collection, doc, id_field='id'):\n",
    "    doc['_key'] = doc[id_field]  # Use id as _key\n",
    "    return collection.insert(doc, overwrite_mode='update')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert documents\n",
    "documents_collection = db.collection('documents')\n",
    "for _, row in documents.iterrows():\n",
    "    doc_doc = {\n",
    "        'id': row['id'],\n",
    "        'human_readable_id': row['human_readable_id'],\n",
    "        'title': row['title'],\n",
    "        'text': row['text']\n",
    "    }\n",
    "    create_update_document(documents_collection, doc_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Chunks\n",
    "chunks_collection = db.collection('chunks')\n",
    "for _, row in text_units.iterrows():\n",
    "    chunk_doc = {\n",
    "        'id': row['id'],\n",
    "        'human_readable_id': row['human_readable_id'],\n",
    "        'text': row['text'],\n",
    "        'document_id': row['document_ids'][0],\n",
    "        'n_tokens': row['n_tokens']\n",
    "    }\n",
    "    create_update_document(chunks_collection, chunk_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id_mapping = {\n",
    "    row['id']: row['title'] for _, row in entities.iterrows()\n",
    "}\n",
    "entity_id_mapping_r = {\n",
    "    row['title']: row['id'] for _, row in entities.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert entities\n",
    "entities = entities[\n",
    "    (entities['title'].notna()) & (entities['title'] != '') &\n",
    "    (entities['type'].notna()) & (entities['type'] != '')\n",
    "]\n",
    "\n",
    "entities_collection = db.collection('entities')\n",
    "for _, row in entities.iterrows():\n",
    "    entity_doc = {\n",
    "        'id': row['id'],\n",
    "        'human_readable_id': row['human_readable_id'],\n",
    "        'title': row['title'],\n",
    "        'type': row['type'],\n",
    "        'description': row['description']\n",
    "    }\n",
    "    create_update_document(entities_collection, entity_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert communities\n",
    "communities_collection = db.collection('communities')\n",
    "for _, row in communities_merged.iterrows():\n",
    "    community_doc = {\n",
    "        'id': row['id'],\n",
    "        'human_readable_id': row['human_readable_id'],\n",
    "        'community': row['community'],\n",
    "        'parent': row['parent'],\n",
    "        'level': row['level'],\n",
    "        'period': row['period'],\n",
    "        'rank': float(row['rank']),\n",
    "        'size': int(row['size']),\n",
    "        'title': row['title_reports'],\n",
    "        'summary': row['summary'],\n",
    "        'full_content': row['full_content'],\n",
    "        'rank_explanation': row['rank_explanation'],\n",
    "        'findings': list(row['findings'])\n",
    "    }\n",
    "    create_update_document(communities_collection, community_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realtionships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relationships between entities\n",
    "relationships_collection = db.collection('relationships')\n",
    "for _, row in relationships.iterrows():\n",
    "    rel_doc = {\n",
    "        '_from': f\"entities/{entity_id_mapping_r[row['source']]}\",\n",
    "        '_to': f\"entities/{entity_id_mapping_r[row['target']]}\",\n",
    "        'id': row['id'],\n",
    "        'human_readable_id': row['human_readable_id'],\n",
    "        'type': 'relationship',\n",
    "        'description': row['description'],\n",
    "        'weight': float(row['weight']),\n",
    "        'combined_degree': int(row['combined_degree'])\n",
    "    }\n",
    "    create_update_document(relationships_collection, rel_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash(edge):\n",
    "    dict_str = json.dumps(edge, sort_keys=True)\n",
    "    hash_object = hashlib.sha256(dict_str.encode())\n",
    "    hex_hash = hash_object.hexdigest()\n",
    "    return hex_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relationships between entities\n",
    "text_units = text_units.dropna(subset=['relationship_ids'])\n",
    "\n",
    "chunk_contains = db.collection('chunk_contains')\n",
    "for _, row in text_units.iterrows():\n",
    "    # Link chunks to entities\n",
    "    for entity_id in row['entity_ids']:\n",
    "        edge = {\n",
    "            '_from': f\"chunks/{row['id']}\",\n",
    "            '_to': f\"entities/{entity_id}\",\n",
    "            'type': 'contains_entity',\n",
    "            'weight': float(1),\n",
    "        }\n",
    "        edge['id'] = create_hash(edge)\n",
    "        create_update_document(chunk_contains, edge)\n",
    "    \n",
    "    # Link chunks to documents  \n",
    "    for doc_id in row['document_ids']:\n",
    "        edge = {\n",
    "            '_from': f\"chunks/{row['id']}\",\n",
    "            '_to': f\"documents/{doc_id}\",\n",
    "            'type': 'part_of_document',\n",
    "            'weight': float(1),\n",
    "        }\n",
    "        edge['id'] = create_hash(edge)\n",
    "        create_update_document(chunk_contains, edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Community Connections\n",
    "\n",
    "community_contains = db.collection('community_contains')\n",
    "for _, row in communities_merged.iterrows():\n",
    "    # Link communities to entities\n",
    "    for entity_id in row['entity_ids']:\n",
    "        edge = {\n",
    "            '_from': f\"communities/{row['id']}\",\n",
    "            '_to': f\"entities/{entity_id}\",\n",
    "            'type': 'community_entity',\n",
    "            'weight': float(1),\n",
    "        }\n",
    "        edge['id'] = create_hash(edge)\n",
    "        create_update_document(community_contains, edge)\n",
    "    \n",
    "    # Link communities to chunks\n",
    "    for chunk in row['text_unit_ids']:\n",
    "        edge = {\n",
    "            '_from': f\"communities/{row['id']}\",\n",
    "            '_to': f\"chunks/{chunk}\",\n",
    "            'type': 'community_chunk',\n",
    "            'weight': float(1),\n",
    "        }\n",
    "        edge['id'] = create_hash(edge)\n",
    "        create_update_document(community_contains, edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Community Connections\n",
    "document_contains = db.collection('document_contains')\n",
    "for _, row in text_units.iterrows():\n",
    "    edge = {\n",
    "        '_from': f\"chunks/{row['id']}\",\n",
    "        '_to': f\"documents/{row['document_ids'][0]}\",\n",
    "        'type': 'part_of_document',\n",
    "        'weight': float(1),\n",
    "    }\n",
    "    edge['id'] = create_hash(edge)\n",
    "    create_update_document(document_contains, edge)\n",
    "    # Link entities to documents\n",
    "    for entity_id in row['entity_ids']:\n",
    "        edge = {\n",
    "            '_from': f\"entities/{entity_id}\",\n",
    "            '_to': f\"documents/{row['document_ids'][0]}\",\n",
    "            'type': 'part_of_document',\n",
    "            'weight': float(1),\n",
    "        }\n",
    "        edge['id'] = create_hash(edge)\n",
    "        create_update_document(document_contains, edge)\n",
    "    \n",
    "    # Link relationships to documents\n",
    "    for relationship_id in row['relationship_ids']:\n",
    "        edge = {\n",
    "            '_from': f\"relationships/{relationship_id}\",\n",
    "            '_to': f\"documents/{row['document_ids'][0]}\",\n",
    "            'type': 'part_of_document',\n",
    "            'weight': float(1),\n",
    "        }\n",
    "        edge['id'] = create_hash(edge)\n",
    "        create_update_document(document_contains, edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph if it doesn't exist\n",
    "if not db.has_graph(\"benchmark\"):\n",
    "    graph = db.create_graph(\"benchmark\")\n",
    "else:\n",
    "    graph = db.graph(\"benchmark\")\n",
    "\n",
    "# Define edge definitions for the graph\n",
    "edge_definitions = [\n",
    "    {\n",
    "        # Edge collection for relationships between entities\n",
    "        \"edge_collection\": \"relationships\",\n",
    "        \"from_vertex_collections\": [\"entities\"],\n",
    "        \"to_vertex_collections\": [\"entities\"]\n",
    "    },\n",
    "    {\n",
    "        # Edge collection for chunks containing entities/relationships\n",
    "        \"edge_collection\": \"chunk_contains\",\n",
    "        \"from_vertex_collections\": [\"chunks\"],\n",
    "        \"to_vertex_collections\": [\"entities\"]\n",
    "    },\n",
    "    {\n",
    "        # Edge collection for communities containing entities/relationships\n",
    "        \"edge_collection\": \"community_contains\",\n",
    "        \"from_vertex_collections\": [\"communities\"],\n",
    "        \"to_vertex_collections\": [\"entities\", \"chunks\"]\n",
    "    },\n",
    "    {\n",
    "        # Edge collection for communities containing entities/relationships\n",
    "        \"edge_collection\": \"document_contains\",\n",
    "        \"from_vertex_collections\": [\"communities\", \"chunks\", \"entities\"],\n",
    "        \"to_vertex_collections\": [\"documents\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Add or update edge definitions\n",
    "for edge_def in edge_definitions:\n",
    "    if graph.has_edge_definition(edge_def[\"edge_collection\"]):\n",
    "        graph.replace_edge_definition(\n",
    "            edge_collection=edge_def[\"edge_collection\"],\n",
    "            from_vertex_collections=edge_def[\"from_vertex_collections\"],\n",
    "            to_vertex_collections=edge_def[\"to_vertex_collections\"]\n",
    "        )\n",
    "    else:\n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=edge_def[\"edge_collection\"],\n",
    "            from_vertex_collections=edge_def[\"from_vertex_collections\"],\n",
    "            to_vertex_collections=edge_def[\"to_vertex_collections\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 1350 nodes and 3134 edges to ../gephi\n"
     ]
    }
   ],
   "source": [
    "# Query ArangoDB and create Gephi-compatible CSV files\n",
    "def export_graph_for_gephi(db, output_dir='../gephi'):\n",
    "    \"\"\"\n",
    "    Export ArangoDB graph to Gephi-compatible CSV files.\n",
    "    Creates two files:\n",
    "    - nodes.csv: Contains all vertices with their properties\n",
    "    - edges.csv: Contains all edges with their properties\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import csv\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Export nodes\n",
    "    nodes_query = \"\"\"\n",
    "    LET entities = (FOR v IN entities RETURN MERGE(v, {category: 'entity'}))\n",
    "    LET chunks = (FOR v IN chunks RETURN MERGE(v, {category: 'chunk'}))\n",
    "    LET documents = (FOR v IN documents RETURN MERGE(v, {category: 'document'}))\n",
    "    LET communities = (FOR v IN communities RETURN MERGE(v, {category: 'community'}))\n",
    "    \n",
    "    FOR v IN APPEND(entities, APPEND(chunks, APPEND(documents, communities)))\n",
    "        RETURN {\n",
    "            Id: v._key,\n",
    "            Label: v.title || v.text || v.id,\n",
    "            Category: v.category,\n",
    "            Type: v.type || '',\n",
    "            Weight: v.weight || 1.0\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Export edges\n",
    "    edges_query = \"\"\"\n",
    "    LET relationships = (\n",
    "        FOR e IN relationships \n",
    "        RETURN MERGE(e, {category: 'relationship'})\n",
    "    )\n",
    "    LET chunk_contains = (\n",
    "        FOR e IN chunk_contains \n",
    "        RETURN MERGE(e, {category: 'chunk_contains'})\n",
    "    )\n",
    "    LET community_contains = (\n",
    "        FOR e IN community_contains \n",
    "        RETURN MERGE(e, {category: 'community_contains'})\n",
    "    )\n",
    "    \n",
    "    FOR e IN APPEND(relationships, APPEND(chunk_contains, community_contains))\n",
    "        RETURN {\n",
    "            Source: SPLIT(e._from, '/')[1],\n",
    "            Target: SPLIT(e._to, '/')[1],\n",
    "            Type: e.type,\n",
    "            Weight: e.weight || 1.0,\n",
    "            Category: e.category\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute queries\n",
    "    nodes = list(db.aql.execute(nodes_query))\n",
    "    edges = list(db.aql.execute(edges_query))\n",
    "    \n",
    "    # Write nodes CSV\n",
    "    with open(f'{output_dir}/nodes_benchmark.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['Id', 'Label', 'Category', 'Type', 'Weight'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(nodes)\n",
    "    \n",
    "    # Write edges CSV\n",
    "    with open(f'{output_dir}/edges_benchmark.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=['Source', 'Target', 'Type', 'Weight', 'Category'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(edges)\n",
    "    \n",
    "    print(f\"Exported {len(nodes)} nodes and {len(edges)} edges to {output_dir}\")\n",
    "    return nodes, edges\n",
    "\n",
    "# Export the graph\n",
    "nodes, edges = export_graph_for_gephi(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Version Deleted - Create New Collection\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "COLLECTION_NAME = 'knowledge_graph'\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=COLLECTION_NAME)\n",
    "    chroma_client.delete_collection(name=COLLECTION_NAME)\n",
    "    print('Old Version Deleted - Create New Collection')\n",
    "    collection = chroma_client.create_collection(name=COLLECTION_NAME)\n",
    "except:\n",
    "    print('No Collection Found - Create New Collection')\n",
    "    collection = chroma_client.create_collection(name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_units['text'].tolist()\n",
    "chunk_embeddings = ollama.embed(model='all-minilm:33m', input=chunks)\n",
    "\n",
    "communities_summary = communities_merged['summary'].tolist()\n",
    "# communities_full_content = communities_merged['full_content'].tolist()\n",
    "communities_summary__embeddings = ollama.embed(model='all-minilm:33m', input=communities_summary)\n",
    "# communities_full_content__embeddings = ollama.embed(model='all-minilm:33m', input=communities_full_content)\n",
    "\n",
    "relationships_description = relationships['description'].tolist()\n",
    "relationships_description__embeddings = ollama.embed(model='all-minilm:33m', input=relationships_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_metadata = []\n",
    "for idx, row in text_units.iterrows():\n",
    "    chunk_metadata.append({\n",
    "        \"id\": row['id'],\n",
    "        'human_readable_id': row['human_readable_id'],\n",
    "        'n_tokens': row['n_tokens']\n",
    "    })\n",
    "\n",
    "communities_metadata = []\n",
    "for idx, row in communities_merged.iterrows():\n",
    "    communities_metadata.append({\n",
    "        \"id\": row['id'],\n",
    "        'title': row['title_reports'],\n",
    "        'rank': row['rank']\n",
    "    })\n",
    "\n",
    "relationships_metadata = []\n",
    "for idx, row in relationships.iterrows():\n",
    "    relationships_metadata.append({\n",
    "        \"id\": row['id'],\n",
    "        'source': row['source'],\n",
    "        'target': row['target']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, e, m in zip([chunks, communities_summary, relationships_description], [chunk_embeddings, communities_summary__embeddings, relationships_description__embeddings], [chunk_metadata, communities_metadata, relationships_metadata]):\n",
    "    collection.add(\n",
    "        documents=d,\n",
    "        embeddings=e.embeddings,\n",
    "        metadatas=m,\n",
    "        ids=[x['id'] for x in m]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded collection (knowledge_graph)\n",
      "52279d41-3e65-4cbf-af78-fca24980fb56 CoRAG is a newly established model that demonstrates state-of-the-art performance on the KILT benchmark, showcasing its effectiveness in handling knowledge-intensive tasks. The evaluation of CoRAG using the KILT benchmark highlights its capabilities across a variety of such tasks, confirming its advanced performance in this domain.\n",
      "f8ce92bc-9661-414d-b2bf-c6fa567561df KILT-RAG is evaluated on the KILT benchmark, showcasing its performance across various tasks\n",
      "77a4ce0c-85cc-41a1-a1ad-87afc791cad5 Tim Rocktäschel is one of the authors of the KILT benchmark paper\n",
      "37237528-964b-429f-a6f6-356576598fb8 James Thorne is one of the authors of the KILT benchmark paper\n",
      "4c797b87-54b3-4af4-b037-16749d687630 Nicola De Cao is one of the authors of the KILT benchmark paper\n",
      "a21f4792-6029-479d-8532-2a4821bbf706 The KILT training set is part of the KILT benchmark used for training models\n",
      "a81d336b-2d27-4d90-bcdf-aa0645a0435b Patrick Lewis is one of the authors of the KILT benchmark paper\n",
      "511157a6-e88c-48e6-8c17-c79403558a87 Yacine Jernite is one of the authors of the KILT benchmark paper\n",
      "82d70fbe-09e3-4b26-995e-39670a278b6e The KILT benchmark encompasses various tasks and datasets for evaluating knowledge-intensive models\n",
      "20b90fa5-96db-4c4b-a5c3-81d70eaeee25 Bamboogle is part of the evaluation process for the KILT benchmark, consisting of questions reserved for evaluation\n",
      "5767f438-9b39-489c-81c0-366af7c2a885 Angela Fan is one of the authors of the KILT benchmark paper\n",
      "c76d2db0-bb51-4ffc-b165-fc32a3f77dce Vassilis Plachouras is one of the authors of the KILT benchmark paper\n",
      "13347217-ae29-4844-b8f0-09afe2ba6381 The KILT leaderboard ranks models based on their performance on the KILT benchmark\n",
      "c2a724c9-4e8b-471d-8233-66ad34ca35aa Majid Yazdani is one of the authors of the KILT benchmark paper\n",
      "1327a287-c60c-4fbc-9395-090421c69fc8 Jean Maillard is one of the authors of the KILT benchmark paper\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "COLLECTION_NAME = 'knowledge_graph'\n",
    "\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=COLLECTION_NAME)\n",
    "    print(f'Loaded collection ({COLLECTION_NAME})')\n",
    "except:\n",
    "    print('No Collection Found')\n",
    "\n",
    "query = \"CoRAG framework and the KILT benchmark\"\n",
    "query_embedding = ollama.embed(model='all-minilm:33m', input=query)\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding.embeddings,\n",
    "    n_results=15,\n",
    ")\n",
    "for rd, re in zip(results['ids'][0], results['documents'][0]):\n",
    "    print(rd, re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
