Id,Label,Category,Type,Weight
199bfae9-7e0b-4f17-9e73-a96d75e3ef81,VENKATESH BALAVADHANI PARTHASARATHY,entity,PERSON,1
b131aa10-5aae-4723-8915-61ae01d3bddf,AHTSHAM ZAFAR,entity,PERSON,1
b06d9c88-a62b-4117-97e3-4aa390f13fce,AAFAQ KHAN,entity,PERSON,1
00a23f02-7c93-41fc-a0b9-9302b4f4d417,ARSALAN SHAHID,entity,PERSON,1
03edc4c2-2354-45c1-a651-f8eec4049b59,CEADAR,entity,ORGANIZATION,1
0285a5b0-e8e0-4d53-9715-f8f6863c92ac,UNIVERSITY COLLEGE DUBLIN,entity,ORGANIZATION,1
b511a3bb-a065-44d4-83c0-4b082463bcfd,LARGE LANGUAGE MODELS,entity,MODEL,1
1a0fd8e3-7161-4cf7-9349-801984a76324,NATURAL LANGUAGE PROCESSING,entity,DOMAIN,1
89e5473e-92b2-4beb-b8d4-b417378c3cc6,LOW-RANK ADAPTATION,entity,TECHNIQUE,1
aece9995-d7f3-44c1-91b4-7a235b28b2a2,HALF FINE-TUNING,entity,TECHNIQUE,1
1a8462be-8626-45ee-b568-320fa4048551,PROXIMAL POLICY OPTIMISATION,entity,ALGORITHM,1
ee27fa84-d930-44dd-85a9-b796bdd659c6,DIRECT PREFERENCE OPTIMISATION,entity,ALGORITHM,1
64aca0f9-73da-469d-b700-942c0e0e0f73,MIXTURE OF EXPERTS,entity,MODEL,1
2f900b37-65e7-47ad-af1c-0d65e3002e5c,MIXTURE OF AGENTS,entity,MODEL,1
86f9a489-361b-4f66-88f6-ea5c70d33da5,HYPERPARAMETER TUNING,entity,TECHNIQUE,1
793877c3-e6a2-41da-ad4c-7d6315cfb560,DATA PREPARATION,entity,TECHNIQUE,1
fd7e06b9-24fa-4095-bd6d-808662e8ee13,MODEL DEPLOYMENT,entity,EVENT,1
c1cc69fc-eb88-4444-b5ab-5f4c9cd8cc45,VALIDATION FRAMEWORKS,entity,FRAMEWORK,1
82a60878-6ee8-46b3-8d3f-0b97a9884815,MULTIMODAL LLMS,entity,MODEL,1
1825f70e-78ff-447e-8172-dd62c8db7c2f,GPT-4,entity,MODEL,1
2b313278-10f5-4432-b130-01f0743740ee,REINFORCEMENT LEARNING FROM HUMAN FEEDBACK,entity,TECHNIQUE,1
e185e9cd-d016-42c6-bc65-92c4bd6d444e,STATISTICAL LANGUAGE MODELS,entity,MODEL,1
30d50d1d-b158-4d37-ba49-f12cdd58297a,NEURAL LANGUAGE MODELS,entity,MODEL,1
ac65625b-064e-42b7-af8b-1c6c4ddf65b6,PRE-TRAINED LANGUAGE MODELS,entity,MODEL,1
73c24291-a065-49c7-b580-9050cdc23ad1,WORD2VEC,entity,TECHNIQUE,1
f86006fe-e2b6-4d87-a4a8-a9d68549b5b9,GPT-2,entity,MODEL,1
33694225-dbe4-452e-8949-c37a82166cdc,BERT,entity,MODEL,1
4c3cdb1b-6a3d-4e48-87ad-208603a0868f,FINE-TUNING,entity,TECHNIQUE,1
402c8735-6062-4805-af7c-4d4b341e5850,UNSUPERVISED FINE-TUNING,entity,TECHNIQUE,1
1f36509d-fcbd-4f13-86a5-418e6b4d6b0d,SUPERVISED FINE-TUNING,entity,TECHNIQUE,1
d47d6d55-f1bf-41e3-b578-3f15cf70fec4,INSTRUCTION FINE-TUNING,entity,TECHNIQUE,1
e434acda-fdbb-48c8-a876-dd94226dd8a8,RETRIEVAL AUGMENTED GENERATION,entity,TECHNIQUE,1
19c9c08d-d402-4a4d-95fe-2ca3c359b2ed,TRANSFORMER ARCHITECTURES,entity,FRAMEWORK,1
689f5617-d6a5-42c5-a850-9f9dd9a69ac7,TEXT CLASSIFICATION,entity,TASK,1
ee476e81-c3e5-4dfb-bb67-a9e1cf72c29b,SENTIMENT ANALYSIS,entity,TASK,1
1ddda3b2-99b2-4d4a-99db-8a8345b2d002,QUESTION-ANSWERING,entity,TASK,1
eae4e8ca-7ccb-4397-8033-ebe6f23e19f1,RETRIEVAL-AUGMENTED GENERATION,entity,TECHNIQUE,1
d7f020f2-f58e-4209-a554-6ff19e8ae625,LLM,entity,MODEL,1
b745cafe-a2da-4436-9fd2-6b25a879667d,PARAMETER-EFFICIENT FINE-TUNING,entity,TECHNIQUE,1
283e6f7f-fe7d-4d69-80b6-a2acd982e29f,DATA INDEXING,entity,TECHNIQUE,1
c973b602-2b8b-4286-9bcf-c337787ad16f,INPUT QUERY PROCESSING,entity,TECHNIQUE,1
19ad4169-fc89-4798-81fc-beee10136904,SEARCHING AND RANKING,entity,TECHNIQUE,1
d6229c5b-498e-4935-9229-52e77f9027f3,PROMPT AUGMENTATION,entity,TECHNIQUE,1
6f1f981b-ca3c-4fe1-8f08-82aa28ab52e4,RESPONSE GENERATION,entity,TECHNIQUE,1
06dc2e59-828c-4c2f-bd76-23bf57d2ec38,USER EXPERIENCE,entity,METRIC,1
4f5e6fdc-5c5a-4291-8fe0-bc56d52bc5c0,COST EFFICIENCY,entity,METRIC,1
bb772acd-2f62-44ff-a813-62a365091549,ACCURACY,entity,METRIC,1
5031e76a-4ebc-4674-9285-1f18000c50b6,RECENCY AND RELEVANCE,entity,METRIC,1
464ff4c5-077d-469e-b264-f5d2c3cd48f6,BUSINESS CONTEXT AWARENESS,entity,METRIC,1
f6cb50df-1dde-43e0-96a1-bc9882562476,SERVICE SCALABILITY,entity,METRIC,1
3f343401-488b-4cf5-91ee-bbc56be1ecac,SECURITY AND GOVERNANCE,entity,METRIC,1
c3a53fb7-b235-4cde-b332-8306803646de,QUESTION AND ANSWER CHATBOTS,entity,APPLICATION,1
3adc6b12-f384-4018-8333-9d20890961a6,SEARCH AUGMENTATION,entity,APPLICATION,1
657d89bf-7dd7-4227-9761-c520076c13a1,KNOWLEDGE ENGINE,entity,APPLICATION,1
ee0d3bd3-34da-444c-9978-1f1421c1e08e,NLP,entity,DOMAIN,1
cf6a3487-18ae-4fe2-94d8-0eb4e2cce3c4,TF-IDF,entity,ALGORITHM,1
2214aa77-024c-41f9-aed8-87147a775408,BM25,entity,ALGORITHM,1
511ad143-8f20-4fc3-bc06-8bdb8c9c806a,HYPERPARAMETER,entity,PARAMETER,1
6b8ffa45-3d92-484d-bf2a-3f904f29eafe,VECTOR DATABASE,entity,DATASET,1
a80f259e-6701-4032-a883-57bf22b8b3bc,VISUAL WORKFLOW,entity,FRAMEWORK,1
63bfe180-dd05-4f2b-bc67-25d16f1dd47b,DYNAMIC DATA RETRIEVAL,entity,TECHNIQUE,1
447f4ca7-4cb1-47db-b7ac-1830a9ba2d53,LARGE LANGUAGE MODEL,entity,MODEL,1
758334b1-d9dc-4bd9-a4a3-30ce20f6984d,DATASET PREPARATION,entity,TECHNIQUE,1
a5af03d8-0926-486a-a595-f673c70d86c8,MODEL INITIALISATION,entity,TECHNIQUE,1
57acd984-5570-4663-8695-535c483db90c,TRAINING ENVIRONMENT SETUP,entity,TECHNIQUE,1
2a93bbca-05da-4d71-9e54-7595032cf837,EVALUATION AND VALIDATION,entity,TECHNIQUE,1
835a04d4-c4e8-4a12-883f-c17d940d21d5,DEPLOYMENT,entity,TECHNIQUE,1
970a30b4-1d9b-46d7-87e4-f0735eff662e,MONITORING AND MAINTENANCE,entity,TECHNIQUE,1
a11f4757-8df6-4837-a222-a390f456eec3,SEVEN STAGE FINE-TUNING PIPELINE,entity,FRAMEWORK,1
ea744e64-ee81-41ac-9637-c404a17e8c44,DATA COLLECTION,entity,TECHNIQUE,1
53e989ec-8a65-4a60-89c6-2e9590786eb5,DATA PREPROCESSING AND FORMATTING,entity,TECHNIQUE,1
a7be0992-9032-4231-8827-1b0736b777a4,HANDLING DATA IMBALANCE,entity,TECHNIQUE,1
1f0899d4-e3bc-44ed-a467-06099e0be0eb,DATA PREPROCESSING,entity,TECHNIQUE,1
9faa351b-febb-4b75-b2f7-36dbb1155860,SMOTE,entity,TECHNIQUE,1
859a658f-bab2-47ea-a9ad-6f0187e9cc90,FOCAL LOSS,entity,LOSS_FUNCTION,1
783a6cfa-aa13-4b9e-887e-e2be32609196,COST-SENSITIVE LEARNING,entity,TECHNIQUE,1
22b5221b-5849-4573-bbff-5f76bca6e0f1,ENSEMBLE METHODS,entity,TECHNIQUE,1
a0e6c56f-aebb-4ee7-adbf-fb0a6758ba02,STRATIFIED SAMPLING,entity,TECHNIQUE,1
9e705c1b-1dc9-4cc7-808e-5dec5bbb5d12,DATA CLEANING,entity,TECHNIQUE,1
9041849b-c0b4-4649-88d6-616c5ff1bbdf,PRECISION-RECALL AUC,entity,METRIC,1
a2b4b4f6-f67b-49af-a145-935e69382cd8,TRAIN TEST SPLIT,entity,TECHNIQUE,1
e1ab4015-df4b-4437-bbc0-fe0d0e83b343,K-FOLD CROSS VALIDATION,entity,TECHNIQUE,1
19a8410a-146a-47b4-8330-291f941cc35c,HUMAN ANNOTATION,entity,TECHNIQUE,1
c6710f64-7661-48f7-bb03-ed06e2d48330,SEMI-AUTOMATIC ANNOTATION,entity,TECHNIQUE,1
77e74303-6720-419f-9aaa-1d3b419f7a54,AUTOMATIC ANNOTATION,entity,TECHNIQUE,1
319e9627-a8ef-495a-8c52-d4416019b3cf,DATA AUGMENTATION,entity,TECHNIQUE,1
54d9d070-0e51-4288-89bb-715736ddc95e,WORD EMBEDDINGS,entity,TECHNIQUE,1
8613a477-0c40-4705-b325-2386f53f7701,BACK TRANSLATION,entity,TECHNIQUE,1
eeb3b2b0-8c06-49f6-a60d-05f31c0411e2,ADVERSARIAL ATTACKS,entity,TECHNIQUE,1
a8f4782e-6153-4962-ab93-75424c1f220c,NLP-AUG,entity,LIBRARY,1
e1d10d76-bb18-48f3-aa05-78099ae2655e,IMBALANCED-LEARN,entity,LIBRARY,1
a7b23313-d5b4-4d20-8a15-c6a686eddec8,SCIKIT-LEARN,entity,LIBRARY,1
3437f135-aea1-4c5e-8bd2-e0ee787d11ff,PRODIGY,entity,LIBRARY,1
3c5e6b22-4736-410b-957d-9ce4fc5e0623,AMAZON SAGEMAKER GROUND TRUTH,entity,SERVICE,1
93ba9cee-0439-463a-af79-5d0bdbf87ca4,EXCEL,entity,SOFTWARE,1
22111a17-10cf-43fb-9130-bfd4d13f1b8e,SNORKEL,entity,LIBRARY,1
ae537033-9f6b-4aac-b3f5-f61046890ec1,TEXTATTACK,entity,LIBRARY,1
3ff03d68-dcf5-4bb1-8b7d-02ddc6f991a7,GOOGLE TRANSLATE API,entity,SERVICE,1
fe19a173-8b2f-489c-ac7e-0b694b49ce3d,LLMXPLORER,entity,DATASET,1
bdec70f5-666e-43f9-a8b6-7fd700e31403,DATA ROBOT PAXATA,entity,ORGANIZATION,1
704ef384-b3c6-4635-8e66-a48b326d0cd5,KNIME ANALYTICS PLATFORM,entity,ORGANIZATION,1
e2797836-dd50-4fbc-828f-0297dfabc263,SPACY,entity,TECHNIQUE,1
dbc86ee7-f8ff-4513-b808-904b4c00ccea,NLTK,entity,TECHNIQUE,1
afda1312-3a08-42a6-a651-e1af4e03632f,HUGGINGFACE TRANSFORMERS,entity,TECHNIQUE,1
dfc69d01-63b7-4d2f-a4e6-f93e1ddee3e4,TRIFACTA WRANGLER,entity,ORGANIZATION,1
68cea89e-68fd-41cc-8dee-f71d75d68bc3,RAPIDMINER,entity,ORGANIZATION,1
cf408106-8cdf-4c3d-ad8a-0f250f8753ab,SAGEMAKER GROUND TRUTH,entity,ORGANIZATION,1
22da3f6f-b382-4416-8148-a0bed9bcee04,ETHICAL DATA HANDLING,entity,TECHNIQUE,1
ab0e2be7-d552-433f-a160-bfc24d8af360,MULTI-STEP GENERATION,entity,TECHNIQUE,1
84048754-edc3-42b3-b638-41698ad21d59,DATA ANNOTATION,entity,TECHNIQUE,1
e12f5b00-ed43-4d58-98ea-b68c44588591,DATA DIVERSITY,entity,METRIC,1
783ee803-9c42-45f0-905a-0e7cf0e27766,DOMAIN RELEVANCE,entity,METRIC,1
37ff8d43-a757-47b5-b735-550107cc1c27,DATA SIZE,entity,METRIC,1
873bb2f5-fd27-451a-a607-2b540dcb4796,SYNTHETIC DATA,entity,DATASET,1
f1d92e6e-5e82-424c-8ff4-2b713e0c57f0,PROMPT ENGINEERING,entity,TECHNIQUE,1
8221a626-3111-43f3-b59e-6da675b052c5,ITERATIVE GENERATION,entity,TECHNIQUE,1
14752850-531b-431d-85d7-8d0fb5020f50,DATA MANAGEMENT,entity,TECHNIQUE,1
4ae7d3a7-ca1a-4722-895a-cdbb355a4965,MODEL PERFORMANCE,entity,METRIC,1
8d5857c0-e96b-4b48-b246-0770ba0b8b5a,DATA QUALITY,entity,METRIC,1
96e9da05-2bb6-4e4d-ba7b-71b783bc70b2,HUGGINGFACE,entity,ORGANIZATION,1
09414cde-d037-428a-b8d4-4048bca9e824,PYTORCH,entity,FRAMEWORK,1
8848d7cf-e190-4ce1-aa52-6484f88a6e1b,TENSORFLOW,entity,FRAMEWORK,1
7796bef0-5049-4051-9c32-70bbef8738d6,AUTO_MODEL,entity,MODEL,1
028dbe7c-f90c-4017-bba6-25468d056033,LLAMA 3,entity,MODEL,1
a324634b-4fd1-4a83-b856-dd11936ac2a3,GPU,entity,TECHNIQUE,1
6682e5c7-73bf-4a53-8f4a-c6959634b8af,TPU,entity,TECHNIQUE,1
60724611-af0c-487a-89cb-892bfe43ba73,CUDA,entity,TECHNIQUE,1
7d6d7da6-1f5b-4822-94d4-5b801406ee5b,CUDNN,entity,TECHNIQUE,1
ade70777-6f11-4b66-8009-7fe68a56e7b6,HYPERPARAMETERS,entity,PARAMETER,1
eeb79749-42ac-42f8-8fc8-fc0136c5e892,OPTIMISERS,entity,OPTIMIZER,1
da63afeb-8953-453f-82de-05f96d00651f,LOSS FUNCTIONS,entity,LOSS_FUNCTION,1
fa50a6f6-e157-43aa-83ad-72d48efe9758,TRAINING ENVIRONMENT,entity,DOMAIN,1
527ac727-0cca-4159-b4be-f6eb889b75e1,PRE-TRAINED MODEL,entity,MODEL,1
d0aefd34-76e3-40cc-9d1f-d881042c0967,AUTO_MODEL_FOR_CAUSAL_LM,entity,MODEL,1
25cf2725-8243-40d3-9662-23f7136fd9d6,4-BIT QUANTIZED MODELS,entity,MODEL,1
98acde03-96bd-4ae4-9b09-245e4dce64b3,PIPELINE,entity,FRAMEWORK,1
9ffb1c43-e666-4fa4-b729-6b9dba1d86f4,DEEP LEARNING FRAMEWORKS,entity,DOMAIN,1
90b7276f-2462-46b3-8b24-3edf69f10d0f,DISTRIBUTED TRAINING,entity,TECHNIQUE,1
3dc72a31-b342-4a60-943f-58fce3e948b4,DATA PARALLELISM,entity,TECHNIQUE,1
a1435cf3-1870-4e12-a919-72f9096363e6,MODEL PARALLELISM,entity,TECHNIQUE,1
da340f29-3b74-48a0-a45a-9e75b8329433,NVIDIA A100,entity,TECHNIQUE,1
8e8be8eb-436f-448c-9191-da62945116a3,NVIDIA V100,entity,TECHNIQUE,1
a781a0cc-33c5-428f-b253-aee97f1f6b1d,HIGH-PERFORMANCE HARDWARE,entity,DOMAIN,1
ae65c09e-a7e6-4c1f-9366-dbf229cb329e,LEARNING RATE,entity,PARAMETER,1
bd25df2d-6189-45da-bc8e-ecf6de9c948f,BATCH SIZE,entity,PARAMETER,1
f82d983b-5520-4646-ba0b-fc2eaea4b6a7,EPOCHS,entity,PARAMETER,1
8b8c9ddf-dd87-4d72-be0c-acdb365bdab8,OPTIMISATION ALGORITHMS,entity,ALGORITHM,1
c8cfadd5-33dd-4d30-9650-176fbb759b08,STOCHASTIC GRADIENT DESCENT,entity,ALGORITHM,1
ecd6300c-71a8-4fdf-abb0-c2ffa274d996,GRID SEARCH,entity,TECHNIQUE,1
1cdb19a9-99df-4aa4-a5c3-a8d05d083a2b,BAYESIAN OPTIMISATION,entity,TECHNIQUE,1
f74224d0-32b5-46c3-b3e4-84c126e19c5a,GRADIENT DESCENT,entity,ALGORITHM,1
c1482033-5b2b-4294-97b8-a34f63cc8094,AUTOMATED HYPERPARAMETER TUNING,entity,TECHNIQUE,1
8e325517-b360-4019-aef3-b8deeed782df,COOLING SYSTEM,entity,TECHNIQUE,1
25f107e9-3d6c-4fc4-8883-e0e3e400c363,POWER SUPPLY,entity,TECHNIQUE,1
5dbdd3bd-1f0a-41ef-a080-6a5261012a1e,BACKPROPAGATION,entity,ALGORITHM,1
d11a4fd6-8732-424c-b7d2-94995eab386f,SGD,entity,OPTIMIZER,1
a75767c0-f0d7-4dbb-bab9-98e167821d11,MINI-BATCH GRADIENT DESCENT,entity,OPTIMIZER,1
9a0dbc97-eba6-4aef-98ca-7539d9cc50c0,ADAGRAD,entity,OPTIMIZER,1
d927866c-adcb-4c78-a741-2651f68d892e,RMSPROP,entity,OPTIMIZER,1
a4af4dff-2700-4ea5-93a2-7348b009c899,ADADELTA,entity,OPTIMIZER,1
227b53c9-3974-42e1-9c54-f9f300896fa3,ADAM,entity,OPTIMIZER,1
da391c46-a5fb-4b50-bd0a-8ed6f51af8c0,ADAMW,entity,OPTIMIZER,1
566003d3-47d3-463c-b74b-ef2600918875,MOMENTUM,entity,PARAMETER,1
fa7794a8-34f6-49eb-ba17-a004be458f21,L2 REGULARIZATION,entity,TECHNIQUE,1
fc4dda1f-e0e8-474d-a900-d8644cb24d3c,HUGGING FACE TRANSFORMERS,entity,FRAMEWORK,1
2756bddf-97b6-488e-9b21-8b0f61cfd486,CHECKPOINTS,entity,METRIC,1
c9a98fd9-9acb-4619-befc-ed6687cefde8,MIXED PRECISION TRAINING,entity,TECHNIQUE,1
d13c12fa-c9bb-4408-9cea-e296ea76136a,OVERFITTING,entity,METRIC,1
8ccc78fa-a172-4647-aa1b-13d1e7ea07fd,UNDERFITTING,entity,METRIC,1
6620ac3c-5f35-46e3-9ea1-1378da7d5c49,OPTUNA,entity,TOOL,1
32db09be-43d6-4bc4-954c-1baf9124c7dd,HYPEROPT,entity,TOOL,1
8290b16c-c8a1-41f0-8aff-cc4bb42e9e0a,RAY TUNE,entity,TOOL,1
ecc4fe39-571d-4e01-971a-629d88855aed,TENSORBOARD,entity,TOOL,1
3d869f7f-fd6c-41e0-bda5-3936581581fe,WEIGHTS & BIASES,entity,TOOL,1
06faf6dd-f361-46d8-af0c-81ff7119f6f6,MLFLOW,entity,TOOL,1
4d0dc508-15a9-4674-a335-8e5c33a0b326,MED-PALM 2,entity,MODEL,1
44425780-4866-4da3-b38e-f87945589ea3,FINGPT,entity,MODEL,1
03aee123-00c8-42c4-87cb-78790e83993b,LAWGPT,entity,MODEL,1
a09874e5-bb7c-4ce8-a935-44a6546f179a,MEDQA,entity,DATASET,1
d1eb09a8-4eb5-4398-a751-b3052ec7ec51,MEDMCQA,entity,DATASET,1
abba39f3-053c-4015-9f79-e01a3d769421,LIVEQA,entity,DATASET,1
f574b0a6-989f-41d5-9865-98f30ba6bbaa,MEDICATIONQA,entity,DATASET,1
ee062bdf-163a-40fd-aaa5-733c539ea7ca,HEALTHSEARCHQA,entity,DATASET,1
79a1d756-ad4e-4c15-ae42-7852cb62bfa4,FINANCIAL NEWS,entity,DATASET,1
f1a11209-b1b6-4055-b45a-c39a375200c6,SOCIAL MEDIA,entity,DATASET,1
b855774a-4c88-46c3-88d4-b7c7efd18567,REGULATORY FILINGS,entity,DATASET,1
4f45d61d-8a78-4f15-9bcc-6ff028a47d45,TRENDS,entity,DATASET,1
7a86bcf5-8675-4c59-90ca-70f5706fb222,ACADEMIC DATASETS,entity,DATASET,1
463f85c9-b946-45b4-9946-0bbe7f7f80c7,OUTPUT LAYER,entity,PARAMETER,1
9c07c951-b91e-4390-bf68-14cfb5f4f07b,TASK-SPECIFIC FINE-TUNING,entity,TECHNIQUE,1
c6ef1379-bd20-48f2-8d49-dc4d30a1b709,DOMAIN-SPECIFIC FINE-TUNING,entity,TECHNIQUE,1
3fc800e2-3cc5-4bce-ab0e-a4eb5d1bce31,PARAMETER-EFFICIENT FINE-TUNING (PEFT),entity,TECHNIQUE,1
cbbdb496-a05c-4f50-a25f-ca5ff0f529e3,HALF FINE-TUNING (HFT),entity,TECHNIQUE,1
92be0b76-efa4-4998-b6db-cbc9fc5246cf,TRAINING LOOP,entity,FRAMEWORK,1
6b7d1125-c038-4291-ab91-edc0424c6693,DYNAMIC LEARNING RATES,entity,PARAMETER,1
78921bc5-0869-48c9-85b4-f75f4ed07c6f,EARLY STOPPING,entity,PARAMETER,1
0ac1efb6-ba16-430a-8c7a-fc7e151e7487,PROXIMAL POLICY OPTIMISATION (PPO),entity,ALGORITHM,1
844688dd-672e-4906-84d7-90e403859f75,DIRECT PREFERENCE OPTIMISATION (DPO),entity,ALGORITHM,1
6e54fd98-7404-4419-bec2-adeedb163710,PRUNING TECHNIQUES,entity,TECHNIQUE,1
3f059b6c-f8ce-4654-98c0-dee074f40063,DYNAMIC PRUNING,entity,TECHNIQUE,1
c4dfe36f-64c9-47fa-a79d-947fda782d70,BENCHMARKS,entity,METRIC,1
2168505e-34c8-489a-80a9-15135bc16112,LORA,entity,TECHNIQUE,1
a0c57b03-21a9-4c2c-8725-84d8d0eb48d5,REINFORCEMENT LEARNING ON STOCK PRICES,entity,TECHNIQUE,1
34e51e24-0709-4ca6-bb71-e2675ce7de91,CHINESE ALPACA PLUS 7B,entity,MODEL,1
9e39403d-c61f-409e-aaf5-45810acd1635,JEC-QA,entity,DATASET,1
67a1f9bd-511f-4100-b7a0-d471bdda0c4a,PHARMAGPT,entity,MODEL,1
321cc8e3-f89f-498e-83c0-2a118685734b,LLAMA,entity,MODEL,1
199780e7-21d4-4cca-951c-22deb4ca0d8d,PALMYRA-FIN-70B-32K,entity,MODEL,1
4e40cf73-d807-43fc-87d4-db08b507207c,ADAPTERS,entity,TECHNIQUE,1
2e143d23-118a-43a5-8eda-a6176c23a18e,RLHF,entity,TECHNIQUE,1
897b24bd-c499-488e-9377-73ec92aa4fea,PHARMACEUTICAL DOMAIN,entity,DOMAIN,1
2b1f7a8f-546e-4a79-b0fb-2ed4d14bf390,LEGAL DOMAIN,entity,DOMAIN,1
3636c337-acf6-48c2-8850-f2dc84d209b8,OPEN-SOURCE DATASET,entity,DATASET,1
f3e96095-441b-4233-8280-0baf6ed61439,HUMAN PREFERENCE EXPERT-ANNOTATED INSTRUCTIONS,entity,DATASET,1
ab4c5fcf-9896-41c1-9170-e42ca111c9cb,QLORA,entity,TECHNIQUE,1
2aebd45c-a49a-4c84-8295-0dd8cdec41ab,DORA,entity,TECHNIQUE,1
b8015594-2fe1-4e4a-ad48-efaa79c0c19b,HUGGING FACE,entity,ORGANIZATION,1
7a6064d7-20a9-4da5-b92b-7f9c6c786716,PEFT,entity,FRAMEWORK,1
1a7bbcb6-ea9c-4c9b-bfdf-03d597b5cd49,PHI-2,entity,MODEL,1
41c7549c-9648-4e68-9ccb-b86bd51e0f38,ADAPTER LAYERS,entity,TECHNIQUE,1
8c623d6e-8383-4f58-9880-a3bd2ce6a2ca,PROMPT-TUNING,entity,TECHNIQUE,1
2e313197-b18d-4f4e-ab82-ceaf166a16d4,4-BIT QUANTISATION,entity,PARAMETER,1
a269b5c5-f5ad-4e0c-a832-29ff62915ab1,8-BIT QUANTISATION,entity,PARAMETER,1
caef1d92-cb77-4179-98d4-96e3a8af440b,GRADIENTS,entity,PARAMETER,1
adf9498e-c8b4-4a01-ae02-dce170ffbc32,CONSUMER GPUS,entity,TECHNIQUE,1
1c9780c3-0739-4f6d-8a39-ef560ffbd6e2,VISION-LANGUAGE APPLICATIONS,entity,DOMAIN,1
05cdf341-25d4-4d8f-8307-e86aa0655aee,HUGGINGFACE LORACONFIG,entity,FRAMEWORK,1
dcb514cd-be62-43f6-9107-3e403310bbfe,MULTI-TASK ADAPTER,entity,MODEL,1
3efaa673-562a-4941-bac8-41fc871c0c48,PEFT LIBRARY,entity,FRAMEWORK,1
5372af22-ea84-4cef-9be8-5596e3e93c22,SINGULAR VALUE DECOMPOSITION,entity,TECHNIQUE,1
4e492e86-164b-49cf-b569-514bd4c6113f,INFERENCE LATENCY,entity,METRIC,1
ff8571bc-8b48-41b7-bb72-371c2ff38721,COMMONSENSE REASONING,entity,BENCHMARK,1
6c8d1914-8dff-4bb0-b44c-ce1652d7b72c,VISUAL INSTRUCTION TUNING,entity,BENCHMARK,1
ac3a847d-6b04-4c71-a6aa-ee9dfcf560ec,IMAGE/VIDEO-TEXT UNDERSTANDING,entity,BENCHMARK,1
695dcc7d-44a7-4e22-a739-2ae8812698d7,ADAPTER CREATION,entity,TECHNIQUE,1
e824e924-a6c7-48cc-bfdc-25b97bb445a2,TASK-SPECIFIC ADAPTATION,entity,TECHNIQUE,1
df98d983-bdc5-47a3-b4d2-601963f7b395,BEHAVIOUR ADJUSTMENT,entity,TECHNIQUE,1
24f70cec-8177-4ca8-8089-b515e2147eac,LAMINI MEMORY TUNING,entity,TECHNIQUE,1
70ad6072-e9ff-4fb1-a133-3ce732357fdc,LLAMA 2-7B,entity,MODEL,1
791d5226-70fc-45a7-8a74-7c600de46e3f,CHINCHILLA RECIPE,entity,FRAMEWORK,1
35e3f939-b3c9-47d7-ba63-44c394789441,MIXTURE OF MEMORY EXPERTS,entity,MODEL,1
ed2551fd-306d-44a3-808e-55bff34195f0,VALIDATION DATASETS,entity,DATASET,1
796edcb9-b74e-4158-8c15-3d3611461d6c,PARAMETERS,entity,PARAMETER,1
7f03ce2e-5751-439d-88d4-131deb823b4e,EVALUATION,entity,EVENT,1
7588a4b8-a0a0-415e-8310-4e161a13f334,CATASTROPHIC FORGETTING,entity,METRIC,1
d2d54fe8-7f33-4eed-9230-186854a45f85,CONTINUAL LEARNING,entity,TECHNIQUE,1
0290fe6f-6375-46e7-b0a7-f2197e119559,FACTUAL RECALL,entity,METRIC,1
bfc2357c-3e99-480e-a476-e484b49d1d55,INFERENCE TIME,entity,EVENT,1
29bf0703-8caa-4599-b3b9-bb9763138294,GPU KERNELS,entity,TECHNIQUE,1
f106d656-97b8-4ae3-941d-2550440464b4,TOKEN SELECTION,entity,TECHNIQUE,1
a515a528-611f-460e-95a8-f4bb5262e7fc,MOME,entity,MODEL,1
6e0131cb-bb36-43f5-ace8-92ec7a96f1c8,LAMINI-1,entity,MODEL,1
8153f88a-f858-42b1-9366-ff7069d7c55f,MIXTRAL 8X7B,entity,MODEL,1
2d89a2f4-c95e-4058-be08-dab449810f41,SPARSE MIXTURE OF EXPERTS,entity,TECHNIQUE,1
a73c5921-0da6-48b8-9504-26ba2cd4dd90,MISTRAL 7B,entity,MODEL,1
819c480c-2470-4ab8-8776-6c95a300b560,LLAMA 2 70B,entity,MODEL,1
680b393d-9927-4821-96d1-267ede2180cc,GPT-3.5,entity,MODEL,1
9d32d28a-95fb-47d3-b51b-e02f41888f8b,MIXTURE-OF-AGENTS,entity,MODEL,1
8b14df1c-b74e-4fee-84c0-396219d2d109,MOE,entity,TECHNIQUE,1
909d74da-682e-4d7f-9f3a-4471eaeae68f,PPO,entity,ALGORITHM,1
eb971bf0-ba94-4b8d-950b-4b2715cd5d43,HUGGINGFACE TRANSFORMER,entity,FRAMEWORK,1
b9b96540-16dc-4dc7-b388-d8d67027a7eb,PPO TRAINER,entity,TECHNIQUE,1
960c0e06-0264-4bf2-88ce-0a3bac3abf84,GPT-4O,entity,MODEL,1
d77e7e5d-db45-4a6a-b090-bbe558ea268f,QWEN,entity,MODEL,1
1b1cf33b-6143-43d8-b5aa-a18b185ce6c3,LLAMA-3,entity,MODEL,1
f633e692-6caf-4093-86bd-9a277a00db0f,WIZARDLM,entity,MODEL,1
cab607ec-002a-402d-a1ac-590d885ac7fc,POLICY GRADIENT,entity,TECHNIQUE,1
d2fbcf7a-4ac2-4f2f-b075-1cbfb5c995cd,SURROGATE OBJECTIVE FUNCTION,entity,PARAMETER,1
5a863c2d-8865-4a6f-a9c1-3d45baa71344,STOCHASTIC GRADIENT ASCENT,entity,OPTIMIZER,1
4279c325-33b2-42fa-8905-167a8859ab64,BLEU SCORES,entity,METRIC,1
ae50f9d0-b11c-44af-bb0e-1f7ed260ec3b,PREFERENCE SCORES,entity,METRIC,1
d006ba19-da74-4677-8a58-f470d5736ae4,HUGGINGFACE TRL,entity,ORGANIZATION,1
6931ddbc-133b-4671-926e-dd5e5cb6c2d9,IMDB,entity,DATASET,1
32f5ffe2-32c6-4598-8db0-f2fe495c9e5e,GPT2,entity,MODEL,1
56ffd54d-b6d7-419e-abf8-7735d8cc2e75,PREFERENCE DATA,entity,DATASET,1
dec1731f-d670-490c-a3fb-004fd193be4d,REWARD MODEL,entity,MODEL,1
081e3834-84fc-44e3-9bc6-8018f279f749,BETA VALUE,entity,PARAMETER,1
f3dc5ef5-8af0-44a0-946b-7e857587302c,HYPERPARAMETER SENSITIVITY,entity,METRIC,1
bded3357-de12-4063-9720-930f68d4cb26,STABILITY AND CONVERGENCE ISSUES,entity,METRIC,1
9943ed93-e1fa-4a19-83ff-2a8d2c61a5cd,REWARD SIGNAL,entity,METRIC,1
4812369e-c494-45b4-bc58-fe799269d6a9,DIRECT ALIGNMENT WITH HUMAN PREFERENCES,entity,BENCHMARK,1
66070e1e-2a65-4ff6-a356-2d46a3d3ba6c,MINIMISED DEPENDENCE ON PROXY OBJECTIVES,entity,BENCHMARK,1
edbb8503-4e7a-4f2e-a34a-6b2aec7e0b26,ENHANCED PERFORMANCE ON SUBJECTIVE TASKS,entity,BENCHMARK,1
26f0164d-ea19-4996-8d77-2c79c7fa0c8e,ETHICAL CONSIDERATIONS,entity,DOMAIN,1
2f8b62ef-1170-4e32-bf4a-d6c41b5f134c,TUNING,entity,TECHNIQUE,1
f0c2a90f-4f17-4d65-8fe7-542aa26c86da,DPO,entity,TECHNIQUE,1
e326e883-ca07-416c-9867-3b8e2d7a2127,ORPO,entity,TECHNIQUE,1
d25247b4-986d-43cf-92ee-540ad03f1c41,CODECONTEST,entity,DATASET,1
b2676711-7835-411a-8a64-77de6473552c,ALPHACODE-41B,entity,MODEL,1
bdcdeafe-e208-4c0f-94f8-642e9a47b966,MISTRAL,entity,MODEL,1
cef43f52-100a-409b-8855-34cf9ee5e0b9,ADVANTAGE NORMALISATION,entity,PARAMETER,1
fa02e37d-7919-484e-a8ec-6bf097f4fb54,EXPONENTIAL MOVING AVERAGE,entity,PARAMETER,1
7510145e-d752-440f-88e8-8099be422419,PREFERENCE DATASET,entity,DATASET,1
3fc60e81-478b-4bd8-8dd0-ce5f3c11ff9b,SFT,entity,TECHNIQUE,1
bcf7452e-4c94-4f76-926f-d1a3600c148d,LOG-LIKELIHOOD,entity,METRIC,1
b66ce636-d68a-41b4-9488-e9c7da505944,SIGMOID FUNCTION,entity,FUNCTION,1
a931acb9-6775-4d9c-8999-d0e68f4a0ade,PREFERENCE ALIGNMENT,entity,TECHNIQUE,1
64fe3f31-6492-4045-967d-c74c7947d2ad,POST-TRAINING PRUNING,entity,TECHNIQUE,1
9da2d575-f587-4d75-9e31-44d4d9119cea,PRE-TRAINING PRUNING,entity,TECHNIQUE,1
9717faee-badf-4caa-86ea-6265a69a3fa5,CROSS-ENTROPY,entity,METRIC,1
a0b09077-b667-4ade-a5a3-a87684fe0c90,PERPLEXITY,entity,METRIC,1
6725cd34-aeb3-4b27-a4b0-f4b4ef8f33b2,FACTUALITY,entity,METRIC,1
a89bc3fc-dfe7-49b5-91d1-9fdd8a52b882,LLM UNCERTAINTY,entity,METRIC,1
995d1325-00be-4c2d-a821-2f5197d3cdbc,PROMPT PERPLEXITY,entity,METRIC,1
3bc7256e-d96e-4a52-832d-167e25620f00,CONTEXT RELEVANCE,entity,METRIC,1
f96a9bbf-547d-42df-9bf1-6e7a244efc05,COMPLETENESS,entity,METRIC,1
24bb1b48-dbeb-4ee7-9a2a-e65c651061a4,CHUNK ATTRIBUTION AND UTILISATION,entity,METRIC,1
c44fe78d-63b9-46dd-aa8b-11dc24887035,DATA ERROR POTENTIAL,entity,METRIC,1
3b56f555-9962-41c8-b421-bc1599e0ef3a,SAFETY METRICS,entity,METRIC,1
8edc278c-9a2e-441a-81ca-a6bf0c8706dd,EVALUATION METRICS,entity,FRAMEWORK,1
fee09dfb-8d8e-4c88-a143-c33325e905bf,TRAINING LOSS CURVE,entity,METRIC,1
8793fa83-7155-4467-988f-07cb3cb003ba,VALIDATION SET,entity,DATASET,1
cd9f3188-ec13-42c1-9605-e1eab383d647,TRAINING EPOCH,entity,EVENT,1
bbd9dd1c-c783-4494-9323-d3217f342345,REGULARISATION,entity,TECHNIQUE,1
52929267-2e3b-4a6a-b528-ac58820aad9d,DROPOUT,entity,TECHNIQUE,1
5f5d0217-489f-4529-9b52-f32271a5eead,CROSS-VALIDATION,entity,TECHNIQUE,1
de19c967-07dc-406b-8bb1-6b1bed9e81f6,BATCH NORMALISATION,entity,TECHNIQUE,1
01d9d6a4-e534-4ec9-a693-7adcd6ac38a8,NUMBER OF TRAINING EPOCHS,entity,PARAMETER,1
270e8808-65bf-47c5-8588-388956ac88b2,OPTIMISER,entity,ALGORITHM,1
7bd7e374-fed3-4b90-a362-ffa03f82b411,CROSS-ENTROPY LOSS,entity,LOSS_FUNCTION,1
2cceebba-e62f-4e34-8463-239be8d2891a,GLUE,entity,BENCHMARK,1
e4e0570a-b41e-48fb-9a74-73119ddd8c7d,SUPERGLUE,entity,BENCHMARK,1
fe623d3c-c209-4b58-a135-87268fff1094,HELLASWAG,entity,BENCHMARK,1
075a6e5f-c7aa-4fd5-b1f1-ed7229615162,TRUTHFULQA,entity,BENCHMARK,1
0e791b5c-16cd-4972-871e-00347ce56fa2,MMLU,entity,BENCHMARK,1
036a22eb-8079-4bc5-9980-89636ef76bf8,BIGCODEBENCH,entity,BENCHMARK,1
3d8f690b-a4ff-43c8-a335-4321e8013e2c,LEARNING RATE SCHEDULING,entity,TECHNIQUE,1
12e81324-8a73-4385-a80c-b113ccd426bd,GRADIENT CLIPPING,entity,TECHNIQUE,1
8bd74ee0-cd43-4b42-b535-f45bd21c70b1,VALIDATION LOOPS,entity,EVENT,1
b9e62e51-9e77-48f3-a409-20f6e1a931a6,VALIDATION METRICS,entity,METRIC,1
f3400ee6-2907-4db4-8fb1-711a0efd2666,DATA SIZE AND QUALITY,entity,DOMAIN,1
ef7cbd26-d34c-46a9-af90-571e6f6adc29,WARMUP STEPS,entity,PARAMETER,1
e58e9b1a-7123-4d56-8a89-ebf5131d66e6,CHATGPT,entity,MODEL,1
5e2217c4-14c1-4e81-b4a9-d425aef2b7f2,GPT3,entity,MODEL,1
96066dee-20a8-4499-9c6e-ada5fb0b9de3,INSTRUCTGPT,entity,MODEL,1
14b0977e-9862-4c85-9a3b-ad95da07b14e,DECODINGTRUST,entity,FRAMEWORK,1
06d8eb80-9c93-412e-bc46-3360c12d3c23,LLM SAFETY LEADERBOARD,entity,FRAMEWORK,1
3171f443-123e-4be1-85c1-734706b8ebba,LLAMA GUARD 2,entity,MODEL,1
3f3482c0-527a-4ed9-a40e-e736673aeeb5,LLAMA GUARD 3,entity,MODEL,1
bc96c79f-db32-4d9a-b81a-82c829ef3e4b,ETHICS,entity,DATASET,1
65f2e436-bfda-4f32-939a-22479fc67c82,JIMINY CRICKET,entity,DATASET,1
24cb70c9-31ff-49ba-9c28-55432a0857a1,OPENAI MODERATION EVALUATION,entity,BENCHMARK,1
644d198b-1cec-4a15-9985-a9b38a6782b3,TOXICCHAT,entity,BENCHMARK,1
a76e3b99-a0c8-433c-9a99-416e796a420a,SHIELD GEMMA,entity,MODEL,1
bd2d56b7-a5ac-4a41-a778-dafef449b799,SAFETY RISK TAXONOMY,entity,FRAMEWORK,1
9c0d1c8c-fa21-4c76-961d-23a0020584d5,CONVERSATIONAL AI,entity,DOMAIN,1
fc23abfd-3f79-401c-9f69-87a168608095,OUT-OF-DISTRIBUTION ROBUSTNESS,entity,METRIC,1
e079ece6-e533-4d5d-b264-945e4788a55d,HALLUCINATION DETECTION,entity,TECHNIQUE,1
afb0b62f-f8fb-47d6-b84b-b6411cb94c58,TONE APPROPRIATENESS,entity,METRIC,1
3b790bd4-8839-4c59-8077-bdcaf57d5015,PRIVACY EVALUATION,entity,METRIC,1
f8aa0f1d-8e41-4b37-857a-9005836568f2,FAIRNESS,entity,METRIC,1
136333e8-c9dc-4b17-9491-14cd2188f36e,META,entity,ORGANIZATION,1
8bc75f9f-6bfd-4ef3-b616-c959dfaf7909,WILDGUARD,entity,MODEL,1
57c058a0-8334-408d-b424-f46030998d1c,WILDGUARD MIX 3,entity,DATASET,1
cf814393-1f49-4e78-8005-1106f1cd0157,WILDGUARD TRAIN,entity,DATASET,1
b5dbd271-cf0f-49da-8f11-59bee1ae226e,WILDGUARD TEST,entity,DATASET,1
8e411f4c-b223-4a1c-a863-1156cd0aabb6,MISTRAL-7B,entity,MODEL,1
4b2b2640-cea5-4fa7-9105-34512dca2f20,AMAZON WEB SERVICES,entity,ORGANIZATION,1
9e163580-e456-460d-805a-5aea507a5601,AMAZON BEDROCK,entity,FRAMEWORK,1
22f38266-7d30-4c87-8296-f15f5403e65c,AMAZON SAGEMAKER,entity,FRAMEWORK,1
e11a92a0-5dec-4b8b-af76-bceadae7e53d,AUTO MODEL FOR CAUSAL LM,entity,FRAMEWORK,1
cdcd5af7-467c-4b90-96ed-d7dab79be5ae,GOOGLE COLAB,entity,FRAMEWORK,1
12694c29-13d7-4c0d-a37e-018c99cc6a53,TOKEN,entity,METRIC,1
2d38620f-67f8-4e33-9faa-e37a03a84588,COST OF OWNERSHIP,entity,METRIC,1
2cdb8656-a3ce-4308-af25-fcbebb2c9d46,MICROSOFT AZURE,entity,ORGANIZATION,1
6b0c178e-8055-4587-a3b5-1e8cbeb7bd3d,GOOGLE CLOUD PLATFORM,entity,ORGANIZATION,1
5653f7db-979d-498b-bc4c-181a4a2ef49d,PETALS,entity,FRAMEWORK,1
97c63049-0fac-4cde-a172-44adcbdff99b,WEBGPU,entity,TECHNIQUE,1
bae0f367-8939-4653-bd30-d52b540820ea,TRADITIONAL ON-PREMISES GPU-BASED DEPLOYMENTS,entity,TECHNIQUE,1
b7aef366-29ad-428b-824e-0eadf5435d6d,DISTRIBUTED LLM,entity,TECHNIQUE,1
a81310f1-8c80-4758-a445-95a96adbbaa8,LARGE-SCALE NLP APPLICATION,entity,EVENT,1
fae9d5f5-8396-4ac5-b417-87e67c9559c7,GLOBAL RESEARCH COLLABORATION,entity,EVENT,1
9d2ec791-f523-4928-89bf-fdcf293c5137,AZURE OPENAI SERVICE,entity,ORGANIZATION,1
a249605d-bf6e-480d-add8-5d1479407c3d,AZURE MACHINE LEARNING,entity,ORGANIZATION,1
056ac7f0-91af-467f-90f9-655b43841076,VERTEX AI,entity,ORGANIZATION,1
5ca3d2a0-7079-427e-b0ac-32be3758dc69,CLOUD AI API,entity,ORGANIZATION,1
3720b395-a2d1-463e-8076-967d5e524a3c,INFERENCE API,entity,ORGANIZATION,1
8d681ffb-64ff-456f-9802-2448bd75dd23,SPACES,entity,ORGANIZATION,1
cfb83b9c-a025-4183-9010-61c62ed53296,DEEPSEED,entity,ORGANIZATION,1
fef890da-beca-474b-a0b0-5a82cf897e78,OPENLLM,entity,ORGANIZATION,1
397837d7-2efd-426d-9f29-6febdce70924,WEBLLM,entity,MODEL,1
8b8476a0-8e07-42cb-a9b8-72f6ec8ed37b,VLLM,entity,MODEL,1
c4791248-2ef2-4b80-ae84-b9c1a6bb4f31,PAGEDATTENTION,entity,ALGORITHM,1
c2aff61a-21ad-4c6a-a375-a8678e1d9f9f,LANGUAGE TRANSLATION,entity,APPLICATION,1
2bf4b9f9-4f70-4845-9877-4eb18bef24d2,CODE AUTOCOMPLETION,entity,APPLICATION,1
bfd02085-e396-4e4a-abec-f4775dd8e21c,CUSTOMER SUPPORT CHATBOTS,entity,APPLICATION,1
6774d032-fd23-4cc3-9b5b-6f90cb2ead7a,DATA ANALYSIS AND VISUALISATION,entity,APPLICATION,1
8db8c35e-42cf-4de5-85e1-d38186f99337,PERSONALISED RECOMMENDATIONS,entity,APPLICATION,1
d2c6586c-800e-40fe-bbb9-fa039998e273,PRIVACY-PRESERVING ANALYTICS,entity,APPLICATION,1
4c88913c-74ba-446d-9241-0b9ae1035e6b,HEALTHCARE STARTUP,entity,ORGANIZATION,1
17a2e7af-d27a-4520-86db-0d26773a6607,EDGE DEVICE DEPLOYMENT,entity,APPLICATION,1
dfce8e5f-39d5-4c0d-9c76-34b0cefcdf9e,MODEL QUANTISATION,entity,TECHNIQUE,1
1845eaff-fced-4889-8810-535bfe73c72d,HIGH-VOLUME CONTENT GENERATION,entity,APPLICATION,1
322f8efc-97bb-4593-b1a4-2b932aa9cd1a,COMPUTE RESOURCES,entity,PARAMETER,1
97c13dbb-c7dc-4d32-9bea-48bca7f7950f,MEMORY,entity,PARAMETER,1
425d7caf-8638-43ef-8672-fc819ce60c0a,HORIZONTAL SCALING,entity,TECHNIQUE,1
a0adaa33-b997-490b-a6d4-01b77ad5b641,LOAD BALANCING,entity,TECHNIQUE,1
e54dc34a-b446-4826-a510-a1d56554dd09,TOKEN-BASED PRICING,entity,METRIC,1
a52e19ca-34ff-4659-a576-ffdb1f8a9e6f,GDPR,entity,REGULATION,1
71cc56a7-1956-413d-aa41-8271b4c94d7e,HIPAA,entity,REGULATION,1
f4cbf639-c5e3-4bb7-8821-36177c5271af,CLOUD HOSTING,entity,HOSTING METHOD,1
6e212317-cec0-4b1d-8a72-87cc513d0506,SELF-HOSTING,entity,HOSTING METHOD,1
e4791611-bece-493c-9203-1ec8783b824a,ACCESS CONTROL,entity,SECURITY MEASURE,1
925e954d-9733-48e3-a93e-0663f30c7f45,MONITORING AND LOGGING,entity,PROCESS,1
682c7d2d-81f2-4728-8d24-70b96700dfaa,ERROR MONITORING,entity,PROCESS,1
9d13548e-632e-4d49-a749-7f2f1c60bac0,FEEDBACK LOOP,entity,PROCESS,1
20823933-d8a7-4426-82b7-38c8fd80f1f3,SECURITY MONITORING,entity,PROCESS,1
e5cef8fb-6f58-45ec-8cd3-3989fb533ba6,MODEL VERSIONING,entity,PROCESS,1
5ed3cbee-1420-4bce-a3fd-175081b0b06b,LATENCY,entity,METRIC,1
003dec89-abf1-47c5-82b1-ffc244d03b93,THROUGHPUT,entity,METRIC,1
ab90aeb0-18f6-40b3-9be4-6022a056ce80,PRECISION,entity,METRIC,1
18a6c03e-60f4-4318-ba02-9d4e2c3f67ea,RECALL,entity,METRIC,1
e01526c9-a71c-40dd-86ba-ddfbe54a7c1f,F1 SCORE,entity,METRIC,1
c336b266-5121-4abe-8f2f-987861d3449e,ERROR RATES,entity,METRIC,1
a37e60bf-ed15-44d2-970e-66daae51e3e9,TOKEN USAGE,entity,METRIC,1
ccbdb231-e1f1-41a1-ae4b-6aecd2972064,MONITORING PROGRAMME,entity,FRAMEWORK,1
4e86bbcd-e815-4a81-b5bf-141aa73225b9,FUNCTIONAL MONITORING,entity,TECHNIQUE,1
9edb5d89-e2ea-49b3-9bae-89d8b06a5f5b,PROMPT MONITORING,entity,TECHNIQUE,1
c11e0c6d-0f9d-4ef3-82d8-a04e24f05606,RESPONSE MONITORING,entity,TECHNIQUE,1
c7574757-a6c4-4f5f-8b2d-e146717bab73,EVALUATIVE LLMS,entity,MODEL,1
b12e25f0-a457-40a7-9189-0bbb846c7684,EMBEDDING DISTANCE METRICS,entity,METRIC,1
9f9b9ff9-99e8-4275-9a76-6bf63df9170a,ALERTING MECHANISMS,entity,TECHNIQUE,1
7d1fb989-636f-4d0d-9ef7-fcdf8271083e,USER INTERFACE,entity,FRAMEWORK,1
c9d17e90-6db6-4207-867b-2b4f94760199,CONTINUED PRETRAINING,entity,TECHNIQUE,1
602a4974-928d-4f27-8d73-e457b36e6ad7,RETRAINING METHODS,entity,TECHNIQUE,1
c26e05fa-90d9-491b-9bea-535748a2a18a,ACTIVE LEARNING,entity,TECHNIQUE,1
fdf59d11-b921-46c7-9c9c-31e4f17e5803,DATA QUALITY AND BIAS,entity,METRIC,1
6ab4c839-5764-4339-a8d4-e04756406472,VERSION CONTROL,entity,FRAMEWORK,1
18229865-dcca-4d77-a76f-9c80cde4770d,CONTINUOUS LEARNING,entity,TECHNIQUE,1
a43bac7a-380c-481c-bc40-0997b59c9c0c,TRANSFER LEARNING,entity,TECHNIQUE,1
9c1f187b-e0f1-451c-b344-5f85299980ac,META-LEARNING,entity,TECHNIQUE,1
8e13d4a0-c9d8-4327-b1a7-29527da2d6b4,EVALUATION DATASETS,entity,DATASET,1
87f3335c-467f-4fc0-a7e4-0742465ce946,ALERT THRESHOLDS,entity,PARAMETER,1
6e17a533-6569-45fe-a341-b4c9a08b2979,MULTIVARIATE DRIFT DETECTION,entity,TECHNIQUE,1
2f21d52d-0687-4125-889b-70ea9fe07284,ROLE-BASED ACCESS CONTROL,entity,FRAMEWORK,1
781571d0-5942-4bb8-bdf7-f186a5672c68,READABILITY,entity,METRIC,1
0af75a03-790c-4eeb-aa76-0c20fda93906,TOXICITY,entity,METRIC,1
1f3486ff-66ab-4bac-aacd-375693038f41,SENTIMENT,entity,METRIC,1
510d0cbf-8b6b-4e77-8862-ee4e0475c9d7,HALLUCINATION,entity,LOSS_FUNCTION,1
e2920158-35c1-4c7f-b9e6-a0a6e1fbad5e,PROMPT INJECTIONS,entity,TECHNIQUE,1
f6fee01f-df48-43d5-8601-0b12216afd62,MULTIVARIATE DRIFT,entity,METRIC,1
82fbdd53-ce40-4c09-a416-f2949b5bf5bc,PERFORMANCE DRIFT,entity,METRIC,1
6268f490-0f39-4798-9547-fb6607184b88,CUSTOM METRICS,entity,METRIC,1
686191d4-192d-420d-a437-d6f8e56c7cac,SLACK,entity,ORGANIZATION,1
4dc890c8-575c-4347-bba6-2149caaf34bf,PAGERDUTY,entity,ORGANIZATION,1
4a0f4f93-3b43-4dcf-97f8-0ece6dacd4a9,OPENAI,entity,ORGANIZATION,1
e8b0485e-97c9-463e-beac-f40f3c917971,AUTOTRAIN,entity,FRAMEWORK,1
f8753606-868b-473a-9d7a-56d9e251e42e,SETFIT,entity,FRAMEWORK,1
3cf4446d-3995-45eb-b2eb-2c30c43e92a6,SAGEMAKER,entity,FRAMEWORK,1
cbc64b8c-e626-4c46-b780-0f33f513a079,HARDWARE IMPROVEMENTS,entity,DOMAIN,1
a79cad53-370b-439a-a2c3-29a959f0f99d,COMPUTATIONAL RESOURCES,entity,DOMAIN,1
844cc091-a2e2-4f7c-8485-a4dc718b70e7,TRANSFORMERS LIBRARY,entity,FRAMEWORK,1
315cb8fd-3376-46ca-b364-565cb08b988f,TRAINER API,entity,API,1
4c17c2cd-a867-4d60-97fd-5e6302177d79,OPTIMUM,entity,TECHNIQUE,1
53daa49a-0317-4001-928b-344ff42f3226,QUANTISATION,entity,TECHNIQUE,1
d76bf00c-4c37-4f45-b8bc-005c45eca443,PRUNING,entity,TECHNIQUE,1
607345c9-8238-4004-acd3-674b97d47e78,MODEL OVERFITTING,entity,CHALLENGE,1
fb46f12c-17e0-485e-aef2-d75757ec153f,DATA PRIVACY,entity,CHALLENGE,1
56ea6ff9-d8b6-4128-88c4-fb3202b1d479,RESOURCE CONSTRAINTS,entity,CHALLENGE,1
af44e102-7ece-4798-8c6c-45c6a71ddca5,HUGGINGFACE MODEL HUB,entity,DATASET,1
ef043259-7647-4199-911c-cd6ce939e00e,AMAZON SAGEMAKER JUMPSTART,entity,ORGANIZATION,1
9192abf8-3686-4bb5-8a23-ba0860a94f08,EMR SERVERLESS,entity,TECHNIQUE,1
de9f7ae1-9629-4f4f-afcb-8166d99db418,APACHE SPARK,entity,TECHNIQUE,1
eb6946db-734c-4e33-a29e-8d13b00073f1,MODEL DISTILLATION,entity,TECHNIQUE,1
ff25d7ef-62e1-4ff8-9122-6f67c9643350,NLP SOLUTIONS,entity,DOMAIN,1
ce281494-e443-4207-b1fa-8cfe4470d3c5,OPTIMISATION TOOLS,entity,FRAMEWORK,1
d06efbc9-ae6c-4ae6-88ac-6d35ae8c3362,REAL-TIME APPLICATIONS,entity,DOMAIN,1
bd54a34e-c6ff-48d0-998c-b52f94c00a9f,AWS SERVICES,entity,ORGANIZATION,1
91fcad1f-5302-402a-9869-577cd8c237e8,OPENAI'S FINE-TUNING API,entity,FRAMEWORK,1
2f0b86f7-db19-4c15-a7d0-d9b613242b02,LLAMA 2,entity,MODEL,1
e0c17e4e-22b2-464d-8054-c0e3943be882,RETRIEVAL AUGMENTED GENERATION (RAG),entity,TECHNIQUE,1
98274d39-5a98-48c5-b027-7acf3d67dde3,ANTHROPIC CLAUDE,entity,MODEL,1
6683886e-a765-4483-b0b4-22e08f9be030,STABILITY AI,entity,MODEL,1
58b30d54-8c3f-4e77-b635-0e30e349a034,FINE-TUNING PROCESS,entity,TECHNIQUE,1
14bc8435-48c7-4515-894d-14287ed59ac1,API CALLS,entity,TECHNIQUE,1
0b6fdc12-a97f-4116-839c-d2df2f97cdd9,NVIDIA NEMO,entity,ORGANIZATION,1
4cd66b2e-3ebb-41c4-ad69-749d89cd3ede,FINE-TUNING API,entity,TECHNIQUE,1
52afd7ca-9000-4fa0-a45c-289947d68ab7,DATASET,entity,DATASET,1
4a08d2ac-879f-4f82-8105-2ee04b733e75,GPU-ACCELERATED TOOLS,entity,TECHNIQUE,1
a8aef9c1-3fe2-4f6d-9142-a95468455c99,NVIDIA TRITON INFERENCE SERVER,entity,FRAMEWORK,1
1fd77f82-8bcb-4700-8b97-56ee3863e275,NEURAL MODULE FACTORY,entity,FRAMEWORK,1
cc17a29c-7ba9-4f2f-a2b4-bd8fb11133f0,APPLICATION SCRIPTS,entity,FRAMEWORK,1
b2510e1e-2b0c-44d3-967a-c70455423f84,PRE-TRAINED MODELS,entity,MODEL,1
a919444a-03fe-40d6-a9b3-93d2837ba57f,DATA PREPARATION AND UPLOAD,entity,TECHNIQUE,1
c32ab185-2447-4dc8-884c-5aca6883d6e7,CURATING RELEVANT DATA,entity,TECHNIQUE,1
1897028c-efcb-4785-93eb-69da27fa8b24,LIMITATIONS OF OPENAI'S FINE-TUNING API,entity,DOMAIN,1
60b83293-49d8-4fad-8532-48dd31a107d0,COST,entity,METRIC,1
2825abaa-5349-47a7-9bd2-4774e24585a4,DATA PRIVACY AND SECURITY,entity,DOMAIN,1
fa659c90-4ed2-413d-b1f0-34a2a0120a88,VENDOR LOCK-IN,entity,DOMAIN,1
0836cca6-8a4e-4dc8-8d99-5c9eaefc47e4,LIMITED CONTROL OVER TRAINING PROCESS,entity,DOMAIN,1
0e5be693-87f4-4339-96ef-11c10ddbe318,NEMO CUSTOMIZER,entity,FRAMEWORK,1
05a5c8c3-ed86-4929-b98e-a9f95e363d2a,ENTERPRISE-READY MODELS,entity,MODEL,1
cde61f68-de4e-4c8e-b3ec-2ba0b94358cf,ACCURATE DATA CURATION,entity,TECHNIQUE,1
2173cd72-ba63-4ea0-91f8-fbbc622e2f48,IMPROVED PERFORMANCE FEATURES,entity,TECHNIQUE,1
3429b43d-700f-4408-9370-f196fd05bdad,"MODULAR, REUSABLE ARCHITECTURE",entity,FRAMEWORK,1
8c7b20c2-ccaa-44fe-bf1f-37ac4bdbe38c,COMPREHENSIVE WORKFLOWS,entity,FRAMEWORK,1
f325f4d3-f81c-4398-a6f3-afe99678f4d8,NEMO COLLECTIONS,entity,DATASET,1
ffa612f1-50ea-4053-90cb-3ad1fb0984c1,STABLE DIFFUSION,entity,MODEL,1
40fd769d-30fd-41d8-9df0-31bcc6fdbea6,NEMOTRON-3 8B FAMILY,entity,MODEL,1
eb7d23bf-a4ba-43b5-8f99-7ba048c84852,NVIDIA,entity,ORGANIZATION,1
a15ebfc7-de42-4b65-8ba8-7793b4942243,RIVA NMT,entity,MODEL,1
80294d9d-82c2-4ac1-aff5-35197c42e1cb,GEMINI,entity,MODEL,1
fd913f1b-dca3-4a6b-9b2a-dd3241e74d38,VISION LANGUAGE MODELS,entity,MODEL,1
e85742e5-1583-4163-b821-798429fca10c,MULTIMODAL AI,entity,TECHNIQUE,1
d3e0e5fe-bba0-4ebc-ba48-ff186430d296,CONTRASTIVE LEARNING,entity,TECHNIQUE,1
87eb2f98-b341-4032-8fce-635abbd0042d,DATA ENGINEERING,entity,DOMAIN,1
84e69e35-f65b-4725-881f-595942eb22cc,REINFORCEMENT LEARNING WITH HUMAN FEEDBACK,entity,TECHNIQUE,1
dcf1d2de-8c6b-48a2-8c30-6d1755bd653f,CUSTOMISED MODELS,entity,MODEL,1
9f31a823-291a-4942-8896-ea9492e00dd7,INFERENCE,entity,EVENT,1
a82ccb15-9979-4cf4-be2e-fba6ae06dbe9,GUARDRAILS,entity,FRAMEWORK,1
a712a753-38ee-4768-8a02-be828b324bdc,TASK-SPECIFIC DATASETS,entity,DATASET,1
184892c3-ea68-4d74-839c-5c180b5637b0,CLIP,entity,MODEL,1
e4d42471-fc39-488e-98d1-12704967f6e9,MULTIMODAL LARGE LANGUAGE MODEL,entity,MODEL,1
bd010c77-5841-4dd4-ad09-6e062bdf58ae,LLM-ADAPTERS,entity,TECHNIQUE,1
b4982a3e-54f8-4a61-9d0e-efeef8d83c13,IA³,entity,TECHNIQUE,1
456f3f27-5f56-49cb-89ed-7bbdde1f7cc1,DYLORA,entity,TECHNIQUE,1
bb51b330-90da-4fab-8fbb-ff014f1ac182,EFFICIENT ATTENTION SKIPPING,entity,TECHNIQUE,1
f6c84bba-1337-402a-a762-5c725a30c0ee,MEMVP,entity,TECHNIQUE,1
3bd2a9d6-c144-4ae9-84e0-e8609bf2147d,LOMO,entity,TECHNIQUE,1
d4934302-3036-4a77-be74-05693eadffe4,MEZO,entity,TECHNIQUE,1
7c6d6459-c7f9-43a0-a6a5-0c1c0ee66f6e,MEDVQA,entity,DOMAIN,1
eff52168-8089-400f-8fce-ac3d2f031d6f,VQA,entity,EVENT,1
96c8f8d7-9b8d-42ab-a32b-5505d5a65ea5,VISION TRANSFORMER,entity,MODEL,1
5222c5f2-cc6b-4e13-9d60-2bf9c99d510d,LLAMA2-CHAT(7B),entity,MODEL,1
89514506-229a-4caa-8c8a-951d5c4c8d72,VISUAL ENCODER,entity,MODEL,1
fca62d85-fc72-4949-80aa-dfa6c3fac625,TEXT ENCODER,entity,MODEL,1
fda116fe-365a-4102-8116-e10d48f4606e,EMBEDDINGS,entity,EMBEDDING,1
47ce24d6-7f0a-4fd0-bf30-2ab5ad948e7e,CAPTION CONVERSION,entity,TECHNIQUE,1
bdda7ed0-053a-4609-a80b-4480b7f5f45b,ZERO-SHOT PREDICTION,entity,TECHNIQUE,1
c799aa30-87f6-4ade-b395-375e3d67ef2b,PROJECTION LAYER,entity,MODEL,1
d933170e-010f-4f31-be61-463a58158731,MULTIMODAL PROMPT TEMPLATE,entity,FRAMEWORK,1
63415027-a43b-4f91-929e-121f63b2fa54,MINIGPT-V2,entity,MODEL,1
bbdf55d4-1998-43be-aabe-b83f1865c37e,ROCO MEDICAL IMAGE-CAPTION DATASET,entity,DATASET,1
924ff88e-738a-4c3b-aa1c-404d1f6c7b0f,MED-VQA,entity,DATASET,1
6262d66e-3285-470d-a11d-8029d022bdac,AUDIO OR SPEECH LLMS,entity,MODEL,1
5e59ee0d-97c9-4043-b210-1a8bcf189afd,HUBERT,entity,TECHNIQUE,1
a94c5dc2-2a66-46e5-adf3-5bdddc608f22,WAV2VEC,entity,TECHNIQUE,1
ea735cf8-c3e9-40e1-bbdc-8c033470d323,AUDIOMODEL,entity,MODEL,1
5a7e914e-243c-4809-9c85-182d26c0cf29,DALL-E,entity,MODEL,1
35d088e2-23f6-4d75-b19e-dfd82251bcd4,MULTIMODAL MODELS,entity,FRAMEWORK,1
c4ae6ba3-58eb-4578-a568-1cecc6f35943,IMAGEFEATURE,entity,PARAMETER,1
76860c22-c617-4d10-9f12-de1c77390354,INSTRUCTION TEMPLATE,entity,FRAMEWORK,1
4c9be055-76cd-4603-964a-9aeedc4ed7ac,VIDEO SUMMARISATION,entity,APPLICATION,1
f215330a-4b48-4a02-9452-f3d3b477622d,GESTURE RECOGNITION,entity,APPLICATION,1
e2700fc7-04f8-4d28-ab29-d9bf1e39a5bf,EDUCATIONAL TOOLS,entity,APPLICATION,1
ec026df6-2d4f-4a79-90a6-c71f21b79375,VIRTUAL ASSISTANTS,entity,APPLICATION,1
432df054-0499-4207-aa21-f475eb95f3e8,AUDIO TOKEN,entity,PARAMETER,1
77536384-0ba1-40d7-aabb-a8f534bcc068,VOCODER,entity,TECHNIQUE,1
41e715d0-f185-4561-8bc0-ed933a4db593,AUDIOLM,entity,MODEL,1
38c0d80f-ab48-4c0e-8465-2a3661817b0c,AUDIOPA,entity,MODEL,1
89c3002c-2583-4d7e-a69e-6d80efe6c989,WHISPER,entity,MODEL,1
742e04a1-ae7d-43c5-8f60-098c6de27d86,LAURAGPT,entity,MODEL,1
23564b05-14e6-4d5e-9abc-2bf8f81d3ddd,SPEECHGPT,entity,MODEL,1
194fa75a-7648-4df0-b60d-9c99266e0319,QWEN-AUDIO,entity,MODEL,1
f77c70b0-916b-41b6-b7a0-3e29c7152f60,FFMPEG,entity,TECHNIQUE,1
3ee54e7e-12d3-4fd2-b3d5-2f04a69965ae,WORD ERROR RATE,entity,METRIC,1
e5bf20cc-5bdd-44f1-a2ff-f29db47500c7,CHARACTER ERROR RATE,entity,METRIC,1
c4d9e6a7-e291-4a2a-9f37-8200ad000e73,FULL PARAMETER FINE-TUNING,entity,TECHNIQUE,1
c3790e13-2eb1-4629-881f-4261842db470,LAYER-SPECIFIC FINE-TUNING,entity,TECHNIQUE,1
bcd361ee-fbb4-47a4-9317-9ec9db469171,COMPONENT-BASED FINE-TUNING,entity,TECHNIQUE,1
b08e198b-5462-4ab5-924f-1af341907e01,MULTI-STAGE FINE-TUNING,entity,TECHNIQUE,1
949f985e-4220-4392-89e1-6112f9304ece,AUTOMATIC SPEECH RECOGNITION,entity,DOMAIN,1
2ab70ae1-6523-4bdf-8a2c-2eaa2de17640,PALM 1,entity,MODEL,1
628addfe-def9-474b-861b-b9d5d9a97db9,T5,entity,MODEL,1
04153b97-7a70-46db-8252-f98316d85586,BERT-LARGE,entity,MODEL,1
8dbbafb4-7ec9-410b-bb35-83e6366a7efa,QUANTIZED LORA,entity,TECHNIQUE,1
d43629ed-3ae4-4065-859d-067a8e70b8dc,SPIEL,entity,TECHNIQUE,1
94eb6286-0aea-4d52-91e4-f11cb2a87827,DEFT,entity,TECHNIQUE,1
0fdc1d18-c70b-448a-839c-a74961da235e,INFLUENCE SCORE,entity,METRIC,1
6eb731b0-891d-4b9c-b31b-defeb15bb8f2,CUSTOMER SERVICE AUTOMATION,entity,EVENT,1
4fa95f0e-c250-404e-89a8-c8eb829c89fe,SCALABILITY ISSUES,entity,DOMAIN,1
182f7847-d59f-4b19-8ca8-ccf2498301bb,MEMORY REQUIREMENTS,entity,METRIC,1
ad29e640-3c1d-4096-a179-00e0f5053b9a,DATA VOLUME,entity,METRIC,1
e00ae123-c087-4412-81aa-190db345c640,MIXED-PRECISION TRAINING,entity,TECHNIQUE,1
b5490b17-25e2-4de8-8eec-d0bc7dde54b0,GRADIENT CHECKPOINTING,entity,TECHNIQUE,1
1fa86254-9f74-4b01-9e70-1321c8d26895,DATA PRUNING,entity,TECHNIQUE,1
ed4e465c-7ed7-44c3-9b6b-b39518eecf15,FEW-SHOT LEARNING,entity,TECHNIQUE,1
fe00b0ce-0398-4e59-b7a1-4701c833d447,EFFORT SCORE,entity,METRIC,1
f48d31cf-2d1c-48cd-ae0a-6487479c055d,SURROGATE MODEL,entity,MODEL,1
8b6b988c-5f63-42e6-b04f-c78eb64af080,DEALREC,entity,MODEL,1
9177d481-fbbe-48a9-b00f-f943765dde4a,NVIDIA'S TENSORRT 3,entity,FRAMEWORK,1
0b571cd7-363f-43f0-b407-8841028f73f6,FAIRNESS INDICATORS,entity,TOOL,1
5a620689-ba58-46cf-970e-b7d4dcf47cfd,FAIRBERTA,entity,FRAMEWORK,1
bdfd88ab-68ba-4a0b-9d8e-5fd56e6d5f0f,BIAS,entity,DOMAIN,1
8f17604c-0e59-43e9-90b3-5f636dc717fc,PRIVACY CONCERNS,entity,DOMAIN,1
c04fb8ba-552a-43ac-9a7b-f88871e9bcea,FAIRNESS-AWARE FINE-TUNING,entity,TECHNIQUE,1
420456c7-79a2-4d4a-8969-3563cb0e0724,DIFFERENTIAL PRIVACY,entity,TECHNIQUE,1
c1184499-502f-4bf7-b5c8-eb156fbd3b2d,FEDERATED LEARNING,entity,FRAMEWORK,1
e122bfb4-fb49-4bf2-b423-7f5b534b25e2,ADVERSARIAL TRAINING,entity,TECHNIQUE,1
ce237f73-ba54-4ab1-a48e-92b2591f81c6,MICROSOFT'S ADVERSARIAL ML THREAT MATRIX,entity,FRAMEWORK,1
cfd00793-f950-4bdd-805d-61de1279036c,MODEL CARDS,entity,FRAMEWORK,1
afa2bcfe-7886-4bf1-afe3-689b1dd66654,TENSORFLOW PRIVACY,entity,FRAMEWORK,1
a76515f2-9b1d-4d2b-8221-934f045e0d29,FDKT,entity,FRAMEWORK,1
737c15ba-37b0-49c1-b662-0b39dbbb72bd,SMALL LANGUAGE MODELS,entity,MODEL,1
4d4d72bf-aae7-4ffc-806b-4527da154bf7,FINANCE,entity,DOMAIN,1
a4fc8d40-5350-4bad-acb1-c8a68959bcde,CUSTOMER SERVICE,entity,DOMAIN,1
820ba36a-913d-4dee-9d20-e11c1dff89c9,SECURITY,entity,DOMAIN,1
b3115a1d-c66d-492c-a2b9-8f6a5ab48812,PRIVACY,entity,DOMAIN,1
b8d99268-187d-4d1b-bb51-d9083a2ce87f,AI FACTSHEETS,entity,FRAMEWORK,1
8d1327f5-45ae-4f41-abc1-e0eda937d2ce,IOT,entity,TECHNIQUE,1
c4ecd80d-edf4-4893-9699-a1a13df265a2,EDGE COMPUTING,entity,TECHNIQUE,1
84343568-e3a0-4aa9-9f7d-a670492fa7f7,PREDICTIVE MAINTENANCE,entity,TECHNIQUE,1
7a4074c0-94a4-42d9-9107-3847cbb4ed87,SMART CITIES,entity,DOMAIN,1
25915e89-dbd7-4983-a2ac-ac46c6a15550,TREC,entity,BENCHMARK,1
9ac55db0-54ef-4f3e-b2b0-a241d01b7144,WMT,entity,BENCHMARK,1
7c9d9366-993e-485f-9c18-f6a2e78fdea4,XNLI,entity,DATASET,1
5dac0087-d158-44d9-9ed1-0f0a1b596ea1,PIQA,entity,DATASET,1
0cc087ad-3cd5-48aa-bd05-661f366f5d33,WINOGRANDE,entity,DATASET,1
6065a735-85f0-4df7-9fde-337c9bae70aa,RAFT,entity,TECHNIQUE,1
efd2e389-94eb-4e34-bc38-cf25c4d59383,FAIRNESS-AWARE FRAMEWORKS,entity,FRAMEWORK,1
b916b4b1-d531-4065-bbd5-3bf03118319b,PRIVACY-PRESERVING TECHNIQUES,entity,TECHNIQUE,1
ea27410b-ee9a-479e-b70f-2fe91f9b7ec4,ROBUST SECURITY MEASURES,entity,TECHNIQUE,1
27726212-dc90-499b-98fe-49b3e369bba6,ETHICAL AI DEPLOYMENT,entity,DOMAIN,1
9853beed-e51f-4602-9e46-1c59d99eb987,PALM,entity,MODEL,1
2be48f7b-f961-44f0-9604-152407a6b0d1,EFFICIENT ESTIMATION OF WORD REPRESENTATIONS,entity,ALGORITHM,1
b3c605ca-5f5b-4692-b9d7-8120a3153764,SURVEY OF REINFORCEMENT LEARNING FROM HUMAN FEEDBACK,entity,DOCUMENT,1
cb9bf1f9-3105-4e4b-995b-524ceb75661c,SURVEY ON EVALUATION OF LARGE LANGUAGE MODELS,entity,DOCUMENT,1
af3cda8f-edb0-4f65-8bec-67e3dfd1266f,BUILDING TRUST IN CONVERSATIONAL AI,entity,DOCUMENT,1
d8187505-e69d-4bcb-807b-4204c9fd97f4,CORAG,entity,MODEL,1
6fd3eeda-aa4c-4d19-8d4c-a425f6d19821,MICROSOFT CORPORATION,entity,ORGANIZATION,1
030403bc-0ab7-420f-a059-ae9c5b194455,RENMIN UNIVERSITY OF CHINA,entity,ORGANIZATION,1
f0350448-cdda-4367-81d4-0f4cf0969b20,KILT,entity,BENCHMARK,1
d40c20c2-8e84-47d0-8573-ffcaf540933e,NQ,entity,DATASET,1
0ec65d28-fab6-47d2-8bcc-1c582b4fb7e4,REJECTION SAMPLING,entity,TECHNIQUE,1
910aa169-46d8-476c-b2ad-55738df1df3d,MULTI-HOP QUESTION ANSWERING,entity,DOMAIN,1
d4e3df7a-3a62-428c-92b4-f1271d192e69,DENSE RETRIEVERS,entity,TECHNIQUE,1
bc8703ba-2876-4697-80c8-07cc1c2950bb,BI-ENCODER ARCHITECTURE,entity,TECHNIQUE,1
a99a0050-2002-4e52-ba38-0f642340ec06,APPROXIMATE NEAREST NEIGHBOR SEARCH,entity,TECHNIQUE,1
78b44813-aaaf-4cbc-8795-86a80d003527,QUERY REFORMULATION,entity,TECHNIQUE,1
74ee62e1-86fc-4b7c-a2a6-08c11d5f541b,DECODING STRATEGIES,entity,TECHNIQUE,1
08f1b42b-56f1-4b74-a1e1-5fb34444e25f,GREEDY DECODING,entity,TECHNIQUE,1
cf8b7673-944e-498e-85d6-e5cd5ef14b0d,BEST-OF-N SAMPLING,entity,TECHNIQUE,1
18c4a806-7e77-4f94-aeff-64876ebd120b,TREE SEARCH,entity,TECHNIQUE,1
ba9198e3-14ef-4a6c-aad6-52184c7ff20f,FOUNDATION MODELS,entity,MODEL,1
c8838ec7-b024-40d5-8903-b4f1ca94011c,LONG-TAIL FACTUAL KNOWLEDGE,entity,DOMAIN,1
934d450c-d747-4d56-9816-2120c1c5d4b2,TOKEN CONSUMPTION,entity,METRIC,1
5b1ec121-f3f1-423a-b19f-b6bfb674361e,EM SCORE,entity,METRIC,1
3ceeb122-e4e2-4439-82fc-5925db1592ca,RAG,entity,DOMAIN,1
1247bc00-51eb-4a02-a6ec-a6aa089be268,FLARE,entity,TECHNIQUE,1
2b490889-9e48-4d8a-98ef-17403d9b4369,ITER-RETGEN,entity,TECHNIQUE,1
4c3681ac-d57c-4090-8631-0456b91ec36e,IRCOT,entity,TECHNIQUE,1
e05f0218-e266-4dcf-8bea-582d6730c968,SELF-RAG,entity,TECHNIQUE,1
5afd4f63-af54-4425-83ae-faafe700c849,AUTO-RAG,entity,TECHNIQUE,1
0f2be2dd-03b8-4022-a54b-d724642f1d4c,CHAIN-OF-THOUGHT,entity,TECHNIQUE,1
aae070fd-9857-48b2-85c6-0a51ca53761e,TREE-OF-THOUGHT,entity,TECHNIQUE,1
ba7711e6-2a76-4f4d-9a1c-d379f4fc56b5,STAR,entity,TECHNIQUE,1
37d51870-0029-43c3-9295-f198833e60d4,OPENAI O1,entity,MODEL,1
b45659a0-9679-44d9-8dde-42650bc21a4d,LONGRAG,entity,TECHNIQUE,1
baab7012-e3fe-4128-86b0-2d579719c469,ITERDRAG,entity,TECHNIQUE,1
bf636d18-a561-4dce-a0f3-4d3ec998d375,SEARCH-O1,entity,MODEL,1
57ad4b31-4b4a-494b-ac22-9fa251c31b35,QWQ,entity,MODEL,1
dfdecb1e-2d55-4b60-bb52-d02e92846646,RETRIEVAL CHAIN,entity,FRAMEWORK,1
ab5adf62-a136-4770-8511-516f9898ff34,QUERY,entity,PARAMETER,1
1bb709e0-68db-4721-a050-f69203ab2fff,SUB-QUERY,entity,PARAMETER,1
12df3e9a-91f8-4d6a-b9df-5e3151474696,SUB-ANSWER,entity,PARAMETER,1
639e9a98-d090-4038-9709-1d3d0ed2d471,TOPK,entity,PARAMETER,1
b604f564-a316-4cec-aed3-4387020a6102,A,entity,PARAMETER,1
b99c1a85-8d67-40f1-8016-18f2704e0805,Q,entity,PARAMETER,1
b7341e1b-c230-40e7-87a3-da54d6169316,A1:L,entity,PARAMETER,1
c42d74de-946e-4079-91b3-420cf9f7a9d6,Q1:L,entity,PARAMETER,1
817d5b3f-1f52-4743-b239-dc9f1ee317b8,MULTI-TASK LEARNING,entity,FRAMEWORK,1
2ae2ebfd-a334-4442-a6b8-638830730bdf,MULTI-TASK LEARNING FRAMEWORK,entity,FRAMEWORK,1
8bb37cc8-3652-4e71-a636-fc8ffc688b25,NEXT-TOKEN PREDICTION,entity,TECHNIQUE,1
3e24b14b-6804-4700-b5e0-d49f22bbc904,LLAMA-3.1-8BINSTRUCT,entity,MODEL,1
f0cfbae8-e6c2-40c5-b1c0-123a9fb0ed91,E5-LARGE,entity,RETRIEVER,1
134100a4-0305-4b27-a74a-667988c95c51,2WIKIMULTIHOPQA,entity,DATASET,1
1735ce9e-48ff-4604-a2d4-11a5ae7eb2b9,HOTPOTQA,entity,DATASET,1
eebba713-749f-425b-adab-9a3bbc9b12ba,BAMBOOGLE,entity,DATASET,1
95ecdc62-1f79-427f-98d1-2268afb93216,MUSIQUE,entity,DATASET,1
2d37bfab-0d44-4a1c-8a61-0fe9aa095a89,EXACT MATCH,entity,METRIC,1
e34e5b08-9454-4a02-b2d5-6ddb66153f90,PENALTY SCORE,entity,METRIC,1
8ec03d9c-0c59-4969-9f04-35237c674925,SEARCH-O1-32B,entity,MODEL,1
abdb2024-f226-407f-b540-70fa981e5f01,LLAMA-8B,entity,MODEL,1
1897bb26-c98e-4656-82be-de722865be6b,CORAG-8B,entity,MODEL,1
b86c8a32-e2e3-44b8-bedc-75e8b6e78c1f,KILT BENCHMARK,entity,BENCHMARK,1
52b35e12-8647-4469-b8d5-68d1792cfbf4,KILT-RAG,entity,MODEL,1
36a89c78-c998-4b2b-80a2-254a21173e06,SEAL,entity,MODEL,1
fcca4106-3451-42d5-a92b-4e27ee953c8c,ATLAS-11B,entity,MODEL,1
a9b348b4-20fd-4bb4-b9a4-21ba4b7393dc,RA-DIT 65B,entity,MODEL,1
67fb4778-22ac-4bf5-84e5-1f55467ac828,FID WITH RS,entity,MODEL,1
7865aa2d-e924-4906-8677-d4e32c3e9680,ITERATIVE REJECTION SAMPLING,entity,TECHNIQUE,1
dd2a8875-f0d9-4975-84cf-3e960934cd68,WEAK-TO-STRONG GENERALIZATION,entity,TECHNIQUE,1
3109ccff-b270-48c6-8031-4ffe3e4f533a,QUERY REWRITING,entity,TECHNIQUE,1
a1640d91-4ca9-4745-9a8b-79f58442252c,AKARI ASAI,entity,PERSON,1
0d13fbbf-f543-4d62-b302-83d54a65cb22,YIZHONG WANG,entity,PERSON,1
33fc3f9d-07d8-4555-aa12-620ac51c3a3b,AVIRUP SIL,entity,PERSON,1
d2d61723-54fb-446d-bdd4-503f542deb19,HANNANEH HAJISHIRZI,entity,PERSON,1
df9438fa-aad9-44bf-b529-2e597c00a811,MICHELE BEVILACQUA,entity,PERSON,1
662fa551-fd84-4e58-b943-ecfebdae89c3,PATRICK S. H. LEWIS,entity,PERSON,1
e53c5e9a-aa07-452c-8278-5baa65ff2f8d,COLLIN BURNS,entity,PERSON,1
eb2208e8-3b63-4fea-90b7-4498017a9987,ABHIMANYU DUBEY,entity,PERSON,1
7b53e6ed-2cfb-47bd-a7e5-51f4f4bf37f5,TRIVIAQA,entity,DATASET,1
33edc701-9705-4a5d-b2b2-8bd506d957d1,GAO,entity,PERSON,1
125b7542-0f49-49e5-b29d-e462a550f025,XINYU GAO,entity,PERSON,1
993be748-89ef-4251-bf4b-a7639c74e7e9,KANGXIANG JIA,entity,PERSON,1
c23c5d8a-b9a5-4ad3-841c-1b11f434d13a,JINLIU PAN,entity,PERSON,1
e0de9043-2c78-4962-b91e-62686de910dd,YUXI BI,entity,PERSON,1
bcc9b5d5-3250-4b44-bcf7-ee8959f7026f,YI DAI,entity,PERSON,1
9176efcb-bdd4-44c0-b047-81698ef2a2a3,JIAWEI SUN,entity,PERSON,1
2c8b5062-76b9-4330-aadf-4e74f8c9a070,QIANYU GUO,entity,PERSON,1
f434bda4-6b32-410e-9c4f-03181cdb63e4,MENG WANG,entity,PERSON,1
ddee87e6-a83b-40ae-8564-f449a01ed168,HAOFEN WANG,entity,PERSON,1
cb5ac374-5044-4dfb-937a-a5e1d6612cbe,RSTAR-MATH,entity,MODEL,1
a0382475-2b0c-4d43-a7ff-f14a4920c1aa,MULTI-HOP QA DATASET,entity,DATASET,1
44d0942f-7175-4b86-8a13-d82ee9593205,ATLAS,entity,MODEL,1
66280169-6d64-4e06-be37-06a2cc71e410,ACTIVE RETRIEVAL AUGMENTED GENERATION,entity,TECHNIQUE,1
517541ee-e0c7-427f-aac9-0bfca0a4bbb8,FLASHRAG,entity,MODEL,1
c77dae9f-a803-4daf-ba7c-27e8458dcaf2,DENSE PASSAGE RETRIEVAL,entity,TECHNIQUE,1
7d5fb5d8-d9bc-4a45-b264-30688cd2b58c,LI LYNA ZHANG,entity,PERSON,1
c7ef38d0-8a84-40e9-88b6-1b15d734a78a,YIFEI LIU,entity,PERSON,1
86f08dfe-2a9c-432f-a364-232ca2f83a44,NING SHANG,entity,PERSON,1
a23828fc-092a-4e76-bdd8-9a95fdc93df5,YOURAN SUN,entity,PERSON,1
f466b652-079c-4df5-9fec-62603d0c4d61,YI ZHU,entity,PERSON,1
cadc7038-8ca4-42e2-9531-224a5cb92a23,FAN YANG,entity,PERSON,1
541c84cf-e0dc-4d95-8dcc-b2f0456b7e8e,MAO YANG,entity,PERSON,1
e2038aca-54ca-4cc6-8d4c-3b0b4b248ad8,SEBASTIAN HOFSTÄTTER,entity,PERSON,1
3c031990-420c-4ef9-adde-ca40f793259e,JIECAO CHEN,entity,PERSON,1
b9ef6d0e-a046-4abf-84a7-475202352acd,KARTHIK RAMAN,entity,PERSON,1
d4043839-a975-466f-a6b1-ecfe489a09b2,HAMED ZAMANI,entity,PERSON,1
cb1fa912-94f2-4c88-abd3-a00e7719f9b7,AARON HURST,entity,PERSON,1
f4b05b1c-ed7b-445c-b2e8-b4c8e511c233,ADAM LERER,entity,PERSON,1
ee50c76d-6d2f-498b-9a1d-b8fc4baec5be,ADAM P GOUCHER,entity,PERSON,1
d6f9acf0-1cc9-4257-a668-503bb372f094,ADAM PERELMAN,entity,PERSON,1
1adba53f-d26e-4688-864d-665118affb5f,ADITYA RAMESH,entity,PERSON,1
494707d3-6129-43e9-b851-88a354e7f74c,AIDAN CLARK,entity,PERSON,1
f740cc38-2243-4469-ac21-23d35e0156e0,AJ OSTROW,entity,PERSON,1
fa58ea67-45da-4f71-8bcc-82ea45f3fed8,AKILA WELIHINDA,entity,PERSON,1
6733465a-f6ae-47a2-9f4b-fda0f3b6f279,ALAN HAYES,entity,PERSON,1
dc6ca841-7caf-44d6-8168-37d631e3d986,ALEC RADFORD,entity,PERSON,1
18260960-9440-4690-b28b-231a7d86f586,GRAHAM NEUBIG,entity,PERSON,1
5d3de51f-aff7-426d-8cb5-c5812f75cb75,ZHENGBAO JIANG,entity,PERSON,1
a9221163-4347-4197-b8e0-ab9b6fe50f70,FRANK XU,entity,PERSON,1
f6885dea-813f-487d-94a0-8a59bbc85213,LUYU GAO,entity,PERSON,1
9b8c97b6-0bee-44ba-b063-8a48d99dacf2,ZHIQING SUN,entity,PERSON,1
b726b5f7-9d41-416b-8e33-715e31c2dc82,QIAN LIU,entity,PERSON,1
f89dca40-b5e8-4ee0-8f34-bea0be29bf38,JANE DWIVEDI-YU,entity,PERSON,1
b01c9430-3634-429d-85b3-7814a9ecbc1c,YIMING YANG,entity,PERSON,1
fad83313-f853-49fd-839a-592a1fcdbabd,JAMIE CALLAN,entity,PERSON,1
8226e40d-d822-4595-a639-f13518d6f4bc,MANDAR JOSHI,entity,PERSON,1
3931eb4d-9ed0-4b22-a226-9b49913918d4,EUNSOL CHOI,entity,PERSON,1
803b6a63-58a3-4159-9b13-97c2bfef7640,DANIEL WELD,entity,PERSON,1
3b2fc8cf-0d9d-48b6-8c69-5a5ab14f358e,LUKE ZETTLEMOYER,entity,PERSON,1
d914d9d3-ac09-4cf8-b7b1-1fd938d69b25,VLADIMIR KARPUKHIN,entity,PERSON,1
90a64e37-8618-4447-917e-d5a9a159f2ad,BARLAS OGUZ,entity,PERSON,1
9303084b-26ad-41ff-906a-53ae9288aa19,SEWON MIN,entity,PERSON,1
52378947-29d9-441f-b49e-23428793d3b3,PATRICK LEWIS,entity,PERSON,1
522aa229-62f5-4c9d-abe7-c0c22ea5468b,LEDELL WU,entity,PERSON,1
9395290c-1e6e-468d-850f-9d9c06925b51,SERGEY EDUNOV,entity,PERSON,1
f8156192-82c6-4469-898b-9ba0c41c4a14,DANQI CHEN,entity,PERSON,1
a16da2a5-a29f-4446-88de-55b8b6a31bc1,WEN-TAU YIH,entity,PERSON,1
f85ae05e-f65a-4365-8bb7-757326a50879,EDUNOV,entity,PERSON,1
a43cbfa8-6418-40b8-b3d3-f332141473a1,ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,entity,ORGANIZATION,1
3f9d3d1e-fdcb-45c9-a21e-f2cfda827d21,EMNLP,entity,EVENT,1
8c0b01ed-7f2c-48e7-8632-ac73e921fac6,NATURAL QUESTIONS,entity,DATASET,1
5406e858-7e63-40e8-b9a9-73bfe191f60f,NEURIPS,entity,EVENT,1
ee9758a9-3b29-4c5b-9952-26a48079647d,SIGIR,entity,EVENT,1
44f1225c-58e1-44d2-9aaf-65522be89bca,FINE-TUNING LLAMA,entity,TECHNIQUE,1
68dac2b4-3ab8-4026-89b7-e64ccb95af7e,MULTI-TASK RETRIEVAL,entity,TECHNIQUE,1
5a95ed3d-10cb-4733-883b-f7faf71d6ee0,JIAJIE JIN,entity,PERSON,1
9d524af7-144e-4415-8694-34d5f55c5b5c,YUJIA ZHOU,entity,PERSON,1
0b420244-da13-4bb4-9b6f-8435bc17398e,YUYAO ZHANG,entity,PERSON,1
5c77b559-3fab-4b36-a568-17ca6eaab08f,PEITIAN ZHANG,entity,PERSON,1
ae0d9e1a-e770-426f-8ea1-445901e364ee,ZHICHENG DOU,entity,PERSON,1
d35cd16b-4115-4ad7-afb6-10434c29e156,ETHAN PEREZ,entity,PERSON,1
3a5326fa-ab34-4de6-956e-e8b6b8b7c48f,FABIO PETRONI,entity,PERSON,1
46c17612-cff8-4ee9-8215-7a9357e07d78,NAMAN GOYAL,entity,PERSON,1
aa3d6e41-890e-4d54-894c-8d59749c4ae1,HEINRICH KÜTTLER,entity,PERSON,1
f568984a-8f2a-49f2-b999-9e16ce538210,MIKE LEWIS,entity,PERSON,1
2e1fedfb-9891-48fd-98f1-412f04f8fbae,TIM ROCKTÄSCHEL,entity,PERSON,1
b390cff2-5ab7-4287-853e-bd00783d6ef7,SEBASTIAN RIEDEL,entity,PERSON,1
a4fe7d60-02f5-4796-8711-a5ad8a152ddd,DOUWE KIELA,entity,PERSON,1
02ff59ba-9274-4a4e-9251-2bb953779956,RAIA HADSELL,entity,PERSON,1
df4b65b3-b0fb-4f12-ab6b-66c997a7dff1,MARC'AURELIO RANZATO,entity,PERSON,1
ef749959-e3c3-40ad-8828-c8cb34a48b0f,GRACE HUI YANG,entity,PERSON,1
933d196e-302b-4c3a-98de-bdef29cfdaa6,HONGNING WANG,entity,PERSON,1
848b0d46-bd6a-4eb5-9c0b-ca87f0852034,SAM HAN,entity,PERSON,1
4d69cbe2-bc78-40d8-a5e2-5263057eadc6,CLAUDIA HAUFF,entity,PERSON,1
b98ac882-00c0-4ee4-aeaa-946d6ed34536,GUIDO ZUCCON,entity,PERSON,1
c63ecd15-345f-4470-93d5-b15745e0deb7,YI ZHANG,entity,PERSON,1
14d9af86-9538-48a3-85b8-fac84a364f5d,JEAN MAILLARD,entity,PERSON,1
0a9f5b02-d19d-4b83-9b3b-792caa18bc85,VESELIN STOYANOV,entity,PERSON,1
535e94d8-4924-4ee9-9379-3ddce0f911dd,GARGI GHOSH,entity,PERSON,1
620ff67e-85fb-45ee-91df-76324e4925b0,WEBGPT,entity,MODEL,1
77bc50b8-9136-477d-8a0b-be54aadb7f3b,LAMA,entity,MODEL,1
9bcddc23-a433-4683-8b8c-b77d48ffed21,RA-DIT,entity,MODEL,1
0f494d7f-a3d5-4c74-9e46-d7aef2e1c111,JEFF WU,entity,PERSON,1
3333d6d0-8b8f-40ed-9922-267214b0e35b,LONG OUYANG,entity,PERSON,1
f0d2df55-7c0e-4e70-9c72-735d4045c31d,CHRISTINA KIM,entity,PERSON,1
de37befe-1af8-41b0-bb45-d9441d5ed1f5,CHRISTOPHER HESSE,entity,PERSON,1
0bf1dae5-c41a-44dd-86c5-0a468af7f40c,SHANTANU JAIN,entity,PERSON,1
9be3a45b-4cff-473e-802a-5e5a9eb6ba55,VINEET KOSARAJU,entity,PERSON,1
95ba7152-6d82-4f8c-bce4-00b3e21a2b79,WILLIAM SAUNDERS,entity,PERSON,1
cae8f96f-0e42-454f-9954-f3257de47b00,ALEKSANDRA PIKTUS,entity,PERSON,1
49648316-67e1-401e-b23b-d67a6c23962e,ANGELA FAN,entity,PERSON,1
aa3803c7-47ac-4dc7-8a18-5bb0302b876f,MAJID YAZDANI,entity,PERSON,1
a4a03741-f3c4-4f9c-8465-322609b4f251,NICOLA DE CAO,entity,PERSON,1
0bd3f0dd-a3ff-49cf-b5e0-36a2ebdaa34d,JAMES THORNE,entity,PERSON,1
2ce09d4d-5e5d-48a9-a0d1-550d26250727,YACINE JERNITE,entity,PERSON,1
5789a26d-0069-43bd-9fc8-8f7b800914d1,VASSILIS PLACHOURAS,entity,PERSON,1
1a083afa-0ff2-437b-abc2-0b3c2cb080d8,NOAH SMITH,entity,PERSON,1
b319e2c8-8d86-46bf-83a6-88973aa8a793,SQUAD,entity,DATASET,1
9ad94f64-fe68-4763-9a31-bacc8e6b4ada,PRANAV RAJPURKAR,entity,PERSON,1
1c25414e-4cd9-4210-9aaa-8f4a2c51b2a0,JIAN ZHANG,entity,PERSON,1
bcba746d-d4f0-4763-9567-7418e241f823,KONSTANTIN LOPYREV,entity,PERSON,1
10d86e5b-496e-48d3-9176-d086f5c26949,PERCY LIANG,entity,PERSON,1
1a7e1d24-a015-4183-bb93-ee072b32e4e9,ZHIHONG SHAO,entity,PERSON,1
6bde2a14-dd4d-405c-b11c-1f1f8f2af402,YEYUN GONG,entity,PERSON,1
0b745412-2a60-460c-9a53-6c9be9c08425,YELONG SHEN,entity,PERSON,1
b2004929-3eff-412e-a7a1-687270f53f22,MINLIE HUANG,entity,PERSON,1
fe6f1c16-d11c-48ed-b7bc-59c4d8ad8750,NAN DUAN,entity,PERSON,1
17fade61-1882-4a4e-b228-2a6cd1181529,WEIZHU CHEN,entity,PERSON,1
4201c6e0-f4ca-4f8e-923c-3368b872e730,GEMINI TEAM,entity,ORGANIZATION,1
567c64e7-a877-4de7-948b-7938106cd0fe,PETKO GEORGIEV,entity,PERSON,1
832740d5-6311-4d7a-a2c0-cd7daf533c55,VING IAN LEI,entity,PERSON,1
faa7944d-49c2-481c-8196-a81fdb7dbc6f,RYAN BURNELL,entity,PERSON,1
c13dfa73-5c9c-49a8-8658-d0a6b1cd5512,LIBIN BAI,entity,PERSON,1
762de9c4-e570-49f1-aff6-19c5d8ebb25d,ANMOL GULATI,entity,PERSON,1
82f65b5f-77a5-42b9-bcfd-5adf334f240f,GARRETT TANZER,entity,PERSON,1
dbbde0aa-dfc7-4972-b423-f21ab028fa1c,DAMIEN VINCENT,entity,PERSON,1
6f28ea67-b1d1-405c-a066-a703df754447,ZHUFENG PAN,entity,PERSON,1
c08e277d-420a-4d57-a6c6-dab28d085aca,SHIBO WANG,entity,PERSON,1
978d703c-7d9a-454b-b526-b980ad989fcf,MEASURING AND NARROWING THE COMPOSITIONALITY GAP,entity,EVENT,1
f7503961-752c-4e00-ad34-29b4b065cb74,ENHANCING RETRIEVAL-AUGMENTED LARGE LANGUAGE MODELS,entity,EVENT,1
b1ade9e2-30cc-47fb-9428-39f15aa4278f,GEMINI 1.5,entity,MODEL,1
82dbed9c-dbfa-4a7c-8979-58102c97f334,MULTIHOP QUESTIONS VIA SINGLE-HOP QUESTION COMPOSITION,entity,EVENT,1
87a4db30-37ee-43ad-8286-5ba1341d8130,INTERLEAVING RETRIEVAL WITH CHAIN-OF-THOUGHT REASONING,entity,EVENT,1
9086075a-ebd8-4f3e-8612-2a3cd81f4145,TEXT EMBEDDINGS BY WEAKLY-SUPERVISED CONTRASTIVE PRE-TRAINING,entity,EVENT,1
e692d6bf-3468-4325-87ef-7fb067095d52,IMPROVING TEXT EMBEDDINGS WITH LARGE LANGUAGE MODELS,entity,EVENT,1
6b952853-66cc-4573-ba8c-38cd6b8a5387,LIANG WANG,entity,PERSON,1
16a44cde-ceaf-4016-ba59-b7b0d1e7818b,NAN YANG,entity,PERSON,1
efb3fa9c-e19a-4606-87d2-656c9a0bb782,XIAOLONG HUANG,entity,PERSON,1
8ea1118a-b690-4c84-8454-51afba3d00e2,LINJUN YANG,entity,PERSON,1
0fcc40c0-2f79-4e7a-9e0e-431a013a897c,RANGAN MAJUMDER,entity,PERSON,1
e257c5ac-e291-49ac-887d-c237e7e27089,FURU WEI,entity,PERSON,1
e3699806-60bc-498c-8021-1653af9036d1,JASON WEI,entity,PERSON,1
04996160-9636-4562-bab5-1d4e236d19a2,XUEZHI WANG,entity,PERSON,1
c1b765e9-8d22-4324-8a3c-29235c527c2a,DALE SCHUURMANS,entity,PERSON,1
af80ec43-415f-4bde-990f-380e092ae777,MAARTEN BOSMA,entity,PERSON,1
a1f2487a-0c86-4293-882c-7a5a3ac0d229,BRIAN ICHTER,entity,PERSON,1
dd81dd0f-b479-4693-bead-48848b241c40,FEI XIA,entity,PERSON,1
8e58d113-8127-4f32-8fea-f0b57fe6702b,ED H. CHI,entity,PERSON,1
762f112d-0b3f-4150-b0cd-eb6c93ae963b,QUOC V. LE,entity,PERSON,1
9eba9149-2562-4763-b2ed-35007a130da3,DENNY ZHOU,entity,PERSON,1
5395d9b1-3a9d-4010-84b5-0bf299fc7082,QWEN2,entity,MODEL,1
b61928d8-293a-4975-a762-7413117c13f2,NEURIPS 2022,entity,EVENT,1
4af15675-e6fc-4cc6-847f-aee219b7e184,NEURIPS 2023,entity,EVENT,1
00aafe91-5fd2-41d7-93dd-e7d7e1e57824,DIAN YU,entity,PERSON,1
a90a31fc-a12f-475e-9a14-dc86bfaf81ca,JEFFREY ZHAO,entity,PERSON,1
ac0e9774-64fd-46f2-94d6-0329047ac73c,IZHAK SHAFRAN,entity,PERSON,1
068e21de-4d0c-4a71-a0dc-b447e40f69ce,TOM GRIFFITHS,entity,PERSON,1
2c777a4e-75b0-490c-8451-14cfd25320e7,YUAN CAO,entity,PERSON,1
ea2dc0b0-3b57-4633-a06c-8205627ba6f7,KARTHIK NARASIMHAN,entity,PERSON,1
fd3f4fe3-0bfd-4448-8cb2-3f84cc0eb656,ZHENRUI YUE,entity,PERSON,1
9ba2249d-b99c-402e-a4dd-3878791271fc,HONGLEI ZHUANG,entity,PERSON,1
45132bc3-3edf-4d47-a42b-4b9a5fd4521d,AIJUN BAI,entity,PERSON,1
2dbcd31f-8b50-4308-867e-dc625c3c8d94,KAI HUI,entity,PERSON,1
bff9b8ab-bd8e-47d3-a031-ff235641277d,ROLF JAGERMAN,entity,PERSON,1
643ce8c8-9dbf-485c-aeab-6ecbeeffdee0,HANSI ZENG,entity,PERSON,1
5feb6111-105f-4917-b9ec-d5406695653f,ZHEN QIN,entity,PERSON,1
ac9a426f-7728-4db9-919d-b2f66f758057,DONG WANG,entity,PERSON,1
3f56f6dd-dc85-417e-a4b6-adb3154a84c5,XUANHUI WANG,entity,PERSON,1
8c2b34c6-96e5-4c68-9411-7575cdba26a7,MICHAEL BENDERSKY,entity,PERSON,1
cd70b1c8-b63d-4e5f-b220-82cc0ba0e149,MULTI-HOP QA TRAINING,entity,EVENT,1
81bcc95d-660c-4dd7-a430-d260fff489c3,T-REX,entity,DATASET,1
101962af-ff95-4825-9ac6-54daa8e5f5ac,ZERO-SHOT RE,entity,DATASET,1
33d0f551-26a4-45ca-b9ac-37b23f064e9d,BLINK,entity,DATASET,1
165872e2-5bd3-42d0-81c1-645e3b86f78c,LLAMA-3-8B-BASE,entity,MODEL,1
46d3c077-9cc6-4002-8090-2eea614a9e3f,RANKLLAMA,entity,MODEL,1
067f0660-45c5-41ec-86ad-28810ed8996f,MULTI-HOP QA,entity,EVENT,1
2a8bd9c3-a968-4dd8-a85f-dd6be8eae6ff,KILT LEADERBOARD,entity,EVENT,1
c555a8f3-7675-4d38-a504-3333d5673cfa,EM,entity,METRIC,1
4e54ea8c-0132-4530-b471-2509bbee7037,SAMPLE RATIO,entity,PARAMETER,1
28e4fa7a-870f-4dd2-ad36-fda0ce7d2921,TEMPERATURE,entity,PARAMETER,1
881ec324-ba20-4204-aefc-eb3db9e071b0,KILT TRAINING SET,entity,DATASET,1
626492c5-0444-4019-8c99-6b1ab136dab0,CO-RAG,entity,MODEL,1
a614f36e-17cb-4cea-a468-51b630859c10,TRAINING LOSS,entity,METRIC,1
53957135-aebc-4f4d-baad-e040ba9a9685,MATHEW THOMAS REHWOLDT,entity,PERSON,1
3cb2f14f-62d6-4a79-9121-2fd7af22fb8d,WWE,entity,ORGANIZATION,1
a1e36695-8a4a-46e0-888e-dd7a3703ec43,JOHAN MJÄLLBY,entity,PERSON,1
f1f7feca-9c92-4df3-908d-eb69046ff92c,NEIL LENNON,entity,PERSON,1
6fe3e369-5ff4-4149-96d1-fac8213b2fe6,THOMAS PARKER SANBORN,entity,PERSON,1
77bf0f66-f4e3-4a85-b138-8fd4127cf5a2,GEORGE SANTAYANA,entity,PERSON,1
626ba683-b3eb-4890-83db-3f3fb0c93653,EMILY BRONTË,entity,PERSON,1
613d1a9e-ce7b-4530-ac01-1a3efef111f2,SPAIN,entity,GEO,1
5acab6c3-fd24-41ad-8757-783a0cddc3eb,UNIVERSITY OF NEW HAMPSHIRE,entity,ORGANIZATION,1
e4d42ec8-56d1-40d6-a66b-481d2e844a40,STONY BROOK UNIVERSITY,entity,ORGANIZATION,1
1054eff0-b913-4d34-a1a6-88241dbe6827,SUB-QUERY GENERATION,entity,TECHNIQUE,1
5b8b2d58-3baa-4500-bd28-36f906643764,INTERMEDIATE ANSWER GENERATION,entity,TECHNIQUE,1
8c28c5c3-10d0-4153-8c31-d2d79f55e137,FINAL ANSWER GENERATION,entity,TECHNIQUE,1
11669edd-9f78-4622-967b-09db2c81d5e6,LEARNING TO STOP,entity,TECHNIQUE,1
88d70f57-051a-4ec6-a599-bc7bdff52e0d,STONY BROOK,entity,GEO,1
a88a6526-916b-47e7-ad9e-ba9409fcaf41,NEW HAMPSHIRE,entity,GEO,1
7dc19d31-4f6e-48b4-a11c-6718591bf176,NEW YORK,entity,GEO,1
9ed73c3fa03f83895a135beea0b9167417e1259afd8d6a2995037542ed2b04b6c11d37acdfa4bd0c280f586919f89c51a6bab22295153fd5f74f0faaccffd271,"<missing-text>

The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities

(Version 1.1)

Venkatesh Balavadhani Parthasarathy, Ahtsham Zafar, Aafaq Khan, and Arsalan Shahid

@ CeADAR Connect Group

CeADAR: Ireland's Centre for AI, University College Dublin, Belfield, Dublin, Ireland { venkatesh.parthasarathy, ahtsham.zafar, aafaq.khan, arsalan.shahid } @ ucd.ie

Abstract

This technical report thoroughly examines the process of fine-tuning Large Language Models (LLMs), integrating theoretical insights and practical applications. It begins by tracing the historical development of LLMs, emphasising their evolution from traditional Natural Language Processing (NLP) models and their pivotal role in modern AI systems. The analysis differentiates between various fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, underscoring their respective implications for specific tasks.

A structured seven-stage pipeline for LLM fine-tuning is introduced, covering the complete lifecycle from data preparation to model deployment. Key considerations include data collection strategies, handling of imbalanced datasets, model initialisation, and optimisation techniques, with a particular focus on hyperparameter tuning. The report also highlights parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) and Half Fine-Tuning, which balance resource constraints with optimal model performance.

The exploration extends to advanced fine-tuning techniques and configurations like memory finetuning, Mixture of Experts (MoE) and Mixture of Agents (MoA), demonstrating how these methods harness specialised networks and multi-agent collaboration for improved outcomes. Proximal Policy Optimisation (PPO) and Direct Preference Optimisation (DPO) are discussed as innovative approaches to aligning models with human preferences, while the benefits of pruning and routing optimisations are examined for enhancing efficiency.

In the latter sections, the report delves into validation frameworks, post-deployment monitoring, and optimisation techniques for inference. It also addresses the deployment of LLMs on distributed and cloud-based platforms. Additionally, cutting-edge topics such as multimodal LLMs and fine-tuning for audio and speech processing are covered, alongside emerging challenges related to scalability, privacy, and accountability.

This report aims to serve as a comprehensive guide for researchers and practitioners, offering actionable insights into fine-tuning LLMs while navigating the challenges and opportunities inherent in this rapidly evolving field.

Contents

<missing-text>

<missing-text>

<missing-text>

<missing-text>

<missing-text>

Chapter 1

Introduction

1.1 Background of Large Language Models (LLMs)

Large Language Models (LLMs) represent a significant leap in computational systems capable of understanding and generating human language. Building on traditional language models (LMs) like N-gram models [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex linguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mechanism within Transformer architectures to efficiently manage sequential data and understand long-range dependencies. Key advancements include in-context learning for generating coherent text from prompts and Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human responses. Techniques like prompt engineering, question-answering, and conversational interactions have significantly advanced the field of natural language processing (NLP) [4].

1.2 Historical Development and Key Milestones

Language models are fundamental to natural language processing (NLP), leveraging mathematical techniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over several decades, language modelling has evolved from early statistical language models (SLMs) to today's advanced large language models (LLMs). This rapid advancement has enabled LLMs to process, comprehend, and generate text at a level comparable to human capabilities [5, 6].

Figure 1.1 shows the evolution of large language models from early statistical approaches to current advanced models.

1.3 Evolution from Traditional NLP Models to State-of-the-Art LLMs

Understanding LLMs requires tracing the development of language models through stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs), and LLMs.

1.3.1 Statistical Language Models (SLMs)

Emerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the likelihood of sentences within texts. For instance, the probability P ( S ) of the sentence 'I am very happy' is given by:

P ( S ) = P ( ω 1 , ω 2 , ω 3 , ω 4 ) = P (I , am , very , happy) (1.1)

This probability can be calculated using conditional probabilities:

P (I , am , very , happy) = P (I) · P (am | I) · P (very | I , am) · P (happy | I , am , very) (1.2)

Conditional probabilities are estimated using Maximum Likelihood Estimation (MLE):

<missing-text>

P ( ω i | ω 1 ω 2 · · · ω i -1 ) = C ( ω 1 ω 2 · · · ω i ) C ( ω 1 ω 2 · · · ω i -1 ) (1.3)

1.3.2 Neural Language Models (NLMs)

NLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors enable computers to understand word meanings",chunk,,1
adb58fb2ac4dc2e636420a23765cd5f718dd4f88e00555d7aa84533fc852b568bd9b80d652ce717022631ce0aeb9d9b625d6d8020e55b7eb40d912c43dc13616," ω i | ω 1 ω 2 · · · ω i -1 ) = C ( ω 1 ω 2 · · · ω i ) C ( ω 1 ω 2 · · · ω i -1 ) (1.3)

1.3.2 Neural Language Models (NLMs)

NLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors enable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector space where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons organised into layers, resembling the human brain's structure. The input layer concatenates word vectors, the hidden layer applies a non-linear activation function, and the output layer predicts subsequent words using the Softmax function to transform values into a probability distribution.

Figure 1.2 illustrates the structure of Neural Language Models, highlighting the layers and connections used to predict subsequent words.

1.3.3 Pre-trained Language Models (PLMs)

PLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language structures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This 'pre-training and fine-tuning' paradigm, exemplified by GPT-2 [8] and BERT [9], has led to diverse and effective model architectures.

1.3.4 Large Language Models (LLMs)

LLMs like GPT-3, GPT-4, PaLM [10], and LLaMA [11] are trained on massive text corpora with tens of billions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed

<missing-text>

by alignment with human values. This approach enables LLMs to understand human commands and values better.

1.4 Overview of Current Leading LLMs

LLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and conversational interaction. Advances in transformer architectures, computational power, and extensive datasets have driven their success. These models approximate human-level performance, making them invaluable for research and practical implementations. LLMs' rapid development has spurred research into architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and integrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions and creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating the latest developments [12].

Figure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.

1.5 What is Fine-Tuning?

Fine-tuning uses a pre-trained model, such as OpenAI's GPT series, as a foundation. The process involves further training on a smaller, domain-specific dataset. This approach builds upon the model's pre-existing knowledge, enhancing performance on specific tasks with reduced data and computational requirements.

Fine-tuning transfers the pre-trained model's learned patterns and features to new tasks, improving performance and reducing training data needs. It has become popular in NLP for tasks like text classification, sentiment analysis, and question-answering.

<missing-text>

1.6 Types of LLM Fine-Tuning

1.6.1 Unsupervised Fine-Tuning

This method does not require labelled data. Instead, the LLM is exposed to a large corpus of unlabelled text from the target domain, refining its understanding of language. This approach is useful for new domains like legal or medical fields but is less precise for specific tasks such as classification or summarisation.

1.6.2 Supervised Fine-Tuning (SFT)

SFT involves providing the LLM with labelled data tailored to the target task. For example, fine-tuning an LLM for text classification in a business context uses a dataset of text snippets with class labels. While effective, this method requires substantial labelled data, which can be costly and time-consuming to obtain.

1.6.3 Instruction Fine-Tuning via Prompt Engineering

This method relies on providing the LLM with natural language instructions, useful for creating specialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the quality of the prompts.

1.7 Pre-training vs Fine-tuning

Table 1.1 provides a comparison between pre-training and fine-tuning, highlighting their respective characteristics and processes.

<missing-text>

1.8 Importance of Fine-Tuning LLMs

 1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting it to specific tasks with reduced computation time and resources.

 2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring pre-trained features to the target task.

 3. Improved Generalisation: Fine-tuning enhances the model's ability to generalise to specific tasks or domains, capturing general language features and customising them.

 4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications, being computationally efficient and well-suited for specific tasks.

 5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, performing well across various applications without task-specific architectures.

 6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by adjusting to the nuances and vocabulary of the target domain.

 7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that already capture general language features.

1.9 Retrieval Augmented Generation (RAG)

A popular method to utilise your own data is by incorporating it into the prompt when querying the LLM model. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant data and using it as additional context for the LLM. Instead of depending solely on knowledge from the training data",chunk,,1
759dea32dd8bddec032bdfe47d91384a521198df10389687cc5b0da299074cfd1fff3f77339d66350d9526343f1e8ade6a97454329ddb15cc98b41cfd9bbb2fd," domain.

 7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that already capture general language features.

1.9 Retrieval Augmented Generation (RAG)

A popular method to utilise your own data is by incorporating it into the prompt when querying the LLM model. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant data and using it as additional context for the LLM. Instead of depending solely on knowledge from the training data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data retrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return relevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This process avoids the costs and time associated with fine-tuning or pre-training the model.

<missing-text>

1.9.1 Traditional RAG Pipeline and Steps

 1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking, and storing data in a vector database using indexing strategies like search indexing, vector indexing, and hybrid indexing.

 2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This can include simplification or vector transformation of queries for enhanced search efficiency.

 3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms such as TF-IDF, BM25, and deep learning models like BERT to interpret the query's intent and context.

 4. Prompt Augmentation: Incorporate relevant information from the search results into the original query to provide the LLM with additional context, enhancing response accuracy and relevance.

 5. Response Generation: Use the augmented prompt to generate responses that combine the LLM's knowledge with current, specific data, ensuring high-quality, contextually grounded answers.

1.9.2 Benefits of Using RAG

 · Up-to-Date and Accurate Responses: Enhances the LLM's responses with current external data, improving accuracy and relevance.

 · Reducing Inaccurate Responses: Grounds the LLM's output in relevant knowledge, reducing the risk of generating incorrect information.

 · Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisation's proprietary data.

 · Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without extensive model fine-tuning.

1.9.3 Challenges and Considerations in Serving RAG

 1. User Experience: Ensuring rapid response times suitable for real-time applications.

 2. Cost Efficiency: Managing the costs associated with serving millions of responses.

 3. Accuracy: Ensuring outputs are accurate to avoid misinformation.

 4. Recency and Relevance: Keeping responses and content current with the latest data.

 5. Business Context Awareness: Aligning LLM responses with specific business contexts.

 6. Service Scalability: Managing increased capacity while controlling costs.

 7. Security and Governance: Implementing protocols for data security, privacy, and governance.

1.9.4 Use Cases and Examples

 1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers from company documents, enhancing customer support.

 2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate informational queries.

 3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR and compliance, using company data.

1.9.5 Considerations for Choosing Between RAG and Fine-Tuning

When considering external data access, RAG is likely a superior option for applications needing to access external data sources. Fine-tuning, on the other hand, is more suitable if you require the model to adjust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing hallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to generating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can result in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data is scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments where data frequently updates or changes. Additionally, it is crucial to ensure the transparency and interpret ability of the model's decision-making process. In that case, RAG systems offer insight that is typically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation alongside example use cases.

<missing-text>

1.10 Objectives of the Report

1.10.1 Goals and Scope

The primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs. This involves exploring theoretical foundations, practical implementation strategies, and challenges. The report examines various fine-tuning methodologies, their applications, and recent advancements.

1.10.2 Key Questions and Issues Addressed

This report addresses critical questions surrounding fine-tuning LLMs, starting with foundational insights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from pre-training, and emphasises its role in adapting models for specific tasks. Key objectives include enhancing model performance for targeted applications and domains.

The report outlines a structured fine-tuning process, featuring a high-level pipeline with visual representations and detailed stage explanations. It covers practical implementation strategies, including model initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation methods, deployment challenges, and recent advancements are also explored.

1.10.3 Overview of the Report Structure

The rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters include an in-depth look at the fine-tuning pipeline, practical applications,",chunk,,1
fd707bba863fc24f3213c4f6a01fc8e26336f84b4e4b2a002437670a4f553f2af14352ff6309790d89d1438e566702ce92d64bba0ad94349a2f138e0bca21fc1,", including model initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation methods, deployment challenges, and recent advancements are also explored.

1.10.3 Overview of the Report Structure

The rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters include an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation metrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight ongoing research challenges, and provide insights for researchers and practitioners.

Chapter 2

Seven Stage Fine-Tuning Pipeline for LLM

Fine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct stages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal performance. These stages encompass everything from initial dataset preparation to the final deployment and maintenance of the fine-tuned model. By following these stages systematically, the model is refined and tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and contextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation, Training Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance.

Figure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages from dataset preparation to monitoring and maintenance.

2.1 Stage 1: Dataset Preparation

Fine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks by updating its parameters using a new dataset. This involves cleaning and formatting the dataset to match the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is composed of < input , output > pairs, demonstrating the desired behaviour for the model. For example, in instruction tuning, the dataset may look like:

###Human: $<Input Query>$ ###Assistant: $<Generated Output>$

Here, the 'Input Query' is what the user asks, and the 'Generated Output' is the model's response. The structure and style of these pairs can be adjusted based on the specific needs of the task.

2.2 Stage 2: Model Initialisation

Model initialisation is the process of setting up the initial parameters and configurations of the LLM before training or deploying it. This step is crucial for ensuring the model performs optimally, trains efficiently, and avoids issues such as vanishing or exploding gradients.

2.3 Stage 3: Training Environment Setup

Setting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure to adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the model's architecture and hyperparameters, and running training iterations to adjust the model's weights and biases. The aim is to enhance the LLM's performance in generating accurate and contextually appropriate outputs tailored to specific applications, like content creation, translation, or sentiment analysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.

<missing-text>

2.4 Stage 4: Partial or Full Fine-Tuning

This stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning updates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half fine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter layers, can be employed to partially fine-tune the model. This method attaches additional layers to the pre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges related to computational efficiency, overfitting, and optimisation.

2.5 Stage 5: Evaluation and Validation

Evaluation and validation involve assessing the fine-tuned LLM's performance on unseen data to ensure it generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure prediction errors, while validation monitors loss curves and other performance indicators to detect issues like overfitting or underfitting. This stage helps guide further fine-tuning to achieve optimal model performance.

2.6 Stage 6: Deployment

Deploying an LLM means making it operational and accessible for specific applications. This involves configuring the model to run efficiently on designated hardware or software platforms, ensuring it can handle tasks like natural language processing, text generation, or user query understanding. Deployment also includes setting up integration, security measures, and monitoring systems to ensure reliable and secure performance in real-world applications.

2.7 Stage 7: Monitoring and Maintenance

Monitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and reliability. This involves continuously tracking the model's performance, addressing any issues that arise, and updating the model as needed to adapt to new data or changing requirements. Effective monitoring and maintenance help sustain the model's accuracy and effectiveness over time.

Chapter 3

Stage 1: Data Preparation

3.1 Steps Involved in Data Preparation

3.1.1 Data Collection

The first step in data preparation is to collect data from various sources. These sources can be in any format such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to gather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats along with the corresponding Python libraries used for data collection.

3.1.2 Data Preprocessing and Formatting

Data preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step involves tasks such as cleaning the data, handling missing values, and formatting the data to match the specific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains some of the most commonly used data preprocessing libraries in python.

3.1.3 Handling Data Imbalance

Handling imbalanced datasets is",chunk,,1
d71a1297e87b844c19e7760a007680aa86930cbaed36e05b91972e837039fa43a994c16ba0e6b8f3da9de31c799503cebee77c63dff953ac09e7fb054ae9ade8," used for data collection.

3.1.2 Data Preprocessing and Formatting

Data preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step involves tasks such as cleaning the data, handling missing values, and formatting the data to match the specific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains some of the most commonly used data preprocessing libraries in python.

3.1.3 Handling Data Imbalance

Handling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several techniques and strategies are employed:

 1. Over-sampling and Under-sampling: Techniques like SMOTE (Synthetic Minority Oversampling Technique) generate synthetic examples to achieve balance. Python Library: imbalanced-learn imbalanced-learn provides various methods to deal with imbalanced datasets, in-

 Description: cluding oversampling techniques like SMOTE.

 2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class, setting class weights inversely proportional to the class frequencies.

 3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and focus training on hard negatives. Python Library: focal loss The focal loss package provides robust implementations of various focal loss func-

 Description: tions, including BinaryFocalLoss and SparseCategoricalFocalLoss.

 4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning algorithm, assigning a higher cost to misclassifying minority class samples.

 5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models and handle class imbalance.

Python Library:

sklearn.ensemble

Description: scikit-learn provides robust implementations of various ensemble methods, including bagging and boosting.

<missing-text>

 6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or proportional representation of each class.

 Python Library: sklearn.model selection.StratifiedShuffleSplit

 Description: scikit-learn offers tools for stratified sampling, ensuring balanced representation across classes.

 7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the minority class.

 Python Library: pandas.DataFrame.sample

 Description: pandas provides methods for sampling data from DataFrames, useful for data cleaning and preprocessing.

 8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohen's Kappa are more informative than accuracy when dealing with imbalanced datasets.

Python Library:

sklearn.metrics

 Description: scikit-learn offers a comprehensive set of tools for evaluating the performance of classification models, particularly with imbalanced datasets.

<missing-text>

3.1.4 Splitting Dataset

Splitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using an 80:20 ratio. Different techniques include:

 1. Random Sampling: Selecting a subset of data randomly to create a representative sample. Python Library: sklearn.model selection.train test split

 2. Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain class balance.

 Python Library: sklearn.model selection.StratifiedShuffleSplit

 3. K-Fold Cross Validation: Splitting the dataset into K folds and performing training and validation K times.

Python Library: sklearn.model selection.KFold

 Using a single data point as the validation set and the rest

 4. Leave-One-Out Cross Validation: for training, repeated for each data point. Python Library: sklearn.model selection.LeaveOneOut

Further details can be found in scikit-learn's documentation on model selection.

3.2 Existing and Potential Research Methodologies

3.2.1 Data Annotation

Data annotation involves labelling or tagging textual data with specific attributes relevant to the model's training objectives. This process is crucial for supervised learning tasks and greatly influences the performance of the fine-tuned model. Recent research highlights various approaches to data annotation:

 · Human Annotation: Manual annotation by human experts remains a gold standard due to its accuracy and context understanding. However, it is time-consuming and costly for large datasets [16]. Tools like Excel , Prodigy 1 , and Innodata 2 facilitate this process.

 · Semi-automatic Annotation: Combining machine learning algorithms with human review to create labelled datasets more efficiently. This approach balances efficiency and accuracy. Tools like Snorkel 3 use weak supervision to generate initial labels, which are then refined by human annotators [17].

 · Automatic Annotation: Fully automated annotation leverages machine learning algorithms to label data without human intervention, offering scalability and cost-effectiveness. Services like Amazon SageMaker Ground Truth 4 utilise machine learning to automate data labelling, although the accuracy may vary depending on the complexity of the task [18].

3.2.2 Data Augmentation

Data Augmentation (DA) techniques expand training datasets artificially to address data scarcity and improve model performance. Advanced techniques often used in NLP include:

 · Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with their semantic equivalents, thereby generating new data instances [19, 20].

 · Back Translation: Translating text to another language and then back to the original language to create paraphrased data. This technique helps in generating diverse training samples [21]. Tools like Google Translate API 5 are commonly used for this purpose.

 · Adversarial Attacks: Generating augmented data through adversarial examples that slightly modify the original text to create new training samples while preserving the original meaning [22]. Libraries like TextAttack 6 provide frameworks for such augmentations.

 · NLP-AUG 7 : This library offers a variety of augmenters for character, word, sentence, audio, and spectrogram augmentation, enhancing dataset diversity.

3.2.3 Synthetic Data Generation using LLMs

Large Language Models (LLMs) can generate synthetic data through",chunk,,1
52f039d8df41d8ef51a2fa96da9382048ae07f0c530332ecf1cbe5c82aacc14572314ff4ab63c8f89c0e4373ce6144fccd3095ca58804ca0d80fcfd85faee41b," Attacks: Generating augmented data through adversarial examples that slightly modify the original text to create new training samples while preserving the original meaning [22]. Libraries like TextAttack 6 provide frameworks for such augmentations.

 · NLP-AUG 7 : This library offers a variety of augmenters for character, word, sentence, audio, and spectrogram augmentation, enhancing dataset diversity.

3.2.3 Synthetic Data Generation using LLMs

Large Language Models (LLMs) can generate synthetic data through innovative techniques such as:

 · Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant and high-quality synthetic data [23].

 · Multi-Step Generation: Employing iterative generation processes where LLMs generate initial data that is refined through subsequent steps [24]. This method can produce high-quality synthetic data for various tasks, including summarising and bias detection.

 It is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using them for fine-tuning processes [25].

3.3 Challenges in Data Preparation for Fine-Tuning LLMs

Key challenges in data preparation include:

 1. Domain Relevance: Ensuring that the data is relevant to the specific domain for accurate model performance. Mismatched domain data can lead to poor generalisation and inaccurate outputs [26].

 2. Data Diversity: Including diverse and well-balanced data to prevent model biases and improve generalisation. A lack of diversity can cause the model to perform poorly on underrepresented scenarios [27].

 3. Data Size: Managing and processing large datasets, with at least 1000 samples recommended for effective fine-tuning. However, large datasets pose challenges in terms of storage, computational requirements, and processing time.

 4. Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies are critical for providing clean inputs to the model. Poorly preprocessed data can degrade model performance significantly.

 5. Data Annotation: Ensuring precise and consistent labelling is essential for tasks requiring labelled data. Inconsistent annotation can lead to unreliable model predictions.

 6. Handling Rare Cases: Adequately representing rare but important instances in the dataset to ensure the model can generalise to less frequent but critical scenarios.

 7. Ethical Considerations: Scrutinising data for harmful or biased content to prevent unintended consequences. Ethical data handling includes removing biases and ensuring privacy [28].

3.4 Available LLM Fine-Tuning Datasets

For a comprehensive list of datasets suitable for fine-tuning LLMs, refer to resources like LLMXplorer, which provides domain and task-specific datasets.

3.5 Best Practices

3.5.1 High-Quality Data Collection

Ensuring high-quality, diverse, and representative data is critical. Leveraging curated sources and ensuring comprehensive coverage across different scenarios enhances model robustness [29]. Tools like DataRobot Paxata 8 and KNIME Analytics Platform 9 offer robust data profiling and transformation capabilities.

3.5.2 Effective Data Preprocessing

Proper data preprocessing is essential for model performance. Utilising libraries like spaCy , NLTK , and HuggingFace Transformers can streamline preprocessing tasks. Platforms like Trifacta Wrangler and RapidMiner automate data cleaning tasks, improving efficiency and ensuring consistency [30].

3.5.3 Managing Data Imbalance

Addressing data imbalance is crucial. Techniques like over-sampling, under-sampling, and SMOTE help balance datasets. Libraries like imbalanced-learn and ensemble methods in scikit-learn provide robust tools for managing imbalanced datasets [31].

3.5.4 Augmenting and Annotating Data

Data augmentation and annotation improve model robustness. Tools like NLP-AUG , TextAttack , and Snorkel offer sophisticated capabilities for creating diverse and well-labelled datasets [32, 33].

3.5.5 Ethical Data Handling

Ensuring ethical data handling involves thorough scrutiny for biases and privacy concerns. Implementing privacy-preserving techniques and filtering harmful content is critical. Services like Amazon SageMaker Ground Truth ensure scalable and secure data annotation [34].

3.5.6 Regular Evaluation and Iteration

Continuous evaluation and iteration of the data preparation pipeline help maintain data quality and relevance. Leveraging feedback loops and performance metrics ensures ongoing improvements and adaptation to new data requirements.

By integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM fine-tuning, ensuring robust and reliable model performance.

Chapter 4

Stage 2: Model Initialisation

4.1 Steps Involved in Model Initialisation

<missing-text>

 1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if available, which can significantly speed up model loading and inference.

 2. Install the Dependencies: Ensure that all necessary software and libraries are installed. This typically includes package managers like pip and frameworks like PyTorch or TensorFlow.

 3. Import the Libraries: Import the required libraries in your script or notebook. Common libraries include transformers from Hugging Face, torch for PyTorch, and other utility libraries.

 4. Choose the Language Model: Select the appropriate pre-trained language model based on your task requirements. This could be models like BERT, GPT-3, or others available on platforms like Hugging Face's Model Hub.

 5. Download the Model from the Repository: Use the chosen framework's functions to download the pre-trained model from an online repository. For instance, using transformers, you might use AutoModel.from pretrained('model name').

 6. Load the Model in the Memory: Load the model into memory, ready for inference or further fine-tuning. This step ensures the model weights are initialised and ready for use.

 7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making predictions, generating text, or fine-tuning the model on a new dataset.

",chunk,,1
8eec8afd1f3d6bc0560cb4761e4d11dabcb2e69a6cb624e5b8f9fa26996a66951f5250048c0a2fde7507ea7f362a500c5c8413eee8ba29b15ec71bfaefb2bb57," the pre-trained model from an online repository. For instance, using transformers, you might use AutoModel.from pretrained('model name').

 6. Load the Model in the Memory: Load the model into memory, ready for inference or further fine-tuning. This step ensures the model weights are initialised and ready for use.

 7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making predictions, generating text, or fine-tuning the model on a new dataset.

4.2 Tools and Libraries for Model Initialisation

Python offers a wide range of libraries for Initialising large language models, providing access to both open and closed-source models. Here are some notable libraries:

1. Python Library: HuggingFace

Description: HuggingFace is renowned for its support of numerous pre-trained large language models, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace, enables users to access these models via classes such as AutoModelForCausalLM. This library supports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers library includes the 'pipeline' feature, making it easy to use pre-trained models for various tasks [35].

2. Python Framework: PyTorch

Description: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning large language models. It provides a flexible and efficient platform for building and deploying deep learning models. HuggingFace's transformers library bridges the gap between PyTorch and other frameworks, enhancing its usability for state-of-the-art language models [36].

3. Python Framework: TensorFlow

Description: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning large language models. Similar to PyTorch, it benefits from the HuggingFace transformers library, which provides a versatile and user-friendly API and interface for working with the latest advancements in large language models [37].

4.3 Challenges in Model Initialisation

<missing-text>

4.4 Tutorials

1. Summarisation using Llama 3

 2. HuggingFace tutorial for getting started with LLMs

 3. PyTorch tutorial for fine-tuning models

 4. TensorFlow tutorial for transformer models

Chapter 5

Stage 3: Training Setup

5.1 Steps Involved in Training Setup

 1. Setting up the training environment: When setting up the environment for training an LLM, it is crucial to configure high-performance hardware, such as GPUs or TPUs, and ensure proper installation of necessary software components like CUDA, cuDNN, and deep learning frameworks such as PyTorch or TensorFlow. Verify hardware recognition and compatibility with the software to leverage computational power effectively, reducing training time and improving model performance.

 2. Defining the Hyper-parameters: When defining hyperparameters for fine-tuning an LLM, it is essential to carefully tune key parameters such as learning rate, batch size, and epochs to optimise the model's performance.

 3. Initialising Optimisers and Loss Functions: When initialising optimisers and loss functions for fine-tuning an LLM, it is crucial to select the appropriate optimiser to efficiently update the model's weights and the correct loss function to measure model performance [43].

5.2 Setting up Training Environment

When fine-tuning a large language model (LLM), the computational environment plays a crucial role in ensuring efficient training. To achieve optimal performance, it's essential to configure the environment with high-performance hardware such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing Units). GPUs, such as the NVIDIA A100 or V100, are widely used for training deep learning models due to their parallel processing capabilities. For larger-scale operations, TPUs offered by Google Cloud can provide even greater acceleration [44].

First, ensure that your system or cloud environment has the necessary hardware installed. For GPUs, this involves setting up CUDA 1 (Compute Unified Device Architecture) and cuDNN 2 (CUDA Deep Neural Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage, you would typically set up a Google Cloud environment with TPU instances, which includes configuring the TPU runtime in your training scripts.

Verify that your hardware is correctly recognised and utilised by your deep learning frameworks. In PyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting up and testing the hardware ensures that the training process can leverage the computational power effectively, reducing training time and improving model performance [36].

When fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth and efficient training process. On the software side, you need a compatible deep learning framework like PyTorch or TensorFlow. These frameworks have extensive support for LLMs and provide utilities for efficient model training and evaluation. Installing the latest versions of these frameworks, along with any necessary dependencies, is crucial for leveraging the latest features and performance improvements

[45].

Additionally, use libraries like Hugging Face's transformers to simplify the process of loading pre-trained models and tokenizers. This library is particularly well-suited for working with various LLMs and offers a user-friendly interface for model fine-tuning. Ensure that all software components, including libraries and dependencies, are compatible with your chosen framework and hardware setup [35].

On the hardware side, consider the memory requirements of the model and your dataset. LLMs typically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more) can be beneficial. If your model is exceptionally large or if you are training with very large datasets, distributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of data parallelism or model parallelism techniques to efficiently utilise the available hardware [46].

Lastly, ensure robust cooling and power supply",chunk,,1
bc20f428f4d69043ff3e6fe70c50c1fc553ca38e3f8705de24d0a371070cc74dcef5773017823747621b050ff38a4c6a33047a7c8b30ff79519729ece250de5b," the memory requirements of the model and your dataset. LLMs typically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more) can be beneficial. If your model is exceptionally large or if you are training with very large datasets, distributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of data parallelism or model parallelism techniques to efficiently utilise the available hardware [46].

Lastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resourceintensive, generating significant heat and requiring consistent power. Proper hardware setup not only enhances training performance but also prolongs the lifespan of your equipment [47].

5.3 Defining Hyperparameters

Key hyperparameters like learning rate, batch size, epochs are crucial for enhancing the model's performance and obtaining superior outcomes. This process entails adjusting hyperparameters and training settings to align with your particular use case. Below are the key hyperparameters:

 1. Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like stochastic gradient descent (SGD). This technique estimates the error gradient for the model's current state using samples from the training dataset and subsequently updates the model's weights via the backpropagation of errors algorithm. The learning rate dictates the speed at which the model adapts to the problem. Smaller learning rates necessitate more training due to the minimal weight adjustments per update, while larger learning rates lead to quicker changes to weights [48].

 2. Batch Size: A batch refers to a subset of the training data used to update a model's weights during the training process. Batch training involves dividing the entire training set into smaller groups, updating the model after processing each batch. The batch size is a hyperparameter that determines the number of samples processed before the model parameters are updated.

 3. Epochs: Epoch refers to a full pass through the entire training dataset. This involves a complete forward and backward pass through the dataset. The dataset can be processed as a single batch or divided into multiple smaller batches. An epoch is considered complete once the model has processed all batches and updated its parameters based on the calculated loss.

5.3.1 Methods for Hyperparameter Tuning

LLM hyperparameter tuning involves adjusting various hyperparameters during the training process to identify the optimal combination that yields the best output. This process often entails significant trial and error, meticulously tracking each hyperparameter adjustment, and recording the resulting performance. Conducting this manually can be highly time-consuming. To address this, automated hyperparameter tuning methods have been developed to streamline the process. The three most common methods of automated hyperparameter tuning are random search, grid search, and Bayesian optimisation:

 1. Random Search: This method randomly selects and evaluates combinations of hyperparameters from a specified range. It is a straightforward and efficient approach capable of exploring a large parameter space. However, it may not always find the optimal combination of hyperparameters and can be computationally expensive [49].

 2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination of hyperparameters from a given range. Although resource-intensive, this systematic approach ensures that the optimal set of hyperparameters is found [50].

 3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of different hyperparameters and selects the best ones accordingly. It is an efficient method that can handle large parameter spaces better and is less resource-intensive than grid search. However, it is more complex to set up and may be less reliable in identifying the optimal set of hyperparameters compared to grid search.

 4. Automated hyperparameter tuning: This facilitates the development of multiple language models, each with a unique combination of hyperparameters. By training these models on the same dataset, it becomes possible to compare their outputs and determine which configuration is best suited for the desired use case. Additionally, models tuned with different sets of hyperparameters can be tailored to various specific applications.

5.4 Initialising Optimisers and Loss Functions

Choosing the right optimiser and loss function is crucial for training and fine-tuning LLMs. Below are descriptions of some commonly used optimisation algorithms, their advantages, disadvantages, and appropriate use cases:

5.4.1 Gradient Descent

Gradient Descent is a fundamental optimisation algorithm used to minimise cost functions in machine learning models. It aims to find the optimal parameters for a neural network.

How it Works: Gradient Descent iteratively updates model parameters in the direction of the negative gradient of the cost function. It calculates gradients for each parameter and applies updates across all data points until convergence. This method utilises the entire dataset to calculate gradients, often requiring a fixed learning rate and being sensitive to the scale of data and learning rate choice.

Pros:

 · Simple and easy to implement.

 · Intuitive and easy to understand.

 · Converges to the global minimum for convex functions.

 · Suitable for small-scale problems.

Cons:

 · Computationally expensive on large datasets.

 · May get stuck in local minima.

 · Requires a large number of iterations.

 · Sensitive to the choice of learning rate.

When to Use: Gradient Descent is best used for small datasets where gradient computation is cheap and simplicity and clarity are preferred.

5.4.2 Stochastic Gradient Descent (SGD)

Stochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation per iteration.

How it Works: SGD updates parameters using a single or few data points at each iteration, introducing randomness in updates. It reduces the computational burden per iteration and often converges faster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance and benefits from momentum to stabilise updates.

Pros:

 · Fast and handles large datasets well.

 · Efficient memory usage.

 · Simple and easy to implement.

 · Can escape local minima due to noise.

Cons:

 · High",chunk,,1
396d4493bc03b03a3c67f2f21923d78e12ebd9ea18e512dec2862248f316c047043e3aac4a42617b792355e6717e8c80976c13e165f5b469293fbfb199789a26," iteration.

How it Works: SGD updates parameters using a single or few data points at each iteration, introducing randomness in updates. It reduces the computational burden per iteration and often converges faster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance and benefits from momentum to stabilise updates.

Pros:

 · Fast and handles large datasets well.

 · Efficient memory usage.

 · Simple and easy to implement.

 · Can escape local minima due to noise.

Cons:

 · High variance in updates can lead to instability.

 · Can overshoot the minimum.

 · Sensitive to the choice of learning rate.

 · Can be slower to converge compared to batch methods.

When to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning environments where computational resources are limited.

5.4.3 Mini-batch Gradient Descent

Mini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent, offering a compromise between batch and stochastic approaches.

How it Works: It splits data into small batches and updates parameters using gradients averaged over each mini-batch. This reduces variance compared to SGD and is more efficient than batch Gradient Descent, helping in generalising the updates.

Pros:

 · Balances between efficiency and stability.

 · More generalisable updates.

 · Reduces the variance of parameter updates.

 · Provides a compromise between SGD and batch.

Cons:

 · Requires tuning of batch size.

 · Can still be computationally expensive for very large datasets.

 · More complex implementation.

 · Can require more iterations than full-batch Gradient Descent.

When to Use: Mini-batch Gradient Descent is suitable for most deep learning tasks, especially when working with moderate to large datasets.

5.4.4 AdaGrad

Adaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, adjusting learning rates to improve performance on sparse data.

How it Works: AdaGrad adapts the learning rate for each parameter based on historical gradient information, accumulating squared gradients. This approach prevents large updates for frequent parameters and helps in dealing with sparse features.

Pros:

 · Adapts learning rate for each parameter.

 · Good for sparse data.

 · No need to manually tune learning rates.

 · Works well with high-dimensional data.

Cons:

 · Learning rate can diminish to zero, stopping learning.

 · May require more tuning for convergence.

 · Accumulation of squared gradients can lead to overly small learning rates.

 · Can slow down significantly.

When to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need to adapt to feature frequency.

5.4.5 RMSprop

Root Mean Square Propagation (RMSprop) is an adaptive learning rate method designed to perform better on non-stationary and online problems.

How it Works: RMSprop modifies AdaGrad by using a moving average of squared gradients to adapt learning rates based on recent gradient magnitudes. It maintains a running average of squared gradients to help in maintaining steady learning rates.

Pros:

 · Addresses the diminishing learning rate problem of AdaGrad.

 · Adapts learning rate based on recent gradients.

 · Effective for recurrent neural networks.

 · More robust against non-stationary targets.

Cons:

 · Can still get stuck in local minima on non-convex problems.

 · Requires hyperparameter tuning.

 · Requires careful tuning of the decay rate.

 · Can be sensitive to the initial learning rate.

When to Use: RMSprop is best for non-convex optimisation problems, training RNNs and LSTMs, and dealing with noisy or non-stationary objectives.

5.4.6 AdaDelta

Adaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive learning rates without diminishing too quickly.

How it Works: AdaDelta eliminates the need for a default learning rate by using a moving window of gradient updates. It adapts learning rates based on recent gradient magnitudes to ensure consistent updates even with sparse gradients.

Pros:

 · Eliminates the need to set a default learning rate.

 · Addresses the diminishing learning rate issue.

 · Does not require manual tuning of the learning rate.

 · Handles gradient sparsity well.

Cons:

 · More complex than RMSprop and AdaGrad.

 · Can have slower convergence initially.

 · Can require more iterations to converge.

 · Implementation can be more complex.

When to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred when avoiding manual learning rate setting.

5.4.7 Adam

Adaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop, making it suitable for problems with large datasets and high-dimensional spaces.

How it Works: Adam uses running averages of both gradients and their squared values to compute adaptive learning rates for each parameter. It includes bias correction and often achieves faster convergence than other methods.

Pros:

 · Combines advantages of AdaGrad and RMSprop.

 · Adaptive learning rates.

 · Includes bias correction.

 · Fast convergence.

 · Works well with large datasets and high-dimensional spaces.

Cons:

 · Requires tuning of hyperparameters (though it often works well with defaults).

 · Computationally intensive.

 · Can lead to overfitting if not regularised properly.

 · Requires more memory.

When to Use: Adam is widely used in most deep learning applications due to its efficiency and effectiveness, particularly in complex neural network architectures.

5.4.8 AdamW

AdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues present in Adam.

How it Works: AdamWintegrates L2 regularisation directly into the parameter updates, decoupling weight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large models.

Pros:

 · Includes weight decay for better regularisation.

 · Combines Adam's adaptive learning rate with L2 regularisation.

 · Improves generalisation.

 · Reduces overfitting compared to Adam.

Cons:

 ·",chunk,,1
bbe8cc0a3582f78a592dbe4ca6182ef0f45238da414aace836b7a45bbdb69d7217f45bfb8fc59200e3e25c262cfba323e4aff9f4c15e4e7cd0fbcbe4ba8cfc26," weight decay regularisation to address overfitting issues present in Adam.

How it Works: AdamWintegrates L2 regularisation directly into the parameter updates, decoupling weight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large models.

Pros:

 · Includes weight decay for better regularisation.

 · Combines Adam's adaptive learning rate with L2 regularisation.

 · Improves generalisation.

 · Reduces overfitting compared to Adam.

Cons:

 · Slightly more complex than Adam.

 · Requires careful tuning of the weight decay parameter.

 · Slightly slower than Adam due to additional computations.

 · Requires more memory.

When to Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing overfitting in large models and fine-tuning pre-trained models.

Acomprehensive collection of optimisation algorithms implemented within the PyTorch library can be found in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising and fine-tuning language models, available here.

5.5 Challenges in Training Setup

 1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs can be complex and time-consuming.

 2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts and leverage the latest features.

 3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal convergence, while too low a rate can make the training process excessively slow.

 4. Determining the optimal batch size that balances memory constraints and training efficiency, especially given the large memory requirements of LLMs.

 5. Choosing the right number of epochs to avoid underfitting or overfitting the model, requiring careful monitoring and validation.

 6. Selecting the most suitable optimiser for the specific training task to efficiently update the model's weights.

 7. Choosing the correct loss function to accurately measure model performance and guide the optimisation process.

5.6 Best Practices

 · Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to 2e-4, to ensure stable convergence. A learning rate schedule, such as learning rate warm-up followed by a linear decay, can also be beneficial. This helps in initially stabilising the training and then allowing the model to converge more accurately.

 · Batch Size Considerations: Opt for a batch size that balances memory constraints and training efficiency. Smaller batch sizes can help in achieving faster convergence but may require more frequent updates. Conversely, larger batch sizes can be more memory-intensive but may lead to more stable updates. Experiment with different batch sizes to find the optimal balance for your specific use case.

 · Save Checkpoints Regularly: Regularly save model weights at various intervals across 5-8 epochs to capture optimal performance without overfitting. Implement early stopping mechanisms to halt training once the model performance starts to degrade on the validation set, thereby preventing overfitting [51].

 · Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search, random search, and Bayesian optimisation to find the optimal set of hyperparameters. Tools such as Optuna, Hyperopt, and Ray Tune can automate this process and help in efficiently exploring the hyperparameter space [49].

 · Data Parallelism and Model Parallelism: For large-scale training, consider using data parallelism or model parallelism techniques to distribute the training workload across multiple GPUs or TPUs. Libraries like Horovod and DeepSpeed can facilitate efficient distributed training, helping to reduce training time and manage memory usage effectively [52, 53].

 · Regular Monitoring and Logging: Implement robust monitoring and logging to track training metrics, resource usage, and potential bottlenecks. Tools like TensorBoard, Weights & Biases, and MLflow can provide real-time insights into the training process, allowing for timely interventions and adjustments.

 · Handling Overfitting and Underfitting: Ensure that your model generalises well by implementing techniques to handle overfitting and underfitting. regularisation techniques such as L2 regularisation, dropout, and data augmentation can help prevent overfitting. Conversely, if your model is underfitting, consider increasing the model complexity or training for more epochs.

 · Use Mixed Precision Training: Mixed precision training involves using both 16-bit and 32-bit floating-point types to reduce memory usage and increase computational efficiency. This technique can significantly speed up training and reduce the required memory footprint, especially when using large models. NVIDIA's Apex and TensorFlow's mixed precision API provide support for implementing mixed precision training [54].

 · Evaluate and Iterate: Continuously evaluate the model performance using a separate validation set and iterate on the training process based on the results. Regularly update your training data and retrain the model to keep it current with new data trends and patterns.

 · Documentation and Reproducibility: Maintain thorough documentation of your training setup, including the hardware configuration, software environment, and hyperparameters used. Ensure reproducibility by setting random seeds and providing detailed records of the training process. This practice not only aids in debugging and further development but also facilitates collaboration and sharing of results with the broader research community.

Chapter 6

Stage 4: Selection of Fine-Tuning Techniques and Appropriate Model Configurations

This chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit the specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are adapted to specific tasks or domains.

6.1 Steps Involved in Fine-Tuning

The following steps outline the fine-tuning process, integrating advanced techniques and best practices.

 1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer and model. The tokenizer ensures that the input text is converted into a format the model can process, while the pre-trained model serves as the foundation for further adaptation. Depending on the task, select",chunk,,1
5ef937c334d721f4acad1364d6210a9d598913b4d4faf2f2d41cc2b4c01cfd27cba25d739cbf6a035fad7d3a8a23a43137ad8dbdf1d72fdf339cf318565a34ce," stage where pre-trained models are adapted to specific tasks or domains.

6.1 Steps Involved in Fine-Tuning

The following steps outline the fine-tuning process, integrating advanced techniques and best practices.

 1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer and model. The tokenizer ensures that the input text is converted into a format the model can process, while the pre-trained model serves as the foundation for further adaptation. Depending on the task, select a model that has been pre-trained on relevant data to provide a strong starting point.

 2. Modify the Model's Output Layer: Adjust the model's output layer to align with the specific requirements of the target task. This may involve modifying existing layers or adding new layers. For instance, tasks like classification may require a softmax layer with the appropriate number of classes, while text generation tasks might involve changes in the decoding mechanism.

 3. Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy that best fits the task and the model architecture. Some Options include:

 · Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classification, and question answering, adapt the model using relevant datasets.

 · Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant to specific domains, such as medical, financial, or legal fields.

 · Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters allow for fine-tuning with reduced computational costs by updating a small subset of model parameters.

 · Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning new tasks by updating only half of the model's parameters during each fine-tuning round.

 4. Set Up the Training Loop: Establish the training loop, incorporating the selected fine-tuning strategy. The loop should include data loading, loss computation, backpropagation, and parameter updates. When using PEFT methods, ensure that only the relevant parameters are updated to maximise efficiency. Implement techniques like dynamic learning rates and early stopping to enhance the training process.

 5. Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple tasks, consider strategies like fine-tuning with multiple adapters or leveraging Mixture of Experts (MoE) architectures. These methods allow a single model to handle various tasks by utilising specialised sub-networks or adapters for each task.

 6. Monitor Performance on a Validation Set: Regularly evaluate the model's performance on a validation set to ensure it generalises well to unseen data. Adjust hyperparameters such as learning rate, batch size, and dropout rates based on the validation performance. Utilise advanced monitoring tools to track metrics like accuracy, loss, and overfitting.

 7. Optimise Model Using Advanced Techniques: Employ techniques such as Proximal Policy Optimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO) for aligning model outputs with human preferences. These techniques are particularly useful in fine-tuning models for tasks requiring nuanced decision-making or human-like responses.

 8. Prune and optimise the Model (if necessary): To deploy the model in resource-constrained environments, consider pruning techniques to reduce its size and complexity. This involves removing unnecessary parameters or components without significantly affecting performance. Utilise dynamic pruning methods during inference to optimise the model on-the-fly for different scenarios.

 9. Continuous Evaluation and Iteration: Continuously evaluate the model's performance across various tasks using appropriate benchmarks. Iterate on the fine-tuning process, making adjustments based on performance metrics and real-world testing. This iterative approach helps in refining the model to meet specific performance criteria.

6.2 Fine-Tuning Strategies for LLMs

6.2.1 Task-Specific Fine-Tuning

Task-specific fine-tuning adapts large language models (LLMs) for particular downstream tasks using appropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning LLMs, including examples of LLMs tailored to these tasks.

<missing-text>

6.2.2 Domain-Specific Fine-Tuning

Domain-specific fine-tuning focuses on tailoring the model to comprehend and produce text relevant to a specific domain or industry. By fine-tuning the model on a dataset derived from the target domain, it enhances the model's contextual understanding and expertise in domain-specific tasks. Below are examples of domain-specific LLMs.

Medical Domain

Model Description: Med-PaLM 2 is trained on meticulously curated medical datasets and is capable of accurately answering medical questions, achieving performance comparable to that of medical professionals [55].

Base Model: PaLM 2 Fine-tuned Model Parameters: Not Known

Fine-Tuning Techniques Used: Instruction fine-tuning

Datasets Used:

 · MedQA

 · MedMCQA

 · LiveQA

 · MedicationQA

 · HealthSearchQA

Results: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating superior performance in handling complex medical knowledge and reasoning tasks.

Finance Domain

Model Description: FinGPT, an open-source LLM tailored for the financial sector, enhances financial research and cooperation by promoting data accessibility and handling finance-specific issues like data acquisition and quality [56].

Base Model: LlaMA, ChatGLM, and other Transformer Models

Fine-tuned Model Parameters: Not Known

Fine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)

Datasets Used:

 · Financial News (Reuters, CNBC, Yahoo Finance)

 · Social Media (Twitter, Facebook, Reddit, Weibo)

 · Regulatory Filings (e.g., SEC filings)

 · Trends (Seeking Alpha, Google Trends)

 · Academic Datasets

Results: Not Applicable

Legal Domain

Model Description: LAWGPT, the first open-source model specifically designed for Chinese legal",chunk,,1
d8ff988e51c6320da4edd940a04a155228a3a29f5b45af8fb8e6438aee0d75d16714f4a4d70b4633bac54f387339c0b431d041bf6ae98235b3200a85a24ccbef,"

Fine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)

Datasets Used:

 · Financial News (Reuters, CNBC, Yahoo Finance)

 · Social Media (Twitter, Facebook, Reddit, Weibo)

 · Regulatory Filings (e.g., SEC filings)

 · Trends (Seeking Alpha, Google Trends)

 · Academic Datasets

Results: Not Applicable

Legal Domain

Model Description: LAWGPT, the first open-source model specifically designed for Chinese legal applications, demonstrates superior capability in handling Chinese legal tasks [57].

Base Model: Chinese Alpaca Plus 7B base model

Fine-tuned Model Parameters: Not Known

Fine-Tuning Techniques Used: LoRA with Alpaca template

Datasets Used:

 · Open-source dataset: 200,000 examples containing crime type prediction and crime consultation tasks.

 · JEC-QA dataset: 20,000 examples containing legal question answering tasks.

 · Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA datasets using ChatGPT.

Results: LAWGPT demonstrates notable performance improvements over the LLaMA 7B model in various legal tasks, but still trails behind proprietary models like GPT-3.5 Turbo and GPT-4.

Pharmaceutical Domain

Model Description: PharmaGPT, a suite of domain-specific large language models tailored to the biopharmaceutical and chemical industries, sets a new benchmark for precision in these fields [58].

Base Model: LlaMA series

Fine-tuned Model Parameters: 13B and 70B

Fine-Tuning Techniques Used:

Instruction fine-tuning and RLHF

Datasets Used:

 · Specific-domain data from academic papers and clinical reports

 · Text data from NLP dataset formats (e.g., question answering, summarisation, dialogue)

 · Instruction fine-tuning dataset for multitask learning

 · RLHF dataset with human preference expert-annotated instructions

Results: PharmaGPT models demonstrated impressive performance on various pharmaceutical benchmarks, consistently outperforming GPT-3.5 Turbo.

Finance Domain

Model Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model specifically designed for the financial sector. [59]

Base Model: LlaMA

Fine-tuned Model Parameters: 70B

Fine-Tuning Techniques Used: Not Known

Datasets Used: Not Known

Results: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across various financial datasets and excelling in financial document analysis, market trend prediction, and risk assessment.

6.3 Parameter-Efficient Fine-Tuning (PEFT) Techniques

Parameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained language models to various applications with remarkable efficiency. PEFT methods fine-tune only a small subset of (additional) model parameters while keeping most of the pre-trained LLM parameters frozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue of catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and experience a significant performance decline on previously learned tasks when trained on new datasets. PEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in low-data scenarios, and exhibit better generalisation to out-of-domain contexts. This technique is applicable to various modalities, such as financial sentiment classification and machine translation of medical terminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will further discuss a few key PEFT-based approaches in the following sections.

6.3.1 Adapters

Adapter-based methods introduce additional trainable parameters after the attention and fully connected layers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. The specific approach varies depending on the adapter; it might involve adding an extra layer or representing the weight updates delta (W) as a low-rank decomposition of the weight matrix. Regardless of the method, adapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for the training of larger models with fewer resources.

HuggingFace supports adapter configurations through the PEFT library. During fine-tuning, new adapters are integrated into the model using LoraConfig 1 . HuggingFace uses PeftConfig to load existing pretrained models and apply PEFT techniques. Additionally, HuggingFace provides built-in support to

<missing-text>

run the fine-tuning process across any distributed configuration using Accelerate 2 , making large-scale training and inference simple, efficient, and adaptable.

6.3.2 Low-Rank Adaptation (LoRA)

Low-Rank Adaptation (LoRA)[62] is a technique designed for fine-tuning large language models, which modifies the fine-tuning process by freezing the original model weights and applying changes to a separate set of weights, added to the original parameters. LoRA transforms the model parameters into a lowerrank dimension, reducing the number of trainable parameters, speeding up the process, and lowering costs. This method is particularly useful in scenarios where multiple clients require fine-tuned models for different applications, allowing for the creation of specific weights for each use case without the need for separate models. By employing low-rank approximation methods, LoRA effectively reduces computational and resource requirements while preserving the pre-trained model's adaptability to specific tasks or domains.

Benefits of Using LoRA

 1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be trained by focusing only on the low-rank matrices, resulting in lower memory and storage requirements compared to full fine-tuning.

 2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing the low-rank matrices instead of the full model weights.

<missing-text>

 3. Reduced Computational Load: Training with low",chunk,,1
3e3be07612fb26940ca29eb89696afb65f35ebfc07e845799df3c85d7a98ba3157a0b9ff463a3d0132a00dede32e035076dd4f9269194e031681037b7d57cbaa," specific tasks or domains.

Benefits of Using LoRA

 1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be trained by focusing only on the low-rank matrices, resulting in lower memory and storage requirements compared to full fine-tuning.

 2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing the low-rank matrices instead of the full model weights.

<missing-text>

 3. Reduced Computational Load: Training with low-rank matrices requires fewer computational resources, making it faster and more scalable.

 4. Lower Memory Footprint: Since fewer parameters are being updated, the memory footprint during training is reduced, enabling the use of larger batch sizes or more complex models within the same hardware constraints.

 5. Flexibility: LoRA can be easily integrated with existing pre-trained models without extensive modifications to the model architecture.

 6. Compatibility: It can be used alongside other fine-tuning techniques, such as adapter layers or prompt-tuning, to further enhance performance.

 7. Comparable Results: Despite the reduction in the number of trainable parameters, LoRA has been shown to achieve performance comparable to full fine-tuning in many tasks.

 8. Task-Specific Adaptation: It effectively adapts the pre-trained model to specific tasks, leveraging the knowledge already embedded in the original model.

 9. Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating overfitting, especially when dealing with smaller task-specific datasets.

Limitations

While LoRA demonstrates considerable power, it also presents challenges:

 · Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding substantial alterations to the pre-trained model's internal representations.

 · Hyperparameter Optimisation: Tuning the rank parameter 'r' requires meticulous adjustment for optimal performance.

 · Ongoing Research: Despite its promise, LoRA is still in active research stages, and its long-term implications remain to be fully explored.

Weight update in regular finetuning

Weight update in LoRA

<missing-text>

Despite these challenges, LoRA stands as a pioneering technique with vast potential to democratise access to the capabilities of LLMs. Continued research and development offer the prospect of overcoming current limitations and unlocking even greater efficiency and adaptability.

Tutorial for Fine-Tuning LLM Using LoRA

An open-source template for fine-tuning LLMs using the LoRA method with the Hugging Face library can be found here. This template is designed specifically for adapting LLMs for instruction fine-tuning processes.

6.3.3 QLoRA

QLoRA[64] is an extended version of LoRA designed for greater memory efficiency in large language models (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored in a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint. This allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the weights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements (see Figure 6.4). Despite the reduction in bit precision, QLoRA maintains performance levels comparable to traditional 16-bit fine-tuning.

It achieves this by backpropagating gradients through a frozen, 4-bit quantised pre-trained language model into Low-Rank Adapters, making the fine-tuning process efficient while preserving model effectiveness. The QLoRA configuration is supported by HuggingFace via the PEFT library, utilising LoraConfig and BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quantisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96

bits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.

Performance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models on benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a single GPU in 24 hours, achieving quality comparable to ChatGPT.

This tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2 model.

<missing-text>

6.3.4 Weight-Decomposed Low-Rank Adaptation (DoRA)

In the context of optimising model fine-tuning, the pattern analysis of LoRA and Full Fine-Tuning (FT) reveals significant differences in learning behaviours and updates. LoRA, employing a strategy of incrementally updating pre-trained weights using the product of two low-rank matrices, maintains the original weights largely static during the fine-tuning process, which allows for efficient inference. Despite its computational efficiency, previous studies have suggested that LoRA's limited number of trainable parameters might contribute to its performance discrepancies when compared to FT.

Weight-Decomposed Low-Rank Adaptation (DoRA) [66] is a novel fine-tuning methodology designed to optimise pre-trained models by decomposing their weights into magnitude and directional components. This approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facilitating substantial parameter updates without altering the entire model architecture. DoRA addresses the computational challenges associated with traditional full fine-tuning (FT) by maintaining model simplicity and inference efficiency, while simultaneously bridging the performance gap typically observed between LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves learning outcomes comparable to FT across diverse tasks-including natural language processing and vision-language applications-but also consistently surpasses LoRA in performance, providing a robust solution for enhancing the adaptability and efficiency of large-scale models.

Python Library DoRA is facilitated via the HuggingFace LoraConfig package.",chunk,,1
d143ab5c6f2a538e54720bdc2ae80a015cce089a63d4c7f12619ae642eda3c739d2fc658a938614c381f84a363055da78b699bbb31aa55668aea66b21e4805bc," fine-tuning (FT) by maintaining model simplicity and inference efficiency, while simultaneously bridging the performance gap typically observed between LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves learning outcomes comparable to FT across diverse tasks-including natural language processing and vision-language applications-but also consistently surpasses LoRA in performance, providing a robust solution for enhancing the adaptability and efficiency of large-scale models.

Python Library DoRA is facilitated via the HuggingFace LoraConfig package. To incorporate DoRA into the fine-tuning process, it is essential to specify the 'use dora = True' parameter during the Lora configuration. Further information on initialisation can be found here.

Benefits of DoRA

 1. Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling full finetuning (FT) by decomposing pre-trained weights into magnitude and directional components, allowing for more nuanced updates.

 2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA) for directional updates, DoRA enables efficient fine-tuning without altering the entire model architecture.

 3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not introduce any additional inference latency over LoRA, maintaining model simplicity and efficiency.

 4. Superior Performance: Experimental results demonstrate that DoRA consistently outperforms LoRA across a wide range of tasks, including natural language processing (NLP), visual instruction tuning, and image/video-text understanding. For example, it shows significant improvements in commonsense reasoning and visual instruction tuning benchmarks.

 5. Versatility Across Backbones: DoRA has been validated across various model backbones, including large language models (LLM) and vision-language models (LVLM), indicating its broad

<missing-text>

Direction

applicability and robustness in different domains.

 6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover fundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning (PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.

Comparison between LoRA and DoRA

Low-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both advanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained models. While they share the common goal of reducing computational overhead, they employ different strategies to achieve this (see Table6.2).

<missing-text>

Tutorial for Fine-Tuning LLM using DoRA

This tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing DoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.

6.3.5 Fine-Tuning with Multiple Adapters

During fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing solely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM for translation involves training a translation adapter with relevant data. This approach allows us to fine-tune separate adapters for each specific task we want the LLM to perform. However, a key question arises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have separate adapters for translation and summarisation tasks, can we merge them so that the LLM can proficiently handle both tasks? (Illustrated via Figure6.6).

The PEFT library simplifies the process of merging adapters with its add weighted adapter function 3 , which offers three distinct methods:

 1. Concatenation: This straightforward method concatenates the parameters of the adapters. For instance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This method is highly efficient.

 2. Linear Combination: Although less documented, this method appears to perform a weighted sum of the adapters' parameters.

 3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While versatile, it is notably slower than the other methods, particularly for adapters with high ranks (greater than 100), which can take several hours.

Each method allows for customising the combination by adjusting weights. For instance, when merging two adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour similar to X over Y.

This approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than creating separate models for each task domain. By adopting this method, there is no longer a need to

individually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each task, allowing queries to yield the desired responses efficiently.

<missing-text>

Steps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters

 1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different prompt formats or task-identifying tags (e.g., [translate fren], [chat]).

 2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained LLM. Utilise LoRA's methods such as concatenation, linear combination, or singular value decomposition (SVD) to combine adapters while minimising computational overhead and maintaining performance.

 3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance performance for individual tasks. Ensure adapters are trained with data relevant to their respective tasks, optimising their ability to generate accurate responses.

 4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired inherited behaviours from individual adapters (e.g., short response generation from a translation

adapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring each adapter performs optimally for its intended task.

 5. Evaluation and Iteration: Evaluate",chunk,,1
5c3cbb3474e411584e3e7916bb35b15405b518f87fe9190a609877dbf844e3ed0e501496b26e152d89303e0e95a2149c3cdfc1196013ba7c7f7783116fa240d4," with task-specific data to enhance performance for individual tasks. Ensure adapters are trained with data relevant to their respective tasks, optimising their ability to generate accurate responses.

 4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired inherited behaviours from individual adapters (e.g., short response generation from a translation

adapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring each adapter performs optimally for its intended task.

 5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple tasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter combinations and training parameters based on performance metrics and user feedback.

Therefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with distinctly varied prompt formats. However, even when using adapters with different prompt formats, the resulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for chatting may only generate short responses, inheriting this tendency from an adapter that was originally trained to halt after producing a single sentence. To adjust the behaviour of the combined adapter, one can prioritise the influence of a specific adapter during the combination process and/or modify the method of combination used.

An illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple adapter layers for various tasks can be found here.

6.4 Half Fine Tuning

Half Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge with the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the model's parameters during each fine-tuning round while updating the other half, allowing the model to retain pre-trained knowledge and enhance new task performance without altering the model architecture. Each repetitive transformer layer is divided into three blocks: self-attention, feed-forward, and layernorm, with half of the parameters in each block updated and the other half frozen, varying with each round. This strategic parameter update helps maintain knowledge parity across training rounds and enhances scalability in successive training sessions.

Research on models like LLAMA 2-7B demonstrated that HFT could significantly restore forgotten basic knowledge while preserving high general ability performance. This method's robustness and efficiency make it applicable to various fine-tuning scenarios, including supervised fine-tuning, direct preference optimisation, and continual learning. Additionally, HFT's ability to maintain the model architecture simplifies its implementation and ensures compatibility with existing systems, further promoting its practical adoption.

6.4.1 Benefits of using Half Fine tuning

 1. Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters to their pre-trained state, HFT effectively recovers a portion of the original knowledge, thereby mitigating catastrophic forgetting of previously acquired capabilities.

 2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses the performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in balancing knowledge retention with task-specific learning.

 3. Robustness: The method is robust to different selection strategies and the number of parameters chosen for updating, ensuring consistent performance across various configurations.

 4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies implementation and allows for scalable applications, particularly beneficial in successive fine-tuning scenarios.

 5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including supervised fine-tuning, direct preference optimisation, and continual learning.

<missing-text>

6.4.2 Comparison between HFT and LoRA

<missing-text>

6.5 Lamini Memory Tuning

Lamini [69] was introduced as a specialised approach to fine-tuning Large Language Models (LLMs), targeting the reduction of hallucinations. This development was motivated by the need to enhance the reliability and precision of LLMs in domains requiring accurate information retrieval. Traditional training methods typically consist of running stochastic gradient descent on vast datasets, which, despite fitting the training data well, often produce models that fail to generalise effectively and are prone to such errors.

Foundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes training for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion tokens. This approach results in substantial loss and is geared more towards enhancing generalisation and creativity where a degree of randomness in token selection is permissible. However, it falls short for tasks demanding high factual precision. In contrast, Lamini Memory Tuning delves deeper by analysing the loss of individual facts, significantly improving the accuracy of factual recall. By augmenting a model with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B parameters for weights), Lamini enables the model to memorise and accurately recall a significant number of facts, closely aligning performance with LLM scaling laws without compromising on generalisation.

6.5.1 Lamini-1 - A model architecture based on Lamini

Departing from traditional transformer-based designs, the Lamini-1 model architecture (Figure 6.8) employs a massive mixture of memory experts (MoME). This system features a pre-trained transformer backbone augmented by adapters that are dynamically selected from an index using cross-attention mechanisms. These adapters function similarly to experts in MoE architectures, and the network is trained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly in the selected experts.

<missing-text>

At inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a large number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton are used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.

Systems Optimisations for Banishing Hallucinations

The",chunk,,1
55c9216082a95c84b1cf56f5f82991f929240e16dda4a91abc17acd88769d6cbfaf0e4d775b9e46df3fc923ac84a7a568b4e3246fe29accc340d93c5e17f27f2," network is trained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly in the selected experts.

<missing-text>

At inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a large number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton are used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.

Systems Optimisations for Banishing Hallucinations

The MoME architecture is designed to minimise the computational demand required to memorise facts. During training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of the backbone network and the cross attention used to select the expert are frozen, and gradient descent steps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the same expert from being selected multiple times for different facts by first training the cross attention

selection mechanism during a generalisation training phase, then freezing its weights.

This method ensures that computation scales with the number of training examples, not the total number of parameters, thereby significantly reducing the computation required for memory tuning. This optimised approach allows Lamini-1 to achieve near-zero loss in memory tuning on real and random answers efficiently, demonstrating its efficacy in eliminating hallucinations while improving factual recall.

6.6 Mixture of Experts

A mixture of experts (MoE) is an architectural design for neural networks that divides the computation of a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnetworks, referred to as 'experts'. Each expert independently carries out its computation, and the results are aggregated to produce the final output of the MoE layer. MoE architectures can be categorised as either dense, where every expert is engaged for each input, or sparse, where only a subset of experts is utilised for each input.

6.6.1 Mixtral 8x7B Architecture and Performance

Mixtral [70] 8x7B employs a Sparse Mixture of Experts (SMoE) architecture (Figure 6.9), mirroring the structure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer. For every token at each layer, a router network selects two experts to process the current state and combine their outputs. Although each token interacts with only two experts at a time, the selected experts can vary at each timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion active parameters during inference. Mixtral 8x7B not only matches but often surpasses Llama 2 70B and GPT-3.5 across all evaluated benchmarks. Its performance is notably superior to Llama 2 70B in mathematics, code generation, and multilingual tasks.

<missing-text>

6.7 Mixture of Agents

Despite the numerous LLMs and their notable accomplishments, they continue to encounter fundamental limitations regarding model size and training data. Scaling these models further is prohibitively expensive, often necessitating extensive retraining on multiple trillion tokens. Simultaneously, different LLMs exhibit distinct strengths and specialise in various aspects of tasks. A recent study has investigated leveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a method known as Mixture of Agents (MoA) [72].

MoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure 6.10). This structure reveals a phenomenon known as the 'collaborativeness of LLMs.' The innovative MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and language generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating improved response quality when incorporating outputs from other models, even if those outputs are not ideal.

<missing-text>

6.7.1 Methodology

To enhance collaboration among multiple LLMs, it is essential to understand their individual strengths and classify them accordingly. The classification includes:

 1. Proposers: These models excel at generating valuable reference responses for other models. While they may not perform exceptionally on their own, they provide useful context and varied perspectives that improve the final output when utilised by an aggregator.

 2. Aggregators: These models are adept at merging responses from various models into a single high-quality result. An effective aggregator should maintain or even enhance the quality of the final response, regardless of the quality of the individual inputs.

The careful selection of LLMs for each MoA layer is crucial Performance metrics, such as average win rates in a given layer, help assess the suitability of models for subsequent layers, ensuring the production of higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models contribute significantly more than homogeneous outputs from a single model. In MoA, given an input prompt, the output of the i th MoA layer y i is calculated as follows:

y i = n ⊕ j =1 [ A i,j ( x i )] + x 1 , x i +1 = y i (6.1)

6.7.2 Analogy with MoE

Mixture-of-Experts (MoE) is a well-established machine learning technique where multiple expert networks, each with specialised skills, collaborate to address complex problems. This approach has demonstrated significant success across various applications and serves as the inspiration for the Mixture-ofAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of multiple expert networks, a gating network, and residual connections to improve gradient flow. The output for layer y i is calculated as follows:

y i = n ∑ j =1 G i,j ( x i ) E i,j ( x",chunk,,1
01b2f162b7fc41e1b57dad94b752b0c3d2ca5a92d1b1cffeebbfa8456706781606189bed106dfd66542b364d3ae966463ead086fa23533b49d975fbcad7f77d9," collaborate to address complex problems. This approach has demonstrated significant success across various applications and serves as the inspiration for the Mixture-ofAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of multiple expert networks, a gating network, and residual connections to improve gradient flow. The output for layer y i is calculated as follows:

y i = n ∑ j =1 G i,j ( x i ) E i,j ( x i ) + x i (6.2)

The MoA framework advances the MoE concept by operating at the model level through prompt-based interactions rather than altering internal activations or weights. Instead of relying on specialised subnetworks within a single model, MoA utilises multiple full-fledged LLMs across different layers. In this approach, the gating and expert networks' functions are integrated within an LLM, leveraging its ability to interpret prompts and generate coherent outputs without additional coordination mechanisms.

6.7.3 What makes MoA works well?

 1. MoA's Superior Performance: MoA significantly outperforms LLM-based rankers, which select one answer from the proposals rather than generating new responses. This suggests that MoA's approach of aggregating all generated responses provides more effective results than simply choosing from pre-existing options.

 2. Effective Incorporation of Proposals: The aggregator in MoA demonstrates a tendency to integrate the best proposed answers. This is supported by positive correlations between aggregator responses and various similarity metrics, such as BLEU scores, which measure n-gram overlaps. The use of alternative similarity measures also shows a consistent positive correlation with preference scores, indicating that the aggregator effectively utilises the proposed responses.

 3. Influence of Model Diversity and Proposer Count: Increasing the number of proposers improves output quality, highlighting the benefits of additional auxiliary information. Additionally, using a diverse set of LLMs as proposers consistently yields better results compared to using a single LLM. This suggests that both the number and diversity of LLM agents in each MoA layer contribute to enhanced performance, with potential for further improvement through scaling.

 4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o, Qwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM excels as a proposer but struggles with aggregating responses from other models.

6.8 Proximal Policy Optimisation (PPO)

PPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks in diverse environments. This algorithm leverages policy gradient methods, where policies-represented

by neural networks-determine the actions taken by the agent based on the current state. PPO effectively handles the dynamic nature of training data generated through continuous agent-environment interactions, a feature that differentiates it from static datasets used in supervised learning. The innovation of PPO lies in its 'surrogate' objective function, optimised via stochastic gradient ascent. This approach allows for multiple updates from the same batch of data, enhancing both training efficiency and stability over traditional policy gradient methods. Developed by OpenAI, PPO was designed to balance ease of implementation with the robust performance characteristics of more complex algorithms like Trust Region Policy Optimisation (TRPO), but without the associated computational complexity. PPO operates by maximising expected cumulative rewards through iterative policy adjustments that increase the likelihood of actions leading to higher rewards. A key feature of PPO is its use of a clipping mechanism in the objective function, which limits the extent of policy updates, thus preventing drastic changes and maintaining stability during training.

<missing-text>

Python Library HuggingFace Transformer Reinforcement Learning (TRL 4 ) package supports the PPO Trainer 5 for training language models from the preference data.

The PPOTrainer expects to align a generated response with a query given the rewards obtained from the Reward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset, we then use these prompts to generate the a responses from the SFT model. Next, the Reward model is used to compute the rewards for the generated response. Finally, these rewards are used to optimise the SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we can rename to query. Each of the other data-points required to optimise the SFT model are obtained during the training loop.

6.8.1 Benefits of PPO

 1. Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable and reliable policy updates. The clipped surrogate objective function is central to this stability, as it limits policy updates to prevent large, potentially destabilising changes. This results in smoother and more consistent learning.

 2. Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively straightforward to implement. It avoids the need for second-order optimisation techniques, making it more

accessible to less experienced practitioners.

 3. Sample Efficiency: PPO achieves data efficiency through its use of the clipped surrogate objective. This mechanism regulates policy updates, ensuring stability while effectively reusing training data. Consequently, PPO tends to be more sample-efficient than other reinforcement learning algorithms, performing well with fewer samples, which is advantageous in scenarios where data collection is costly or time-consuming.

6.8.2 Limitations of PPO

 1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate policy and value networks, necessitating substantial computational resources for training. This complexity often results in extended training durations and increased operational expenses.

 2. Hyperparameter Sensitivity: PPO's performance is highly dependent on several hyperparameters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance requires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy outcomes",chunk,,1
f27137d3a6af0576ba9f86633927ed4ed3740b65361c610a36e775fc48b6383241d896786996c04961b8e081b45dea62c7326b28b7bc5cc120789e8f928591d2," 1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate policy and value networks, necessitating substantial computational resources for training. This complexity often results in extended training durations and increased operational expenses.

 2. Hyperparameter Sensitivity: PPO's performance is highly dependent on several hyperparameters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance requires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy outcomes or instability during the learning process.

 3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared to earlier methods, it can still encounter convergence issues, particularly in highly dynamic or complex environments. Maintaining stable policy updates remains a significant challenge.

 4. Reward Signal Dependence: PPO's effectiveness is heavily reliant on a well-defined reward signal to guide the learning process. In scenarios where designing an appropriate reward function is challenging or impractical, PPO may struggle to attain the desired results.

6.8.3 Tutorial for training models using PPO technique

The tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO technique can be found here.

6.9 Direct Preference Optimisation (DPO)

Direct Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models (LMs) with human preferences, bypassing the complexity of reinforcement learning from human feedback (RLHF). Large-scale unsupervised LMs typically lack precise behavioural control, necessitating methods like RLHF that fine-tune models using human feedback. However, RLHF is intricate, involving the creation of reward models and the fine-tuning of LMs to maximise estimated rewards, which can be unstable and computationally demanding. DPO addresses these challenges by directly optimising LMs with a simple classification objective that aligns responses with human preferences. This approach eliminates the need for explicit reward modelling and extensive hyperparameter tuning, enhancing stability and efficiency. DPO optimises the desired behaviours by increasing the relative likelihood of preferred responses while incorporating dynamic importance weights to prevent model degeneration. Thus, DPO simplifies the preference learning pipeline, making it an effective method for training LMs to adhere to human preferences.

Python Library HuggingFace TRL package supports the DPO Trainer 6 for training language models from the preference data. The DPO training process requires a dataset formatted in a very specific manner. If you are utilising the default DPODataCollatorWithPadding data collator, your final dataset object must include three specific entries, which should be labelled as follows:

 · Prompt

 · Chosen

 · Rejected

HuggingFace offers datasets compatible with DPO and can be accessed here.

<missing-text>

<missing-text>

Preference data

<missing-text>

Maximum likelihood

6.9.1 Benefits of DPO

 1. Direct Alignment with Human Preferences: DPO directly optimises models to generate responses that align with human preferences, thereby producing more favourable outputs.

 2. Minimised Dependence on Proxy Objectives: In contrast to methods that rely on nextword prediction, DPO leverages explicit human preferences, resulting in responses that are more reflective of human behaviour.

 3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such as dialogue generation or creative writing, DPO excels in aligning the model with human preferences.

6.9.2 Best Practices for DPO

 1. High-Quality Preference Data: The performance of the model is heavily influenced by the quality of preference data. Ensure the dataset includes clear and consistent human preferences.

 2. Optimal Beta Value: Experiment with various beta values to manage the influence of the reference model. Higher beta values prioritise the reference model's preferences more strongly.

 3. Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch size, and LoRA configuration to determine the best settings for your dataset and task.

 4. Evaluation on Target Tasks: Continuously assess the model's performance on the target task using appropriate metrics to monitor progress and ensure the achievement of desired results.

 5. Ethical Considerations: Pay attention to potential biases in the preference data and take steps to mitigate them, preventing the model from adopting and amplifying these biases.

6.9.3 Tutorial for training models using DPO technique

The tutorial for DPO training, including the full source code of the training scripts for SFT and DPO, is available here.

6.9.4 Is DPO Superior to PPO for LLM Alignment?

The recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of rewardbased and reward-free methods within RLHF. Reward-based methods, such as those developed by OpenAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like Proximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods, including Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,

<missing-text>

with DPO focusing exclusively on policy optimisation through a logarithmic representation of the reward function.

One of the objectives of this study is to determine whether DPO is genuinely superior to PPO in the RLHF domain. The study combines theoretical and empirical analyses to uncover the inherent limitations of DPO and identify critical factors that enhance PPO's practical performance in RLHF.

Theoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution responses. Empirical results indicate that DPO's performance is notably affected by shifts in the distribution between model outputs and the preference dataset. Furthermore, the study highlights that while iterative DPO may offer improvements over static data training, it still fails to enhance performance in challenging tasks such as code generation. Ablation studies on",chunk,,1
4a68b919c4c781ccd28999ce6e49433dc29d2c020b6db5597567299ec590e8ddc77f693954076d74a3a6038d2d5b18bfbe3ddf7452fbfed145f4d08b55b32233," limitations of DPO and identify critical factors that enhance PPO's practical performance in RLHF.

Theoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution responses. Empirical results indicate that DPO's performance is notably affected by shifts in the distribution between model outputs and the preference dataset. Furthermore, the study highlights that while iterative DPO may offer improvements over static data training, it still fails to enhance performance in challenging tasks such as code generation. Ablation studies on PPO reveal essential components for optimal performance, including advantage normalisation, large batch sizes, and exponential moving average updates for the reference model's parameters. These findings form the basis of practical tuning guidelines, demonstrating PPO's robust effectiveness across diverse tasks and its ability to achieve stateof-the-art results in challenging code competition tasks. Specifically, on the CodeContest dataset, the PPO model with 34 billion parameters surpasses AlphaCode-41B, showing a significant improvement in performance metrics.

6.10 Odds-Ratio Preference Optimization (ORPO)

Odds-Ratio Preference Optimization (ORPO) is a novel approach designed to align the output of language models with desired responses by introducing a penalisation mechanism for undesirable outputs. Unlike traditional supervised fine-tuning (SFT) approaches, which focus solely on maximising the likelihood of correct responses, ORPO adds a specific odds-ratio based loss to penalise unwanted generations. This technique provides a refined method for improving preference alignment without relying on a reference model, making it efficient for large-scale implementations.

Given an input sequence x , the log-likelihood of generating an output sequence y of length m is computed as:

log P θ ( y | x ) = 1 m m ∑ i =1 log P θ ( y i | x )

The odds of generating the output sequence y given input x is expressed as:

odds θ ( y | x ) = P θ ( y | x ) 1 -P θ ( y | x )

ORPO introduces an odds-ratio that contrasts the likelihood of generating a preferred (chosen) response y w with a less preferred (rejected) response y l , defined as:

OR θ ( y w , y l | x ) = odds θ ( y w | x ) odds θ ( y l | x )

The ORPO loss function incorporates two components:

· Supervised Fine-tuning Loss (SFT) :

L SFT = -1 M M ∑ k =1 | V | ∑ i =1 y k i log p k i

where y k i is a binary indicator for the i -th token in the vocabulary, and p k i is its predicted probability.

· Odds-Ratio Loss :

L OR = -log σ ( log odds θ ( y w | x ) odds θ ( y l | x ) )

where σ is the sigmoid function applied to stabilise the log odds ratio.

Thus, the total ORPO objective is:

L ORPO = L SFT + λL OR

where λ controls the strength of preference alignment. This loss function effectively guides the model towards generating the chosen response while discouraging the rejected one, facilitating efficient alignment without the need for additional reference models [76].

Advantages of ORPO : ORPO's strength lies in its ability to perform preference alignment in a monolithic manner, bypassing the need for separate phases of fine-tuning and preference optimisation. This reduces computational overhead and provides state-of-the-art performance across various models, including LLaMA and Mistral, when evaluated on benchmark tasks such as AlpacaEval and MT-Bench [77].

6.11 Pruning LLMs

Pruning LLMs involves eliminating unnecessary or redundant components from a neural network to reduce its size and complexity, thereby enhancing its efficiency and performance. This process assists AI developers and engineers in addressing the challenges associated with deploying AI models in resourcelimited environments, such as mobile devices, edge computing, or embedded systems. Pruning AI models can be achieved through various techniques, each suited to the type and structure of the neural network, the pruning objective, and the pruning criterion. The following are common approaches:

 1. Weight Pruning: Involves removing weights or connections with minimal magnitude or impact on the output. This method reduces the number of parameters and operations in the model, although it may not necessarily decrease memory footprint or latency.

 2. Unit Pruning: Eliminates entire units or neurons with the lowest activation or contribution to the output. This technique can reduce the model's memory footprint and latency but may require retraining or fine-tuning to maintain performance.

 3. Filter Pruning: Involves removing entire filters or channels in convolutional neural networks that have the least importance or relevance to the output. This strategy also reduces memory footprint and latency, though it may necessitate retraining or fine-tuning to preserve performance [78].

6.11.1 When to Prune AI Models?

Pruning AI models can be conducted at various stages of the model development and deployment cycle, contingent on the chosen technique and objective.

 1. Pre-Training Pruning: Leverages prior knowledge or heuristics to determine the optimal network structure before training begins. This approach can save time and resources during training but may necessitate careful design and experimentation to identify the best configuration.

 2. Post-Training Pruning: Involves using metrics or criteria to assess the importance or impact of each network component after training. This method helps maintain model performance but may require additional validation and testing to ensure quality and robustness.

 3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback or signals. This approach can optimise the model for different scenarios or tasks but may involve higher computational overhead and complexity to implement and execute.

6.11.2 Benefits of Pruning

 1",chunk,,1
37d97c622365f841418608936d8c4aed1956f90742820abb6d399dbcd904be73782aed71112a0c0e81d616fdf7e7ddb9f6661c13e981f8b6d1329738f6c44259,": Involves using metrics or criteria to assess the importance or impact of each network component after training. This method helps maintain model performance but may require additional validation and testing to ensure quality and robustness.

 3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback or signals. This approach can optimise the model for different scenarios or tasks but may involve higher computational overhead and complexity to implement and execute.

6.11.2 Benefits of Pruning

 1. Reduced Size and Complexity: Pruning decreases the size and complexity of AI models, making them easier to store, transmit, and update.

 2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and more reliable.

 3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone to overfitting, and more adaptable to new data or tasks.

6.11.3 Challenges of Pruning

 1. Balance Between Size Reduction and Performance: Achieving the optimal balance between reducing size and complexity and maintaining performance is challenging; excessive or insufficient pruning can degrade model quality and functionality.

 2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objective for the specific neural network type and structure is crucial, as different methods can produce varying effects and outcomes.

 3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure pruning has not introduced errors, biases, or vulnerabilities that could impact performance and robustness.

Chapter 7

Stage 5: Evaluation and Validation

7.1 Steps Involved in Evaluating and Validating Fine-Tuned Models

 1. Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy, to measure the difference between the predicted and actual distributions of the data.

 2. Interpret Training Loss Curve: Monitor and analyse the training loss curve to ensure the model is learning effectively, avoiding patterns of underfitting or overfitting.

 3. Run Validation Loops: After each training epoch, evaluate the model on the validation set to compute relevant performance metrics and track the model's generalisation ability.

 4. Monitor and Interpret Results: Consistently observe the relationship between training and validation metrics to ensure stable and effective model performance.

 5. Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning rate, batch size, and number of training epochs to optimise model performance and prevent overfitting.

7.2 Setting Up Evaluation Metrics

Cross-entropy is a key metric for evaluating LLMs during training or fine-tuning. Originating from information theory, it quantifies the difference between two probability distributions.

7.2.1 Importance of Cross-Entropy for LLM Training and Evaluation

Cross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model to produce high-quality predictions by minimising discrepancies between the predicted and actual data. In LLMs, each potential word functions as a separate class, and the model's task is to predict the next word given the context. This task is inherently complex, requiring the model to understand syntax, semantics, and context deeply.

7.2.2 Beyond Cross-Entropy: Advanced LLM Evaluation Metrics

While cross-entropy remains fundamental, evaluating LLMs effectively necessitates additional metrics tailored to various aspects of model performance. Here are some advanced metrics employed in LLM evaluation:

Perplexity

Perplexity measures how well a probability distribution or model predicts a sample. In the context of LLMs, it evaluates the model's uncertainty about the next word in a sequence. Lower perplexity indicates better performance, as the model is more confident in its predictions.

Factuality

Factuality assesses the accuracy of the information produced by the LLM. It is particularly important for applications where misinformation could have serious consequences. Higher factuality scores correlate with higher output quality.

LLM Uncertainty

LLM uncertainty is measured using log probability, helping to identify low-quality generations. Lower uncertainty indicates higher output quality. This metric leverages the log probability of each generated token, providing insights into the model's confidence in its responses.

Prompt Perplexity

This metric evaluates how well the model understands the input prompt. Lower prompt perplexity indicates a clear and comprehensible prompt, which is likely to yield better model performance.

Context Relevance

In retrieval-augmented generation (RAG) systems, context relevance measures how pertinent the retrieved context is to the user query. Higher context relevance improves the quality of generated responses by ensuring that the model utilises the most relevant information.

Completeness

Completeness assesses whether the model's response fully addresses the query based on the provided context. High completeness ensures that all relevant information is included in the response, enhancing its utility and accuracy.

Chunk Attribution and Utilisation

These metrics evaluate how effectively the retrieved chunks of information contribute to the final response. Higher chunk attribution and utilisation scores indicate that the model is efficiently using the available context to generate accurate and relevant answers.

Data Error Potential

This metric quantifies the difficulty the model faces in learning from the training data. Higher data quality results in lower error potential, leading to better model performance.

Safety Metrics

Safety metrics ensure that the LLM's outputs are appropriate and non-harmful. These are included in the final sections of the chapter.

Integrating these advanced metrics provides a holistic view of LLM performance, enabling developers to fine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to ensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and reliably across diverse applications 1 .

7.3 Understanding the Training Loss Curve

The training loss curve plots the loss value against training epochs and is essential for monitoring model performance.

7.3.1 Interpreting Loss Curves

An ideal",chunk,,1
99d62c255831002f8aa70912d828d7aeb6243f7c7a021aa4528320877a5c445caa0e92feb3f8729404bd33afe8015af7ed1e7d3a2bcde06366629d740b598fc0," a holistic view of LLM performance, enabling developers to fine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to ensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and reliably across diverse applications 1 .

7.3 Understanding the Training Loss Curve

The training loss curve plots the loss value against training epochs and is essential for monitoring model performance.

7.3.1 Interpreting Loss Curves

An ideal training loss curve shows a rapid decrease in loss during initial stages, followed by a gradual decline and eventual plateau. Specific patterns to look for include:

 1. Underfitting: High loss value that does not decrease significantly over time, suggesting the model cannot learn the data.

 2. Overfitting: Decreasing training loss with increasing validation loss, indicating the model memorises the training data.

 3. Fluctuations: Significant variations may indicate a high learning rate or noisy gradients.

<missing-text>

7.3.2 Avoiding Overfitting

Techniques to prevent overfitting include:

 1. Regularisation: Adds a penalty term to the loss function to encourage smaller weights.

 2. Early Stopping: Stops training when validation performance no longer improves.

 3. Dropout: Randomly deactivates neurons during training to reduce sensitivity to noise.

 4. Cross-Validation: Splits data into multiple subsets for training and validation to assess model generalisation.

 5. Batch Normalisation: Normalises inputs to each layer during training to stabilise the learning process.

 6. Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount of diverse data and batch sizes.

7.3.3 Sources of Noisy Gradients

Noisy gradients are common during the training of machine learning models, including LLMs. They arise from variability in gradient estimates due to stochastic gradient descent and its variants. Strategies to manage noisy gradients include:

 1. Learning Rate Scheduling: Gradually decreasing the learning rate during training can reduce the impact of noisy gradients.

 2. Gradient Clipping: Setting a threshold for gradient values prevents large updates that can destabilise training.

7.4 Running Validation Loops

Validation loops provide an unbiased evaluation of model performance. Typical steps include:

 1. Split Data: Divide the dataset into training and validation sets.

 2. Initialise Validation: Evaluate the model on the validation set at the end of each epoch.

 3. Calculate Metrics: Compute relevant performance metrics, such as cross-entropy loss.

 4. Record Results: Log validation metrics for each epoch.

 5. Early Stopping: Optionally stop training if validation loss does not improve for a predefined number of epochs.

7.5 Monitoring and Interpreting Results

Monitoring validation results involves analysing trends in validation metrics over epochs. Key aspects include:

 1. Consistent Improvement: Indicates good model generalisation if both training and validation metrics improve and plateau.

 2. Divergence: Suggests overfitting if training metrics improve while validation metrics deteriorate.

 3. Stability: Ensure validation metrics do not fluctuate significantly, indicating stable training.

7.6 Hyperparameter Tuning and Other Adjustments

Fine-tuning involves adjusting key hyperparameters to achieve optimal performance. Important hyperparameters include:

 1. Learning Rate: Determines the step size for updating model weights. A good starting point is 2e-4, but this can vary.

 2. Batch Size: Larger batch sizes lead to more stable updates but require more memory.

 3. Number of Training Epochs: Balancing the number of epochs ensures the model learns sufficiently without overfitting or underfitting.

 4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models.

Other tunable parameters include dropout rate, weight decay, and warmup steps.

7.6.1 Data Size and Quality

The efficacy of LLMs is directly impacted by the quality of their training data. Ensuring that datasets are clean, relevant, and adequate is crucial. Data cleanliness refers to the absence of noise, errors, and inconsistencies within the labelled data. For example, having a phrase like 'This article suggests. . . ' multiple times in the training data can corrupt the response of LLMs and add a bias towards using this specific phrase more often and in inappropriate situations.

7.7 Benchmarking Fine-Tuned LLMs

Modern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE, HellaSwag, TruthfulQA, and MMLU (See Table 7.1). These benchmarks evaluate various capabilities and provide an overall view of LLM performance.

<missing-text>

As LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging current benchmarks and setting new standards in the domain. Given the diverse nature of LLMs and the tasks they can perform, the choice of benchmarks depends on the specific tasks the LLM is expected to handle. For generic applicability, various benchmarks for different downstream applications and reasoning should be utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant benchmarks like BigCodeBench for coding.

7.8 Evaluating Fine-Tuned LLMs on Safety Benchmark

The safety aspects of Large Language Models (LLMs) are increasingly scrutinised due to their ability to generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the embedded safety and ethical guidelines within the models, similar to code injection techniques used in traditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions, potentially violating OpenAI's guidelines. This underscores the necessity for robust safeguards to ensure LLM outputs adhere",chunk,,1
fd0c0a4b7afe7c0d37804a97acade5f90d0317202f7120030ceb7080a7c3c8171f5d5bb2a947b14441cf940e78b7dc930abb11c591c86c451739a5ba03237020,") are increasingly scrutinised due to their ability to generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the embedded safety and ethical guidelines within the models, similar to code injection techniques used in traditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions, potentially violating OpenAI's guidelines. This underscores the necessity for robust safeguards to ensure LLM outputs adhere to ethical and safety standards.

DecodingTrust [79] provides a comprehensive evaluation of the trustworthiness of LLMs, notably comparing GPT-4 with GPT-3.5 (ChatGPT). This evaluation spans several critical areas:

 1. Toxicity: Optimisation algorithms and generative models are employed to create challenging prompts that test the model's ability to avoid generating harmful content.

 2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess model bias, helping to understand and mitigate prejudiced responses.

 3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by challenging them with sophisticated algorithms intended to deceive or mislead.

 4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle inputs that differ significantly from their training data, such as poetic or Shakespearean styles.

 5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading information are used to test the model's robustness across various tasks.

 6. Privacy: Various levels of privacy evaluation assess how well models safeguard sensitive information during interactions and understand privacy-related contexts.

 7. Hallucination Detection: Identifies instances where the model generates information not grounded in the provided context or factual data. Lower hallucination rates improve the reliability and trustworthiness of the LLM's outputs.

 8. Tone Appropriateness: Assesses whether the model's output maintains an appropriate tone for the given context. This is particularly important for applications in customer service, healthcare, and other sensitive areas.

 9. Machine Ethics: Ethical assessments involve testing models with scenarios that require moral judgments, using datasets like ETHICS and Jiminy Cricket.

 10. Fairness: The fairness of models is evaluated by generating tasks that vary protected attributes, ensuring equitable responses across different demographic groups.

The dataset employed for evaluating the aforementioned eight safety dimensions can be found here. In partnership with HuggingFace, the LLM Safety Leaderboard utilises DecodingTrust's framework to provide a unified evaluation platform for LLM safety. This allows researchers and practitioners to better understand the capabilities, limitations, and risks associated with LLMs. Users are encouraged to submit their models to HuggingFace for evaluation, ensuring they meet the evolving standards of safety and reliability in the field.

7.9 Evaluating Safety of Fine-Tuned LLM using AI Models

7.9.1 Llama Guard

Llama Guard 2[80] is a safeguard model built on LLMs for managing risks in conversational AI applications. It effectively categorises both input prompts and responses from AI agents using a detailed safety risk taxonomy tailored to identify potential legal and policy risks in AI interactions. It utilises a detailed safety risk taxonomy designed to identify and manage potential legal and policy risks in interactions involving conversational AI. This taxonomy enables effective classification in areas such as:

 · Violence & Hate, addressing content that could incite violent acts or discrimination.

 · Sexual Content, targeting sexually explicit material or behaviour, especially involving minors.

 · Guns & Illegal Weapons, concerning the promotion or instruction of illegal armaments.

 · Regulated or Controlled Substances, covering illegal drugs and other controlled substances.

 · Suicide & Self-Harm, aimed at content that could encourage self-destructive behaviour.

 · Criminal Planning, for content that could assist in planning or executing criminal activities.

The core of Llama Guard 2 is its robust framework that allows for both prompt and response classification, supported by a high-quality dataset that enhances its ability to monitor conversational exchanges. Operating on a Llama2-7b model, Llama Guard 2 has been instruction-tuned to deliver strong performance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches or surpasses the capabilities of existing content moderation tools.

The model supports multi-class classification and generates binary decision scores. Its instruction finetuning allows for extensive customisation of tasks and adaptation of output formats. This feature enables users to modify taxonomy categories to align with specific use cases and supports flexible prompting capabilities, including zero-shot and few-shot applications. The adaptability and effectiveness of Llama Guard make it a vital resource for developers and researchers. By making its model weights publicly available, Llama Guard 2 encourages ongoing development and customisation to meet the evolving needs of AI safety within the community.

Llama Guard 3 represents the latest advancement over Llama Guard 2, having been fine-tuned on the Llama 3 8b model. The key difference between the two versions is that Llama Guard 3 expands upon the capabilities of Llama Guard 2 by introducing three new categories: Defamation, Elections, and

Code Interpreter Abuse.

Python Library: Llama Guard 3 is accessible via HuggingFace's AutoModelForCausalLM. 2 A detailed tutorial is available at this link. Please note that access to the model requires submitting a request to Hugging Face with the user details. Additionally, the model weights can be downloaded from the Meta platform by providing user details, and the link can be found here.

The prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available here and Llama Guard 3 is accessible here.

7.9.2 Shield Gemma

ShieldGemma [81] is an advanced content moderation model built on the Gem",chunk,,1
b1428d0e93ff3de1d2002860a4210d98267103b40d5cc8e2ebea9c48ec7a8e0479b626418f293a91cb885ac5a5deb9e1d81b96767f8daa0a210ea2634fa23295," that access to the model requires submitting a request to Hugging Face with the user details. Additionally, the model weights can be downloaded from the Meta platform by providing user details, and the link can be found here.

The prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available here and Llama Guard 3 is accessible here.

7.9.2 Shield Gemma

ShieldGemma [81] is an advanced content moderation model built on the Gemma2 platform, designed to enhance the safety and reliability of interactions between LLMs and users. It effectively filters both user inputs and model outputs to mitigate key harm types, including offensive language, hate speech, misinformation, and explicit content. The model's scalability, with options ranging from 2B to 27B parameters, allows for tailored applications that meet specific needs, such as reducing latency in online safety applications or enhancing performance in complex decision-making tasks.

A distinguishing feature of ShieldGemma is its novel approach to data curation. It leverages synthetic data generation techniques to create high-quality datasets that are robust against adversarial prompts and fair across diverse identity groups. This reduces the need for extensive human annotation, streamlining the data preparation process while ensuring the model's effectiveness. Compared to existing content moderation tools like LlamaGuard and WildGuard, which typically offer fixed-size models and limited customisation, ShieldGemma's flexible architecture and advanced data handling capabilities provide a more adaptable and efficient solution. These innovations position ShieldGemma as a significant advancement in LLM-based content moderation, offering developers and researchers a versatile tool that promotes safer and more reliable AI interactions across various platforms.

Python Library: The ShieldGemma series is available on HuggingFace via AutoModelForCausalLM. The models can be accessed here. A tutorial for running ShieldGemma 2B on Google Colab can be found here. Similar to Llama Guard series, ShieldGemma series also has guidelines for prompting and it can be found here.

7.9.3 WILDGUARD

WILDGUARD [82] is an innovative open-source tool developed to enhance the safety of interactions with large language models (LLMs). This tool addresses three critical moderation tasks: detecting

harmful intent in user prompts, identifying safety risks in model responses, and determining when a model appropriately refuses unsafe requests. Central to its development is WILDGUARD MIX 3 , a meticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and adversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used for training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples for evaluation.

The WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD TRAIN dataset, enabling it to perform all three moderation tasks in a unified, multi-task manner. Results show that WILDGUARD surpasses existing open-source moderation tools in effectiveness, particularly excelling in handling adversarial prompts and accurately detecting model refusals. On many benchmarks, WILDGUARD's performance is on par with or exceeds that of GPT-4, a much larger, closed-source model.

The quick start guide and additional information on WILDGUARD are available in GitHub and it can be accessed here.

Chapter 8

Stage 6: Deployment

8.1 Steps Involved in Deploying the Fine-Tuned Model

 1. Model Export: Save the fine-tuned model in a suitable format (e.g., ONNX, TensorFlow SavedModel, PyTorch) for deployment.

 2. Infrastructure Setup: Prepare the deployment environment, including necessary hardware, cloud services, and containerisation tools.

 3. API Development: Create APIs to allow applications to interact with the model, facilitating prediction requests and responses.

 4. Deployment: Deploy the model to the production environment, making it accessible to end-users or applications.

8.2 Cloud-Based Providers for LLM Deployment

Cloud-based large language model (LLM) inferencing frequently employs a pricing model based on the number of tokens processed. Users are charged according to the volume of text analysed or generated by the model. While this pricing structure can be cost-effective for sporadic or small-scale usage, it may not always be economical for larger or continuous workloads.

In some scenarios, hosting an LLM solution in-house may offer better long-term cost savings, especially if there is consistent or high-volume usage. Managing your own infrastructure provides greater control over resource allocation and allows for cost optimisation based on specific needs. Additionally, self-hosting offers advantages in terms of data privacy and security, as sensitive information remains within your own environment.

However, it is crucial to carefully evaluate the total cost of ownership when comparing cloud-based solutions with self-hosted alternatives. This evaluation should consider factors such as hardware expenses, maintenance, and operational overheads. Ultimately, the decision should be informed by a comprehensive cost-benefit analysis, considering both short-term affordability and long-term sustainability.

Several companies offer deployment services for large language models (LLMs), providing a range of tools and platforms to efficiently implement and manage these models. Here's a detailed list of some prominent providers and their services:

· Amazon Web Services (AWS)

 - Amazon Bedrock: This service offers a suite of foundation models including Amazon Titan, which supports various NLP tasks such as summarisation and text generation. Bedrock integrates seamlessly with other AWS services for scalable and secure deployment.

 - Amazon SageMaker: Provides an end-to-end machine learning service that includes tools for building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained models and step-by-step guides to simplify the deployment process.

 - Tutorial: This tutorial explains the deployment of LLM Agents on Amazon Bedrock. Another tutorial explains end-to-end fine-tuning and deployment",chunk,,1
9690610e670ef782901274ba10a183fcfbe92007a4ffcaf4ed1b2de0f978f78496d80895d3698fcf7af06c0b4cbefd216fb1656e9c4059844a556aa912735e5b," various NLP tasks such as summarisation and text generation. Bedrock integrates seamlessly with other AWS services for scalable and secure deployment.

 - Amazon SageMaker: Provides an end-to-end machine learning service that includes tools for building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained models and step-by-step guides to simplify the deployment process.

 - Tutorial: This tutorial explains the deployment of LLM Agents on Amazon Bedrock. Another tutorial explains end-to-end fine-tuning and deployment of LLMs with Sagemaker Canvas and Amazon Bedrock. General guidelines of Amazon Bedrock for LLM users can be found here.

· Microsoft Azure

 - Azure OpenAI Service: This service offers access to OpenAI's powerful models like GPT3.5 and Codex. It provides capabilities for embedding, image generation with DALL-E, and speech-to-text with Whisper. Azure's integration with OpenAI models ensures robust deployment options for various applications.

 - Azure Machine Learning: Supports the deployment of custom and pre-trained models, offering tools for model management, deployment, and monitoring. It integrates with Azure's broader ecosystem for scalable and secure ML operations.

 - Tutorial: Here is the tutorial for creating and deploying an Azure OpenAI Service in Microsoft Azure platform.

· Google Cloud Platform (GCP)

 - Vertex AI: This platform allows the deployment of large language models with tools for training, tuning, and serving models. Vertex AI supports models like BERT and GPT-3, providing extensive MLOps capabilities for end-to-end management.

 - Cloud AI API: Offers APIs for NLP tasks such as translation, sentiment analysis, and entity recognition. These APIs are backed by Google's powerful infrastructure, ensuring high performance and reliability.

 - Tutorial: This document contains a tutorial for training and deploying an LLM in GCP.

· Hugging Face

 - Inference API: This service allows users to deploy and manage LLMs hosted on Hugging Face's infrastructure. It supports various models from the Transformers library and provides an easy-to-use API for integrating these models into applications.

 - Spaces: A collaborative environment where users can deploy and share models using Hugging Face's hosting platform. It supports deploying custom models and interactive demos.

 - Tutorial: This document contains a tutorial for training and deploying an LLM using HuggingFace Inference API.

· Other Platforms

 - OpenLLM: Provides deployment solutions here.

 - Deepseed: Offers deployment solutions here.

8.3 Techniques for Optimising Model Performance During Inference

Optimising model performance during inference is crucial for the efficient deployment of large language models (LLMs). The following advanced techniques offer various strategies to enhance performance, reduce latency, and manage computational resources effectively.

8.3.1 Traditional On-Premises GPU-Based Deployments

This conventional approach to deploying large language models (LLMs) involves using Graphics Processing Units (GPUs) due to their parallel processing capabilities, which enable fast and efficient inference. However, this method requires upfront hardware investment and may not be suitable for applications with fluctuating demand or limited budgets. GPU-based deployments face several challenges:

 1. Resource utilisation may suffer during periods of low demand due to idle servers.

 2. Scaling up or down often requires physical hardware modifications, which can be time-consuming.

 3. Centralised servers can introduce single points of failure and scalability limitations.

To mitigate these issues, strategies such as load balancing between multiple GPUs, fallback routing, model parallelism, and data parallelism can be employed to achieve better results. Optimisation techniques like distributed inference using PartialState from accelerate can further enhance efficiency.

Example use case: Large-Scale NLP Application

For instance, a large e-commerce platform implemented traditional on-premises GPU-based deployment to handle millions of customer queries daily. By utilising load balancing and model parallelism, they were able to achieve a significant reduction in latency and improved customer satisfaction.

8.3.2 Distributed LLM: Torrent-Style Deployment and Parallel Forward Passes

An innovative deployment strategy for large language models (LLMs) involves distributing them across multiple GPUs in a decentralised, torrent-style manner. Libraries like Petals 1 can perform this task. Petals functions as a decentralised pipeline designed for rapid neural network inference by partitioning the model into distinct blocks or layers, which are distributed across multiple geographically dispersed servers. Users can connect their own GPUs to this network, acting as both contributors and clients who can access and apply the model to their data.

When a client request is received, the network routes it through a series of servers optimised to minimise the total forward pass time. Each server dynamically selects the most optimal set of blocks, adapting to the current bottlenecks in the pipeline. This framework leverages decentralisation principles to distribute computational load across diverse regions, sharing computational resources and GPUs in a way that reduces the financial burden on individual organisations. This collaborative approach not only optimises resource utilisation but also fosters a global community dedicated to shared AI goals.

<missing-text>

Example use case: Global Research Collaboration

A consortium of research institutions implemented a distributed LLM using the Petals framework to analyse large datasets across different continents. By leveraging the decentralised nature of Petals, they achieved high efficiency in processing and collaborative model development.

8.3.3 WebGPU-Based Deployment of LLM

This deployment option for large language models (LLMs) involves utilising WebGPU, a web standard that provides a low-level interface for graphics and compute applications on the web platform. With WebGPU, organisations can harness the power of GPUs directly within web browsers, enabling efficient inference for LLMs in web-based applications. WebGPU enables high-performance computing and graphics rendering directly within the client's web browser. It allows developers to utilise the client's GPU for tasks such as rendering graphics, accelerating computational workloads, and performing parallel processing, all without the need for plugins or additional software",chunk,,1
d893e35311b8207ed22375cb51b32b8090af60464f36a956b8d9d3bebe7cb2708a5be82abddf784ae09fdc1dc23197c933f2dd0655a52315aac4a28501b36316,", a web standard that provides a low-level interface for graphics and compute applications on the web platform. With WebGPU, organisations can harness the power of GPUs directly within web browsers, enabling efficient inference for LLMs in web-based applications. WebGPU enables high-performance computing and graphics rendering directly within the client's web browser. It allows developers to utilise the client's GPU for tasks such as rendering graphics, accelerating computational workloads, and performing parallel processing, all without the need for plugins or additional software installations. This capability permits complex computations to be executed efficiently on the client's device, leading to faster and more responsive web applications.

8.3.4 LLM on WebGPU using WebLLM

Clients can access powerful large language models and chatbots directly in their browser, leveraging WebGPU acceleration. This approach eliminates server dependencies, providing users with exceptional performance and enhanced privacy. WebLLM facilitates the use of large language models directly in the client's browser to perform tasks such as filtering out personally identifiable information (PII) or named entity recognition (NER) on data without transmitting it over the network. This ensures enhanced privacy and security by retaining sensitive information on the client side.

<missing-text>

Additional Use Cases for WebLLM

 1. Language Translation: Enable real-time translation of text directly in the browser, allowing users to communicate across language barriers without transmitting their messages over the network.

 2. Code Autocompletion: Develop code editors that provide intelligent autocompletion suggestions based on context, leveraging WebLLM to understand and predict code snippets.

 3. Customer Support Chatbots: Implement chatbots on websites to provide instant customer support and answer frequently asked questions without relying on external servers.

 4. Data Analysis and Visualisation: Create browser-based tools for analysing and visualising data, with WebLLM assisting in data processing, interpretation, and generating insights.

 5. Personalised Recommendations: Develop recommendation engines that offer personalised product recommendations, content suggestions, or movie/music recommendations based on user preferences and behaviour.

 6. Privacy-Preserving Analytics: Develop analytics platforms that perform data analysis directly in the browser, ensuring that sensitive information remains on the client side and reducing the risk of data breaches.

Example use case: Privacy-Focused Web Application

A healthcare startup deployed an LLM using WebLLM to process patient information directly within the browser, ensuring data privacy and compliance with healthcare regulations. This approach significantly reduced the risk of data breaches and improved user trust.

8.3.5 Quantised LLMs

Model quantisation is a technique utilised to reduce the size of an AI model by representing its parameters with fewer bits. In traditional machine learning models, each parameter (e.g., weights and biases in neural networks) is typically stored as a 32-bit floating-point number, necessitating significant memory and computational resources, particularly for large models. Quantisation aims to alleviate this by reducing the precision of these parameters. For instance, instead of storing each parameter as a 32-bit floatingpoint number, they may be represented using fewer bits, such as 8-bit integers. This compression reduces the memory footprint of the model, making it more efficient to deploy and execute, especially in resource-constrained environments like mobile devices or edge devices. QLoRA is a popular example of this quantisation for LLMs and can be used to deploy LLMs locally or host them on external servers.

Example use case: Edge Device Deployment

A tech company used quantised LLMs to deploy advanced NLP models on mobile devices, enabling offline functionality for applications such as voice recognition and translation. This deployment significantly improved app performance and user experience by reducing latency and reliance on internet connectivity.

8.3.6 vLLMs

The vLLM 2 system efficiently handles requests by employing a block-level memory management method and preemptive request scheduling. It utilises the PagedAttention[84] algorithm to manage the keyvalue (KV) cache, thereby reducing memory waste and fragmentation. By batching requests and sharing physical blocks across multiple samples, vLLM optimises memory usage and enhances throughput. Performance tests indicate that vLLM surpasses other systems in various decoding scenarios. Consider a transformer-based model tasked with summarising a lengthy book. Traditional transformers process the entire book simultaneously, which can be both computationally and memory-intensive, especially for extended texts. With PagedAttention, the book is divided into smaller segments or pages. The model then focuses on summarising one page at a time, rather than the entire book simultaneously. This approach reduces computational complexity and memory requirements, making it more feasible to process and summarise lengthy texts efficiently.

Example use case: High-Volume Content Generation

A content marketing agency implemented vLLMs for generating large volumes of SEO-optimised content. By leveraging the efficient memory management of vLLMs, they were able to handle multiple concurrent requests, significantly increasing their content production rate while maintaining high quality.

8.4 Key Considerations for Deployment of LLMs

Deploying large language models (LLMs) effectively requires careful planning and consideration of various factors to ensure optimal performance, cost-efficiency, and security. Key considerations include:

· Infrastructure Requirements:

 - Compute Resources: Ensure adequate CPU/GPU resources to handle the model's computational demands. High-performance GPUs are typically required for efficient inference and training.

 - Memory: LLMs, especially those with billions of parameters, require substantial memory. Memory management techniques such as quantisation and model parallelism can be employed to optimise usage.

· Scalability:

 - Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers, which can improve performance and handle increased demand.

 - Load Balancing: Implement load balancing strategies to ensure even distribution of requests and prevent any single point of failure.

· Cost Management:

 - Token-based Pricing: Understand the cost implications of token-based pricing models offered by cloud providers. This model charges based on the number of tokens",chunk,,1
1a87ad9fada8f10085523210a40b6ddf986278ee943e47882e5cd2091d0fbc8e545fd2de9a944abf28d934d10b9845d26cc30ae3cfc5fed66ac6cd6ea8ca0dfe," such as quantisation and model parallelism can be employed to optimise usage.

· Scalability:

 - Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers, which can improve performance and handle increased demand.

 - Load Balancing: Implement load balancing strategies to ensure even distribution of requests and prevent any single point of failure.

· Cost Management:

 - Token-based Pricing: Understand the cost implications of token-based pricing models offered by cloud providers. This model charges based on the number of tokens processed, which can become expensive with high usage.

 - Self-Hosting: Evaluate the costs and benefits of self-hosting versus cloud hosting. Selfhosting might offer long-term savings for consistent, high-volume usage but requires significant upfront investment in hardware and ongoing maintenance.

· Performance Optimisation:

 - Latency: Minimise latency to ensure real-time performance, particularly for applications requiring instant responses like chatbots and virtual assistants.

 - Throughput: Maximise throughput to handle a high volume of requests efficiently. Techniques like batching and efficient memory management (e.g., PagedAttention) can help.

· Security and Privacy:

 - Data Security: Implement robust security measures to protect sensitive data, including encryption and secure access controls.

 - Privacy: Ensure compliance with data privacy regulations by keeping sensitive data within your environment if self-hosting, or ensuring cloud providers comply with relevant privacy standards.

· Maintenance and Updates:

 - Model Updates: Regularly update the model to incorporate new data and improve performance. Automate this process if possible to reduce manual effort.

 - System Maintenance: Plan for regular maintenance of the infrastructure to prevent downtime and ensure smooth operation.

· Flexibility and Customisation:

 - Fine-Tuning: Allow for model fine-tuning to adapt the LLM to specific use cases and datasets. Fine-tuning can improve accuracy and relevance in responses.

 - API Integration: Ensure the deployment platform supports easy integration with existing systems and workflows through APIs and SDKs.

· User Management:

 - Access Control: Implement role-based access control to manage who can deploy, use, and maintain the LLM.

 - Monitoring and Logging: Set up comprehensive monitoring and logging to track usage, performance, and potential issues. This helps in proactive troubleshooting and optimisation.

· Compliance:

 - Regulatory Compliance: Ensure that the deployment adheres to all relevant regulatory and legal requirements, including data protection laws like GDPR, HIPAA, etc.

 - Ethical Considerations: Implement ethical guidelines to avoid biases and ensure the responsible use of LLMs.

· Support and Documentation:

 - Technical Support: Choose a deployment platform that offers robust technical support and resources.

 - Documentation: Provide comprehensive documentation for developers and users to facilitate smooth deployment and usage.

Chapter 9

Stage 7: Monitoring and Maintenance

9.1 Steps Involved in Monitoring and Maintenance of Deployed Fine-Tuned LLMs

Continuous monitoring and maintenance of fine-tuned LLMs are essential to ensure their optimal performance, accuracy, and security over time. Below are the key steps involved in this process:

 1. Setup Initial Baselines: Establish initial performance baselines by evaluating the model on a comprehensive test dataset. Record metrics such as accuracy, latency, throughput, and error rates to serve as reference points for future monitoring.

 2. Performance Monitoring: Implement systems to continuously track key performance metrics such as response time, server load, and token usage. Regularly compare these metrics against the established baselines to detect any deviations.

 3. Accuracy Monitoring: Continuously evaluate the model's predictions against a ground truth dataset. Use metrics like precision, recall, F1 score, and cross-entropy loss to ensure the model maintains high accuracy levels.

 4. Error Monitoring: Track and analyse errors, including runtime errors and prediction errors. Implement logging mechanisms to capture detailed information about each error for troubleshooting and improvement.

 5. Log Analysis: Maintain comprehensive logs for each prediction request and response, including input data, output predictions, response times, and encountered errors. Regularly review logs to identify patterns and areas for improvement.

 6. Alerting Mechanisms: Set up automated alerting systems to notify stakeholders of any anomalies or deviations from expected performance metrics. Integrate alerts with communication tools like Slack, PagerDuty, or email for timely responses.

 7. Feedback Loop: Establish a feedback loop with end-users to gather insights on model performance and user satisfaction. Use this feedback to continuously refine and improve the model.

 8. Security Monitoring: Implement robust security measures to monitor for threats, including unauthorised access, data breaches, and adversarial attacks. Use encryption, access control, and regular security audits to protect the model and data.

 9. Drift Detection: Continuously monitor for data and concept drift using statistical tests and drift detectors. Regularly evaluate the model on holdout datasets to detect changes in input data distribution or model performance.

 10. Model Versioning: Maintain version control for different iterations of the model. Track performance metrics for each version to ensure that the best-performing model is in production.

 11. Documentation and Reporting: Keep detailed documentation of monitoring procedures, metrics, and findings. Generate regular reports to provide stakeholders with insights into the model's performance and maintenance activities.

 12. Periodic Review and Update: Regularly assess and update the monitoring processes to incorporate new techniques, tools, and best practices, ensuring the monitoring system remains effective and up-to-date.

9.2 Continuous Monitoring of Model Performance

While large language model (LLM) applications undergo some form of evaluation, continuous monitoring remains inadequately implemented in most cases. This section outlines the components necessary to establish an effective monitoring programme aimed at safeguarding users and preserving brand integrity.

9.2.1 Functional Monitoring

Initially, it is crucial to monitor fundamental metrics consistently. This includes tracking metrics such as request volume, response times, token utilisation, costs incurred, and error rates.

9.2.2 Prompt Monitoring

Following functional",chunk,,1
38a6728aac6bcf737e45a075a16b7faf71404c0ee615c18851ae6e42633927de50016a6f9ad87167844e64c90b737990588cf4065936e21d036fd315f319cc71,"While large language model (LLM) applications undergo some form of evaluation, continuous monitoring remains inadequately implemented in most cases. This section outlines the components necessary to establish an effective monitoring programme aimed at safeguarding users and preserving brand integrity.

9.2.1 Functional Monitoring

Initially, it is crucial to monitor fundamental metrics consistently. This includes tracking metrics such as request volume, response times, token utilisation, costs incurred, and error rates.

9.2.2 Prompt Monitoring

Following functional metrics, attention should be directed towards monitoring user-generated prompts or inputs. Metrics like readability can provide valuable insights. LLM evaluators should be employed to detect potential toxicity in responses. Additionally, metrics such as embedding distances from reference

prompts prove insightful, ensuring adaptability to varying user interactions over time. Introducing a new evaluation category involves identifying adversarial attempts or malicious prompt injections, often overlooked in initial evaluations. Comparison against reference sets of known adversarial prompts helps identify and flag malicious activities. Evaluative LLMs play a crucial role in classifying prompts as benign or malicious.

9.2.3 Response Monitoring

Monitoring responses involves several critical checks to ensure alignment with expected outcomes. Parameters such as relevance, coherence (hallucination), topical alignment, sentiment, and their evolution over time are essential. Metrics related to toxicity and harmful output require frequent monitoring due to their critical impact. Prompt leakage represents an adversarial tactic wherein sensitive prompt information is illicitly extracted from the application's stored data. Monitoring responses and comparing them against the database of prompt instructions can help detect such breaches. Embedding distance metrics are particularly effective in this regard. Regular testing against evaluation datasets provides benchmarks for accuracy and highlights any performance drift over time. Tools capable of managing embeddings allow exportation of underperforming output datasets for targeted improvements.

9.2.4 Alerting Mechanisms and Thresholds

Effective monitoring necessitates well-calibrated alerting thresholds to avoid excessive false alarms. Implementing multivariate drift detection and alerting mechanisms can enhance accuracy. Consideration of false alarm rates and best practices for setting thresholds is paramount for effective monitoring system design. Alerting features should include integration with communication tools such as Slack and PagerDuty. Some systems offer automated response blocking in case of alerts triggered by problematic prompts. Similar mechanisms can be employed to screen responses for personal identifiable information (PII), toxicity, and other quality metrics before delivery to users. Custom metrics tailored to specific application nuances or innovative insights from data scientists can significantly enhance monitoring efficacy. Flexibility to incorporate such metrics is essential to adapt to evolving monitoring needs and advancements in the field.

9.2.5 Monitoring User Interface (UI)

The monitoring system's UI is pivotal, typically featuring time-series graphs of monitored metrics. Differentiated UIs facilitate in-depth analysis of alert trends, aiding root cause analysis. Advanced UI capabilities may include visualisations of embedding spaces through clustering and projections, providing insights into data patterns and relationships. Mature monitoring systems categorise data by users, projects, and teams, ensuring role-based access control (RBAC) to protect sensitive information. Optimising alert analysis within the UI interface remains an area where improvements can significantly reduce false alarm rates and enhance operational efficiency.

9.3 Updating LLM Knowledge

To improve the knowledge base of an LLM, continued pretraining is used to help LLM evolve with the latest knowledge and information. The world and language are constantly evolving. New information emerges, trends shift, and cultural references change. LLMs trained on static data can become outdated, leading to:

 · Factual Errors: Outdated information can cause LLMs to provide inaccurate responses.

 · Irrelevance: Models might miss the context of current events or use outdated references.

 · Bias Perpetuation: Biases present in training data can become entrenched if not addressed through updates.

9.3.1 Retraining Methods

 · Periodic Retraining: This involves refreshing the model's knowledge base at regular intervals (weekly, monthly, yearly) with new data. This is a straightforward method but requires a steady stream of high-quality, unbiased data.

 · Trigger-Based Retraining: This approach monitors the LLM's performance. When metrics like accuracy or relevance fall below a certain threshold, a retraining process is triggered. This method is more dynamic but requires robust monitoring systems and clear performance benchmarks.

9.3.2 Additional Methods

 · Fine-Tuning: LLMs can be fine-tuned for specific tasks by training them on smaller, domainspecific datasets. This allows for specialisation without complete retraining.

 · Active Learning: This approach involves selectively querying the LLM to identify areas where it lacks knowledge. The retrieved information is then used to update the model.

9.3.3 Key Considerations

 · Data Quality and Bias: New training data must be carefully curated to ensure quality and mitigate bias. Techniques like human annotation and fairness checks are crucial.

 · Computational Cost: Retraining LLMs can be computationally expensive, requiring significant resources. Optimisations like transfer learning (using pre-trained models as a starting point) can help reduce costs.

 · Downtime: Retraining often takes time, leading to LLM downtime. Strategies like rolling updates or deploying multiple models can minimise service disruptions.

 · Version Control: Tracking different versions of the LLM and their training data is essential for rollbacks in case of performance issues.

9.4 The Future of LLM Updates

Research is ongoing to develop more efficient and effective LLM update strategies. One promising area is continuous learning , where LLMs can continuously learn and adapt from new data streams without retraining from scratch. Continuous learning aims to reduce the need for frequent full-scale retraining by enabling models to update incrementally with new information. This approach can significantly enhance the model's ability to remain current with evolving knowledge and language use, improving its long-term performance and relevance.

Innovations in transfer learning and meta-learning are also contributing",chunk,,1
07f3c83a7132ae24df43eb0fc6755313b2f284bdd8cfa1e5092738d8e21dfec955ca207901638d74bb4fbf0185bd00f1fe8f03832f14e7512a5c5bfa742c2089," to develop more efficient and effective LLM update strategies. One promising area is continuous learning , where LLMs can continuously learn and adapt from new data streams without retraining from scratch. Continuous learning aims to reduce the need for frequent full-scale retraining by enabling models to update incrementally with new information. This approach can significantly enhance the model's ability to remain current with evolving knowledge and language use, improving its long-term performance and relevance.

Innovations in transfer learning and meta-learning are also contributing to advancements in LLM updates. These techniques allow models to leverage pre-existing knowledge and adapt quickly to new tasks or domains with minimal additional training. By integrating these advanced learning methods, future LLMs can become more adaptable and efficient in processing and understanding new information. Furthermore, ongoing improvements in hardware and computational resources will support more frequent and efficient updates. As processing power increases and becomes more accessible, the computational burden of updating large models will decrease, enabling more regular and comprehensive updates.

Collaboration between academia and industry is vital in driving these advancements. By sharing research findings and best practices, the field can collectively move towards more robust and efficient LLM update methodologies, ensuring that models remain accurate, relevant, and valuable over time.

Chapter 10

Industrial Fine-Tuning Platforms and Frameworks for LLMs

The evolution of fine-tuning techniques has been propelled by leading tech companies and platforms that have introduced innovative frameworks and services. Companies like HuggingFace, Amazon Web Services (AWS), Microsoft Azure, and OpenAI have developed tools and platforms that simplify and democratise the fine-tuning process. These advancements have not only lowered the barrier to entry for leveraging state-of-the-art AI models but have also enabled a wide range of applications across various industries, from healthcare and finance to customer service and content creation. Each of these platforms offers unique capabilities that cater to different needs, whether it be through automated fine-tuning workflows, scalable cloud-based training environments, or accessible API interfaces for deploying custom models.

HuggingFace, for example, has made significant strides with its Transformers library 1 and tools like Autotrain 2 and SetFit, which allow users to fine-tune models with minimal coding and data. Their platform provides a robust infrastructure that supports both the research community and industry practitioners, facilitating the rapid development and deployment of custom AI solutions. Similarly, AWS's SageMaker 3 and SetFit 4 provides an extensive suite of services that cover the entire machine learning lifecycle, from data preparation and training to model deployment and optimisation, making it a comprehensive solution for enterprise-level applications.

On the other hand, Microsoft Azure integrates its fine-tuning capabilities with enterprise-grade tools and services, offering solutions like Azure Machine Learning and the Azure OpenAI Service that cater to large organisations looking to incorporate advanced AI into their operations. Azure's focus on MLOps and seamless integration with other Azure services ensures that fine-tuned models can be efficiently deployed and maintained in production environments. Meanwhile, OpenAI has pioneered the concept of 'fine-tuning as a service' allowing businesses to leverage their powerful models like GPT-4 through a user-friendly API 5 , enabling custom model adaptations without the need for in-house AI expertise or infrastructure.

The collective efforts of these tech companies have not only enhanced the efficiency and scalability of fine-tuning but also democratised access to sophisticated AI tools. By reducing the technical barriers and providing comprehensive, user-friendly platforms, these innovations have enabled a wider range of industries to deploy advanced AI models tailored to their specific needs. Tables 10.1 and 10.2 offer a quick comparison of LLM fine-tuning tools and frameworks from different providers.

<missing-text>

<missing-text>

10.1 Autotrain

Autotrain is HuggingFace's innovative platform that automates the fine-tuning of large language models, making it accessible even to those with limited machine learning expertise. The complexity and resource demands of fine-tuning LLMs can be daunting, but Autotrain simplifies the process by handling the most challenging aspects, such as data preparation, model configuration, and hyperparameter optimisation. This automation is particularly valuable for small teams or individual developers who need to deploy custom LLMs quickly and efficiently.

10.1.1 Steps Involved in Fine-Tuning Using Autotrain

Following are the steps involved in fine-tuning LLMs using Autotrain. Figure 10.1 represents the visual workflow.

 · Dataset Upload and Model Selection:

<missing-text>

 -Users begin by uploading their datasets to the Autotrain platform.

 -They then select a pre-trained model from the extensive HuggingFace Model Hub.

· Data Preparation:

 -Autotrain automatically processes the uploaded data, including tasks like tokenization to convert text into a format the LLM can understand.

· Model Configuration:

 -The platform configures the model for fine-tuning, setting up the training environment and necessary parameters.

· Automated Hyperparameter Tuning:

 -Autotrain explores various hyperparameter configurations (such as learning rate, batch size, and sequence length) and selects the best-performing ones.

· Fine-Tuning:

 -The model is fine-tuned on the prepared data with the optimised hyperparameters.

· Deployment:

 -Once fine-tuning is complete, the model is ready for deployment in various NLP applications, such as text generation, completion, and language translation.

10.1.2 Best Practices of Using Autotrain

 · Data Quality: Ensure high-quality, well-labelled data for better model performance.

 · Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize fine-tuning effort.

 · Hyperparameter Optimisation: Leverage Autotrain's automated hyperparameter tuning to achieve optimal performance without manual intervention.

10.1.3 Challenges of Using Autotrain

 · Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.

",chunk,,1
816807a2d7a60f897c845ecc43d24cc9098aa5633ef2723f46c74f458072eb231570eb97a294641ccdf42625ee580a12b57f0fe0a34075669d77875036306e11," Autotrain

 · Data Quality: Ensure high-quality, well-labelled data for better model performance.

 · Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize fine-tuning effort.

 · Hyperparameter Optimisation: Leverage Autotrain's automated hyperparameter tuning to achieve optimal performance without manual intervention.

10.1.3 Challenges of Using Autotrain

 · Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.

 · Resource Constraints: Managing computational resources effectively, especially in environments with limited access to powerful hardware.

 · Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data and using appropriate regularization techniques.

10.1.4 When to Use Autotrain

 1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive machine learning or LLM background who need to fine-tune models quickly and effectively.

 2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is critical, such as proof-of-concept projects or MVPs.

 3. Resource-Constrained Environments: Useful for scenarios with limited computational resources or where a quick turnaround is necessary.

In summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP tasks, especially in environments with limited resources or expertise. However, it may not be suitable for highly specialised applications or those requiring significant customisation and scalability.

10.1.5 Tutorials

 1. How To Create HuggingFace Custom AI Models Using AutoTrain

 2. Finetune models with HuggingFace AutoTrain

10.2 Transformers Library and Trainer API

The Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning large language models (LLMs) such as BERT, GPT-3, and GPT-4. This comprehensive library offers a wide array of pre-trained models tailored for various LLM tasks, making it easier for users to adapt these models to specific needs with minimal effort. Whether you're fine-tuning for tasks like sentiment analysis, text classification, or generating customer support responses, the library simplifies the process by allowing seamless model selection from the HuggingFace Model Hub and straightforward customisation through its high-level APIs.

Central to the fine-tuning process within the Transformers Library is the Trainer API. This API includes the Trainer class, which automates and manages the complexities of fine-tuning LLMs. After completing data preprocessing, the Trainer class streamlines the setup for model training, including data handling, optimisation, and evaluation. Users only need to configure a few parameters, such as learning rate and batch size, and the API takes care of the rest. However, it's crucial to note that running Trainer.train() can be resource-intensive and slow on a CPU. For efficient training, a GPU or TPU is recommended. Platforms like Google Colab provide free access to these resources, making it feasible for users without high-end hardware to fine-tune models effectively.

The Trainer API also supports advanced features like distributed training and mixed precision, which are essential for handling the large-scale computations required by modern LLMs. Distributed training allows the fine-tuning process to be scaled across multiple GPUs or nodes, significantly reducing training time. Mixed precision training, on the other hand, optimises memory usage and computation speed by using lower precision arithmetic without compromising model performance. HuggingFace's dedication to accessibility is evident in the extensive documentation and community support they offer, enabling users of all expertise levels to fine-tune LLMs. This democratisation of advanced NLP technology empowers developers and researchers to deploy sophisticated, fine-tuned models for a wide range of applications, from specialised language understanding to large-scale data processing.

10.2.1 Limitations of the Transformers Library and Trainer API

 · Limited Customisation for Advanced Users: While the Trainer API simplifies many aspects of training, it might not offer the deep customisation that advanced users or researchers might need for novel or highly specialised applications.

 · Learning Curve: Despite the simplified API, there is still a learning curve associated with understanding and effectively using the Transformers Library and Trainer API, particularly for those new to NLP and LLM.

 · Integration Limitations: The seamless integration and ease of use are often tied to the HuggingFace ecosystem, which might not be compatible with all workflows or platforms outside their environment.

In summary, the Transformers Library and Trainer API provide robust, scalable solutions for fine-tuning LLMs across a range of applications, offering ease of use and efficient training capabilities. However, users must be mindful of the resource requirements and potential limitations in customisation and complexity management.

10.3 Optimum: Enhancing LLM Deployment Efficiency

Optimum 6 is HuggingFace's tool designed to optimise the deployment of large language models (LLMs) by enhancing their efficiency across various hardware platforms. As LLMs grow in size and complexity, deploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum addresses these challenges by applying a range of hardware-specific optimisations, such as quantisation, pruning, and model distillation, which reduce the model's size and improve inference speed without significantly affecting accuracy. The following are the key techniques supported by Optimum:

 · Quantisation: Quantisation is one of the key techniques supported by Optimum. This process involves converting the model's weights from high-precision floating-point numbers to lower-precision formats, such as int8 or float16. This reduction in precision decreases the model's memory footprint and computational requirements, enabling faster execution and lower power consumption, especially on edge devices and mobile platforms. Optimum automates the quantisation process, making it accessible to users who may not have expertise in low-level hardware optimisation.

 · Pruning: Pruning is another critical optimisation strategy offered by Optimum",chunk,,1
06626a8bf437d9ff039f9da18900405390479566802f504c817d034e7e24f6d589f621d1cae473075196ebe217a266b0d41f20f26ead73f254580f848a1d3387," process involves converting the model's weights from high-precision floating-point numbers to lower-precision formats, such as int8 or float16. This reduction in precision decreases the model's memory footprint and computational requirements, enabling faster execution and lower power consumption, especially on edge devices and mobile platforms. Optimum automates the quantisation process, making it accessible to users who may not have expertise in low-level hardware optimisation.

 · Pruning: Pruning is another critical optimisation strategy offered by Optimum. It involves identifying and removing less significant weights from the LLM, reducing its overall complexity and size. This leads to faster inference times and lower storage needs, which are particularly beneficial for deploying models in environments with limited computational resources. Optimum's pruning algorithms carefully eliminate these redundant weights while maintaining the model's performance, ensuring that it continues to deliver high-quality results even after optimisation.

 · Model Distillation: In addition to these techniques, Optimum supports model distillation, a process where a smaller, more efficient model is trained to replicate the behaviour of a larger, more complex model. This distilled model retains much of the knowledge and capabilities of the original while being significantly lighter and faster. Optimum provides tools to facilitate the distillation process, allowing users to create compact LLMs that are well-suited for real-time applications. By offering a comprehensive suite of optimisation tools, Optimum ensures that HuggingFace's LLMs can be deployed effectively across a wide range of environments, from powerful cloud servers to resource-constrained edge devices.

10.3.1 Best Practices of Using Optimum

 · Understand Hardware Requirements: Assess the target deployment environment (e.g., edge devices, cloud servers) to optimise model configuration accordingly.

 · Iterative Optimisation: Experiment with different optimisation techniques (quantisation levels, pruning thresholds) to find the optimal balance between model size, speed, and accuracy.

 · Validation and Testing: Validate optimised models thoroughly to ensure they meet performance and accuracy requirements across different use cases.

 · Documentation and Support: Refer to HuggingFace's resources for detailed guidance on using Optimum's tools effectively, and leverage community support for troubleshooting and best practices sharing.

 · Continuous Monitoring: Monitor deployed models post-optimisation to detect any performance degradation and adjust optimisation strategies as needed to maintain optimal performance over time.

10.3.2 Tutorials

1. An Introduction to Using Transformers and Hugging Face

10.4 Amazon SageMaker JumpStart

Amazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed to simplify and expedite the fine-tuning of large language models (LLMs). It provides users with a rich library of prebuilt models and solutions that can be quickly customised for various use cases. This tool is particularly valuable for organisations looking to deploy NLP solutions efficiently without deep expertise in machine learning or the extensive computational resources typically required for training LLMs from scratch. The architecture depicted in Figure 10.2 outlines a comprehensive pipeline for the fine-tuning and deployment of large language models (LLMs) Utilising AWS services.

10.4.1 Steps Involved in Using JumpStart

· Data Preparation and Preprocessing:

 - Data Storage: Begin by securely storing raw datasets in Amazon S3, AWS's scalable object storage service.

 - Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient data preprocessing. This step refines and prepares the raw data for subsequent model training and evaluation.

 - Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing, ensuring accessibility and readiness for the next stages.

· Model Fine-Tuning with SageMaker JumpStart:

 - Model Selection: Choose from a variety of pre-built models and solutions available through SageMaker JumpStart's extensive library, tailored for tasks such as sentiment analysis, text generation, or customer support automation.

 - Fine-Tuning Execution: Utilise Amazon SageMaker's capabilities, integrated with SageMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and configurations to optimise the model's performance for specific use cases.

 - Workflow Simplification: Leverage pre-built algorithms and model templates provided by SageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort required for deployment.

 · Model Deployment and Hosting:

<missing-text>

 - Deployment Setup: Deploy the fine-tuned model using Amazon SageMaker's endpoint deployment capabilities. This setup ensures that the model is hosted in a scalable environment capable of handling real-time predictions efficiently.

 - Scalability: Benefit from AWS's infrastructure scalability, allowing seamless scaling of resources to accommodate varying workloads and operational demands.

 - Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker endpoints, enabling efficient integration into production applications for real-time inference tasks.

10.4.2 Best Practices for Using JumpStart

 · Robust Data Management: Maintain secure and organised data storage practices in Amazon S3, facilitating efficient data access and management throughout the pipeline.

 · Cost-Effective Processing: Utilise serverless computing frameworks like EMR Serverless with Apache Spark for cost-effective and scalable data preprocessing.

 · Optimised Fine-Tuning: Capitalise on SageMaker JumpStart's pre-built models and algorithms to expedite and optimise the fine-tuning process, ensuring optimal model performance without

extensive manual configuration.

 · Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms postdeployment to track model performance metrics. This allows for timely optimisations and adjustments to maintain accuracy and efficiency over time.

 · Integration with AWS Services: Leverage AWS's comprehensive suite of services and integration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of large-scale language models across diverse operational environments.

10.4.3 Limitations of Using JumpStart

 · Limited Customisation: While JumpStart simplifies the process for common use cases, it may offer limited flexibility for highly specialised or complex applications that require significant customisation beyond the provided templates and workflows.

",chunk,,1
42839dbf6d2edb9383f24a1ec33ac1831b085346254beb4cf62638e17d3f65cd787c6bbd9a6755a64906decb566404f388ed05a8f3401a94c233f37f549d27c6," adjustments to maintain accuracy and efficiency over time.

 · Integration with AWS Services: Leverage AWS's comprehensive suite of services and integration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of large-scale language models across diverse operational environments.

10.4.3 Limitations of Using JumpStart

 · Limited Customisation: While JumpStart simplifies the process for common use cases, it may offer limited flexibility for highly specialised or complex applications that require significant customisation beyond the provided templates and workflows.

 · Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services, which may pose challenges for users who prefer or need to operate in multi-cloud environments or those with existing infrastructure outside of AWS.

 · Resource Costs: Utilising SageMaker's scalable resources for fine-tuning LLMs, especially large models, can incur substantial costs, which might be a barrier for smaller organisations or those with limited budgets.

10.4.4 Tutorials

 1. Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart

 2. LLM Agents Using AWS SageMaker JumpStart Foundation Models

10.5 Amazon Bedrock

Amazon Bedrock 7 is a fully managed service designed to simplify access to high-performing foundation models (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. It provides a unified API that integrates these models and offers extensive capabilities for developing secure, private, and responsible generative AI applications. With Amazon Bedrock, users can effortlessly experiment with and assess leading FMs tailored to their specific needs. The service supports private customisation of models through fine-tuning and Retrieval Augmented Generation (RAG), enabling the creation of intelligent agents that leverage enterprise data and systems. Amazon Bedrock's serverless architecture allows for quick deployment, seamless integration, and secure customisation of FMs without the burden of infrastructure management, Utilising AWS tools to deploy these models into applications efficiently and securely.

10.5.1 Steps Involved in Using Amazon Bedrock

Amazon Bedrock offers a streamlined workflow for deploying and fine-tuning LLMs, making it an ideal choice for businesses looking to quickly integrate advanced AI capabilities into their operations. Here's a high-level overview of how Bedrock operates:

 · Model Selection: Users start by choosing from a curated selection of foundation models available through Bedrock. These include models from AWS (like Amazon Titan) and third-party providers (such as Anthropic Claude and Stability AI).

· Fine-Tuning:

 -Once a model is selected, users can fine-tune it to better fit their specific needs. This involves feeding the model with domain-specific data or task-specific instructions to tailor its outputs.

 -The fine-tuning process is handled via simple API calls, eliminating the need for extensive setup or detailed configuration. Users provide their custom data, and Bedrock manages the training process in the background.

· Deployment:

 -After fine-tuning, Bedrock takes care of deploying the model in a scalable and efficient manner. This means that users can quickly integrate the fine-tuned model into their applications or services.

 -Bedrock ensures that the model scales according to demand and handles performance optimisation, providing a seamless user experience.

· Integration and Monitoring:

 -Bedrock integrates smoothly with other AWS services, allowing users to embed AI capabilities directly into their existing AWS ecosystem.

 -Users can monitor and manage the performance of their deployed models through AWS's comprehensive monitoring tools, ensuring that the models continue to perform optimally.

10.5.2 Limitations of Using Amazon Bedrock

While Amazon Bedrock offers a robust suite of tools and services for addressing certain AI challenges, it is not a comprehensive solution for all AI needs. One key limitation is that it does not eliminate the requirement for human expertise. Organisations still need skilled professionals who understand the intricacies of AI technology to effectively develop, fine-tune, and optimise the models provided by Bedrock.

Additionally, Amazon Bedrock is not designed to function as a standalone service. It relies on integration with other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless computing, and AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon Bedrock will also need to use these complementary AWS services to fully realise its potential. This interconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS ecosystem, it may present a steep learning curve and require significant infrastructure management for those new to AWS.

10.5.3 Tutorials

 1. Finetuning LLMs on Amazon Bedrock

 2. Amazon Bedrock for Generative AI

10.6 OpenAI's Fine-Tuning API

OpenAI's Fine-Tuning API is a comprehensive platform that facilitates the customisation of OpenAI's pre-trained LLMs to cater to specific tasks and domains. This service is designed to be user-friendly, enabling a broad range of users, from businesses to individual developers, to harness the power of advanced AI without the complexities typically associated with model training and deployment.

10.6.1 Steps Involved in Using OpenAI's Fine-Tuning API

· Model Selection:

 - Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAI's extensive lineup. This includes powerful models like GPT-4, which offer a robust starting point for a wide range of language processing tasks.

 - Customisable Base: These models come pre-trained with vast amounts of data, providing a solid foundation that can be further refined to suit specific requirements.

 · Data Preparation and Upload:

 - Curating Relevant Data: Users need to gather and prepare a dataset that reflects the specific task or domain they wish to fine-tune the model for. This data is crucial for teaching the model to perform the desired function more effectively.

 - Uploading Data to the API: The Fine-Tuning API facilitates easy data",chunk,,1
14ce6b242621df447d4867e6f1c65ccc4101427b13b44013094d219014206726a1e577b3ef7515021015540a1cef7d7ecbb414db7a21a250ecf4f2d34ce1bc84,".

 - Customisable Base: These models come pre-trained with vast amounts of data, providing a solid foundation that can be further refined to suit specific requirements.

 · Data Preparation and Upload:

 - Curating Relevant Data: Users need to gather and prepare a dataset that reflects the specific task or domain they wish to fine-tune the model for. This data is crucial for teaching the model to perform the desired function more effectively.

 - Uploading Data to the API: The Fine-Tuning API facilitates easy data upload. Users can feed their curated datasets into the API through straightforward commands, making the process accessible even to those with limited technical backgrounds.

· Initiating Fine-Tuning:

 - Automated Process: Once the data is uploaded, OpenAI's infrastructure handles the finetuning process. The API adjusts the model's parameters based on the new data to improve performance on the specified tasks.

· Deploying the Fine-Tuned Model:

 - API Integration: The fine-tuned model can be accessed and deployed via OpenAI's API. This allows for seamless integration into various applications, such as chatbots, automated content creation tools, or specialised customer service systems.

10.6.2 Limitations of OpenAI's Fine-Tuning API

 · Pricing Models: Fine-tuning and using OpenAI's models through the API can be costly, especially for large-scale deployments or continuous usage. This can be a significant consideration for smaller organisations or budget-constrained projects.

 · Data Privacy and Security: Users must upload their data to OpenAI's servers for the finetuning process. This raises potential concerns about data privacy and the security of sensitive or proprietary information.

 · Dependency on OpenAI Infrastructure: The reliance on OpenAI's infrastructure for model hosting and API access can lead to vendor lock-in, limiting flexibility and control over the deployment environment.

 · Limited Control Over Training Process: The fine-tuning process is largely automated and managed by OpenAI, offering limited visibility and control over the specific adjustments made to the model.

10.6.3 Tutorials

1. Fine-Tuning GPT-3 Using the OpenAI API

10.7 NVIDIA NeMo Customizer

NVIDIA NeMo Customiser 8 is part of the NeMo framework, a suite of tools and models designed by NVIDIA to facilitate the development and fine-tuning of LLM models. The Customiser focuses specifically on making it easier to fine-tune large language models (LLMs) for specialised tasks and domains. Like other fine-tuning tools, NeMo Customiser is geared toward users who want to adapt pre-trained models for specific applications, such as conversational AI, translation, or domain-specific text generation. It delivers enterprise-ready models by offering accurate data curation, extensive customisation options, retrieval-augmented generation (RAG), and improved performance features. The platform supports training and deploying generative AI models across diverse environments, including cloud, data center, and edge locations. It provides a comprehensive package with support, security, and reliable APIs as part of the NVIDIA AI Enterprise.

10.7.1 Key Features of NVIDIA NeMo

NVIDIA NeMo is designed to enhance AI projects with several standout features.[86]

 · State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like NeMo Curator for preparing large-scale, high-quality datasets. These tools facilitate efficient pretraining of generative AI models by leveraging thousands of compute cores, which significantly reduces training time and enhances the accuracy of large language models (LLMs).

 · Advanced Customisation for LLMs The NeMo Customiser microservice allows for precise finetuning and alignment of LLMs for specific domains. It uses model parallelism to speed up training and supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.

 · Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server to streamline AI inference at scale. This integration accelerates generative AI inference, ensuring confident deployment of AI applications both on-premises and in the cloud.

 · User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that simplifies the development of conversational AI models. It supports comprehensive workflows from data processing to deployment and includes pre-trained models for automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech (TTS), which can be fine-tuned or used as-is.

 · Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained models and training scripts, facilitating rapid application development or fine-tuning for specific tasks. Currently, NeMo supports models like Llama 2, Stable Diffusion, and NVIDIA's Nemotron-3 8B family.

 · Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance, lowlatency information retrieval, enhancing generative AI applications with enterprise-grade retrievalaugmented generation (RAG) capabilities. This feature supports real-time business insights and data Utilisation.

10.7.2 Components of NVIDIA NeMo

 · NeMo Core Provides essential elements like the Neural Module Factory for training and inference, streamlining the development of conversational AI models.

 · NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including pre-trained models and training scripts, making the platform versatile.

 · Neural Modules Serve as the building blocks of NeMo, defining trainable components such as encoders and decoders, which can be connected to create comprehensive models.

 · Application Scripts Simplify the deployment of conversational AI models with ready-to-use scripts, enabling quick training or fine-tuning on specific datasets for various AI applications.

10.7.3 Customising Large Language Models (LLMs)

While general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organisations to achieve successful proof-of-concept projects, transitioning to production presents additional challenges. Figure 10.3 illustrates NVIDIA's",chunk,,1
f3a2cd723aae90eaef967df830379257eb38483336940cc7282dfc5cb61936b4518e1c2b71728bd4011f4a216f34c28a6edfa03acbcb576e80549b05882451f2," which can be connected to create comprehensive models.

 · Application Scripts Simplify the deployment of conversational AI models with ready-to-use scripts, enabling quick training or fine-tuning on specific datasets for various AI applications.

10.7.3 Customising Large Language Models (LLMs)

While general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organisations to achieve successful proof-of-concept projects, transitioning to production presents additional challenges. Figure 10.3 illustrates NVIDIA's detailed LLM customisation lifecycle, offering valuable guidance for organisations that are preparing to deploy customised models in a production environment [87].

1. Model Selection or Development

NVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and supports the integration of other open-source models of any size. Alternatively, users can develop their own models, starting with data curation, which includes selecting, labeling, cleansing, validating, and integrating data. This process, better termed data engineering, involves additional analysis, designing storage, evaluating model training results, and incorporating reinforcement learning with human feedback (RLHF). While building a custom foundation model is often costly, complex, and time-consuming, most enterprises opt to start with a pre-trained model and focus on customisation.

<missing-text>

2. Model Customisation

Model customisation involves optimising performance with task-specific datasets and adjusting model weights. NeMo offers recipes for customisation, and enterprises can choose models already tailored to specific tasks and then fine-tune them with proprietary data.

3. Inference

Inference refers to running models based on user queries. This phase involves considering hardware, architecture, and performance factors that significantly impact usability and cost in production.

4. Guardrails

NVIDIA employs guardrails as intermediary services between models and applications. These services review incoming prompts for policy compliance, execute arbitration or orchestration steps, and ensure model responses adhere to policies. Guardrails help maintain relevance, accuracy, safety, privacy, and security.

5. Applications

NVIDIA's framework presents enterprise applications as LLM-ready, though this is not always the case. Existing applications may be connected to LLMs to enable new features. However, creating assistants for knowledge access or task execution often involves designing new applications specifically for natural language interfaces.

10.7.4 Tutorials

 1. Introduction to NVIDIA NeMo - Tutorial and Example

 2. How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo

Chapter 11

Multimodal LLMs and their Fine-tuning

A multimodal model is a machine learning model that can process information from various modalities, such as images, videos, and text. For instance, Google's multimodal model, Gemini[88], can analyse a photo of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.

The difference between Generative AI and Multimodal AI is that generative AI refers to the use of machine learning models to create new content, such as text, images, music, audio, and videos, typically from a single type of input. Multimodal AI extends these generative capabilities by processing information from multiple modalities, including images, videos, and text. This enables the AI to understand and interpret different sensory modes, allowing users to input various types of data and receive a diverse range of content types in return.

<missing-text>

11.1 Vision Language Model (VLMs)

Vision language models encompass multimodal models capable of learning from both images and text inputs. They belong to the category of generative models that utilise image and text data to produce textual outputs. These models, especially at larger scales, demonstrate strong zero-shot capabilities, exhibit robust generalisation across various tasks, and effectively handle diverse types of visual data such as documents and web pages. Typical applications include conversational interactions involving images, image interpretation based on textual instructions, answering questions related to visual content, understanding documents, generating captions for images, and more. Certain advanced vision language models can also understand spatial attributes within images. They can generate bounding boxes or segmentation masks upon request to identify or isolate specific subjects, localise entities within images, or respond to queries regarding their relative or absolute positions. The landscape of large vision language models is characterised by considerable diversity in training data, image encoding techniques, and consequently, their functional capabilities.

11.1.1 Architecture

Vision-language models adeptly integrate both visual and textual information, leveraging three fundamental components:

 · Image Encoder: This component translates visual data (images) into a format that the model can process.

 · Text Encoder: Similar to the image encoder, this component converts textual data (words and sentences) into a format the model can understand.

 · Fusion Strategy: This component combines the information from both the image and text encoders, merging the two data types into a unified representation.

These elements work collaboratively, with the model's learning process (loss functions) specifically tailored to the architecture and learning strategy employed. Although the concept of vision-language models is not new, their construction has evolved significantly. Early models used manually crafted image descriptions and pre-trained word vectors. Modern models, however, utilise transformers-an advanced neural network architecture-for both image and text encoding. These encoders can learn features either independently or jointly.

A crucial aspect of these models is pre-training. Before being applied to specific tasks, the models are trained on extensive datasets using carefully selected objectives. This pre-training equips them with the foundational knowledge required to excel in various downstream applications. Following is one of the example architectures of VLMs.

11.1.2 Contrastive Learning

Contrastive learning is a technique that focuses on understanding the differences between data points. It computes a similarity score between instances and aims to minimise contrastive loss, making it particularly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation",chunk,,1
4c719ed369938af023ac2508f7ce537433b66a3d710b550f23b33aab90d96ec789839f16718d4f4a8add370b67e087e7f528c927bdf6c8c90158c0b9eae756fe," trained on extensive datasets using carefully selected objectives. This pre-training equips them with the foundational knowledge required to excel in various downstream applications. Following is one of the example architectures of VLMs.

11.1.2 Contrastive Learning

Contrastive learning is a technique that focuses on understanding the differences between data points. It computes a similarity score between instances and aims to minimise contrastive loss, making it particularly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation process to classify unseen data points.

How it works

For instance, to recognise a cat, contrastive learning compares a cat image with a similar cat image and a dog image. The model learns to distinguish between a cat and a dog by identifying features such as facial structure, body size, and fur. By determining which image is closer to the 'anchor' image, the model predicts its class.

CLIP is a model that utilises contrastive learning to compute similarity between text and image embeddings through textual and visual encoders. It follows a three-step process for zero-shot predictions:

 · Pre-training: Trains a text and image encoder to learn image-text pairs.

 · Caption Conversion: Converts training dataset classes into captions.

 · Zero-Shot Prediction: Estimates the best caption for a given input image based on learned similarities.

<missing-text>

11.2 Fine-tuning of multimodal models

For fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and QLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for large language models, with the primary difference being the nature of the input data. In addition to LoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools such as LLM-Adapters and (IA) ³ [91] can be effectively used. LLM-Adapters integrate various adapter modules into the pre-trained model's architecture, enabling parameter-efficient fine-tuning for diverse tasks by updating only the adapter parameters while keeping the base model parameters fixed. (IA) ³ , or Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learning vectors to weight model parameters through activation multiplications, supporting robust few-shot performance and task mixing without manual adjustments. Moreover, dynamic adaptation techniques like DyLoRA[92] allow for the training of low-rank adaptation blocks across different ranks, optimising the learning process by sorting the representations during training. LoRA-FA[93], a variant of LoRA, optimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a random projection while training the other, thereby reducing the number of parameters by half without compromising performance.

The Efficient Attention Skipping (EAS)[94] module introduces a novel parameter and computationefficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and computation costs for downstream tasks. However, MemVP[95] critiques this approach, noting that it still increases the input length of language models. To address this, MemVP integrates visual prompts with the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time and inference latency, ultimately outperforming previous PEFT methods.

11.2.1 Full-parameter Fine-Tuning

Methods such as those introduced by LOMO[96] and MeZO[97] provide alternative solutions by focusing on memory efficiency. LOMO utilises a low-memory optimisation technique derived from Stochastic Gradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser. MeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes to compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint equivalent to inference [89].

11.2.2 Case study of fine-tuning MLLMs for Medical domain

The following section provides a case study on fine-tuning MLLMs for the Visual Question Answering (VQA) task. In this example, we present a PEFT for fine-tuning MLLM specifically designed for MedVQA applications. To ensure accurate performance measurement, human evaluations were conducted, demonstrating that the model achieves an overall accuracy of 81.9% and surpasses the GPT-4v model by a substantial margin of 26% in absolute accuracy on closed-ended questions.

The model consists of three components: the vision encoder, a pre-trained Large Language Model (LLM) for handling multimodal inputs and generating responses, and a single linear layer for projecting embeddings from the visual encoding space to the LLM space, as shown in figure 11.3.

The Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual embeddings, with model weights remaining frozen during the fine-tuning process. The technique from MiniGPT-v2 is utilised, grouping four consecutive tokens into one visual embedding to efficiently reduce resource consumption by concatenating on the embedding dimension.

These grouped visual tokens are then processed through the projection layer, resulting in embeddings (length 4096) in the LLM space. A multimodal prompt template integrates both visual and question information, which is input into the pre-trained LLM, LLaMA2-chat(7B), for answer generation. The low-rank adaptation (LoRA) technique is applied for efficient fine-tuning, keeping the rest of the LLM frozen during downstream fine-tuning. A beam search with a width of 1 is utilised.

<missing-text>

The multimodal prompt includes input images, questions, and a specific token for VQA tasks, following the MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled as ImageFeature , with the corresponding questions serving as text instructions. The special token [VQA] is used",chunk,,1
42bfb4b06b67b4a8eb0efdaae543c723bd857829a065ee81cf0d09f6aa64f8c9d40e6a7051ae3c5f2eb025a297bbf7397b9d0ed18dc362cb171b5882de8ec38b,"-tuning, keeping the rest of the LLM frozen during downstream fine-tuning. A beam search with a width of 1 is utilised.

<missing-text>

The multimodal prompt includes input images, questions, and a specific token for VQA tasks, following the MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled as ImageFeature , with the corresponding questions serving as text instructions. The special token [VQA] is used as the task identifier, forming the complete multimodal instructional template:

[INST]<img><ImageFeature></img>[VQA] Instruction [/INST].

Model Training

Weights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multimodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating only a small portion of the entire model, as detailed below:

 · Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO medical image-caption dataset, which contains medical image-caption pairs of varying lengths. The prompt template used is <Img><ImageHere></Img>[caption] <instruction> , with the instruction prompt randomly selected from a pool of four candidates, such as 'Briefly describe this image.' During training, only the linear projection layer and the LoRA layer in the LLM are fine-tuned, while other parts of the model remain frozen.

 · Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA dataset, VQA-RAD, which contains triplets of images, questions, and answers. Following the instruction template proposed in MiniGPT-v2, the template used is: ' [INST] <img><ImageFeature></img>[VQA] Instruction [/INST] ', where the instruction prompt is: 'Based on the image, respond to this question with a short answer: question,' with question signifying the question corresponding to the given medical image. The motivation for generating short answers is to validate against the existing labelled data in VQA-RAD, where the answers are typically short in both open-ended and closed-ended QA pairs. Similar to the first stage, the vision encoder and the LLM remain frozen while only the linear projection and LoRA layers in the LLM are updated.

11.3 Applications of Multimodal models

 1. Gesture Recognition - These models interpret and recognise human gestures, which is crucial for sign language translation. Multimodal models facilitate inclusive communication by processing gestures and converting them into text or speech.

 2. Video Summarisation - Multimodal models can summarise lengthy videos by extracting key visual and audio elements. This capability streamlines content consumption, enables efficient content browsing, and enhances video content management platforms.

 3. DALL-E is a notable example of multimodal AI that generates images from textual descriptions. This technology expands creative possibilities in content creation and visual storytelling, with applications in art, design, advertising, and more.

 4. Educational Tools - Multimodal models enhance learning experiences by providing interactive educational content that responds to both visual and verbal cues from students. They are integral to adaptive learning platforms that adjust content and difficulty based on student performance and feedback.

 5. Virtual Assistants - Multimodal models power virtual assistants by understanding and responding to voice commands while processing visual data for comprehensive user interaction. They are essential for smart home automation, voice-controlled devices, and digital personal assistants.

11.4 Audio or Speech LLMs Or Large Audio Models

Audio or speech LLMs are models designed to understand and generate human language based on audio inputs. They have applications in speech recognition, text-to-speech conversion, and natural language understanding tasks. These models are typically pre-trained on large datasets to learn generic language patterns, which are then fine-tuned on specific tasks or domains to enhance performance.

Audio and Speech Large Language Models (LLMs) represent a significant advancement in the integration of language processing with audio signals. These models leverage a robust Large Language Model as a foundational backbone, which is enhanced to handle multimodal data through the inclusion of custom audio tokens. This transformation allows the models to learn and operate within a shared multimodal space, where both text and audio signals can be effectively processed.

Unlike text, which is inherently discrete, audio signals are continuous and need to be discretized into manageable audio tokens. Techniques like HuBERT[99] and wav2vec[100] are employed for this purpose, converting audio into a tokenized format that the LLM can process alongside text. The model, typically autoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as predicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like transcription or sentiment analysis. This capability to handle and generate audio and text simultaneously allows for a wide range of applications, from audio question answering to speech-based sentiment detection, making Audio and Speech LLMs a versatile tool in multimodal AI. The figure 11.4 illustrates an example of a multimodal Audio LM architecture. In this setup, a prompt provides instructions in both text and audio formats. The audio is tokenized using an audio tokenizer. The multimodal model then combines these text and audio tokens and generates spoken speech through a vocoder (also known as a voice decoder).

<missing-text>

Audio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like Whisper and LLaMA, integrate capabilities for understanding and generating audio data, including speech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation. These models have shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through sophisticated tokenization and fine-tuning techniques.

11.",chunk,,1
3e34d23371fe3cd151a5b973e80faa93669e315dfe83d9aab6c0b779892340f75b352bf7c6085d4de9334cc0414aea4123c866f71aab2563b1baa50279e5fe56,"-text>

Audio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like Whisper and LLaMA, integrate capabilities for understanding and generating audio data, including speech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation. These models have shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through sophisticated tokenization and fine-tuning techniques.

11.4.1 Tokenization and Preprocessing

A key aspect of adapting LLMs for audio is the tokenization of audio data into discrete representations that the model can process. For instance, AudioLM and AudioPaLM utilise a combination of acoustic and semantic tokens. Acoustic tokens capture the high-quality audio synthesis aspect, while semantic tokens help maintain long-term structural coherence in the generated audio. This dual-token approach allows the models to handle both the intricacies of audio waveforms and the semantic content of speech.

11.4.2 Fine-Tuning Techniques

Fine-tuning audio and speech LLMs typically involve several key strategies:

 · Full Parameter Fine-Tuning: This involves updating all the model's parameters during finetuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters to adapt pre-trained text LLMs to various audio tasks, although this can be computationally expensive.

 · Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update only specific layers or modules of the model. This method significantly reduces computational requirements while still allowing effective adaptation. Models like Qwen-Audio leverage LoRA to fine-tune pretrained components for enhanced performance on speech recognition tasks.

 · Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper encoder, freeze certain parts of the model (like the speech encoder) and only fine-tune a linear projector or specific adapters to align the speech and text modalities. This approach simplifies the training process and enhances efficiency[104].

 · Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning, starting with a text-based pre-training phase, followed by fine-tuning on a mixture of tasks that include both text and audio data. This staged approach leverages the strengths of pre-trained text models while adapting them for multimodal tasks.

11.4.3 Fine-Tuning Whisper for Automatic Speech Recognition (ASR)

Whisper 1 is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed to convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels at capturing and transcribing diverse speech patterns across various languages and accents. Unlike traditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and selfsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of speech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice assistants, transcription services, and multilingual speech recognition systems.

Why Fine-Tune Whisper?

Fine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised domains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture the nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper to adapt to particular audio characteristics and terminologies, leading to more accurate and reliable transcriptions. This process is especially beneficial in industries with domain-specific jargon, like medical, legal, or technical fields, where the generic model might struggle with specialised vocabulary.

Steps to Fine-Tune Whisper

 · Data Collection and Preparation: Gather a sizable dataset that matches the target domain or task. Ensure the dataset includes diverse examples with clear transcriptions. Clean and preprocess the audio files and transcripts, ensuring they are in a consistent format and aligned correctly. Tools like FFmpeg 2 can help standardise audio formats and sample rates.

 · Data Augmentation: To improve robustness, augment the dataset with variations such as different noise levels, accents, or speeds. Techniques like adding background noise, altering pitch, or changing the tempo can help the model generalise better to real-world conditions.

 · Preprocessing: Convert the audio files into a format suitable for Whisper, typically into mel spectrograms or another time-frequency representation. This transformation is crucial as Whisper relies on such representations to learn and transcribe speech effectively.

 · Model Configuration: Initialise the Whisper model with pre-trained weights. Configure the model to accommodate the target language or domain-specific adjustments. This includes setting appropriate hyperparameters, like learning rate and batch size, tailored to the dataset's size and complexity.

 · Training: Fine-tune the Whisper model on the prepared dataset using a framework like PyTorch or TensorFlow. Ensure to monitor the model's performance on a validation set to avoid overfitting. Techniques like gradient clipping, learning rate scheduling, and early stopping can help maintain training stability and efficiency.

 · Evaluation and Testing: After training, evaluate the model's performance on a separate test set to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character Error Rate (CER) provide insights into how well the model transcribes audio compared to ground truth transcriptions.

11.4.4 Case Studies and Applications

 1. Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant improvements in transcribing doctor-patient interactions. Models like Whisper have been fine-tuned on medical terminologies, resulting in more accurate and reliable transcriptions.

 2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe court proceedings and legal discussions. Domain-specific fine-tuning has enhanced the models' ability to recognise and accurately transcribe legal jargon.

 3. Customer Service Automation: Companies are using fine-tuned speech models to automate customer service interactions. These models are trained on customer support data",chunk,,1
fcd52633400f60bbfb1d46151f08400d2dd7422d92e514e060c0591c285c2330e224a65d3a30610bea30850f76e0f7e1c05f11e7c71ed1e0c7ed4db1d80a5268,". Models like Whisper have been fine-tuned on medical terminologies, resulting in more accurate and reliable transcriptions.

 2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe court proceedings and legal discussions. Domain-specific fine-tuning has enhanced the models' ability to recognise and accurately transcribe legal jargon.

 3. Customer Service Automation: Companies are using fine-tuned speech models to automate customer service interactions. These models are trained on customer support data to understand and respond to queries more effectively, providing a more seamless user experience.

Chapter 12

Open Challenges and Research Directions

12.1 Scalability Issues

The fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM 1 , and T5 2 has become a critical area of research, presenting several significant challenges and opening up new avenues for exploration, particularly in scaling these processes efficiently. This discussion focuses on the two main aspects: the challenges in scaling fine-tuning processes and potential research directions for scalable solutions.

12.1.1 Challenges in Scaling Fine-Tuning Processes

 1. Computational Resources: Large-scale models such as GPT-3 and PaLM require enormous computational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model like GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data and complex operations. The sheer volume of parameters translates to extensive computational demands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can be computationally intensive to fine-tune.

 2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each parameter in the model requires storage, and during training, additional memory is needed to store intermediate computations, gradients, and optimiser states. For example, loading a 7 billion parameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB of GPU memory, while fine-tuning demands around 112 GB of GPU memory[105]. This memory demand is beyond the capability of most consumer-grade hardware, making fine-tuning accessible primarily to well-funded organisations or research institutions.

 3. Data Volume: LLMs typically require vast amounts of training data to achieve state-of-the-art performance during fine-tuning. This data needs to be loaded, preprocessed, and fed into the model at high speeds to maintain efficient training. Managing large datasets can become a bottleneck, especially if the data is stored in a distributed fashion across multiple systems or if it needs to be fetched from remote storage.

 4. Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs fully utilised. However, data pipelines can become bottlenecks if not properly optimised. For example, shuffling large datasets or loading them into memory quickly enough to keep up with the training process can be challenging. Techniques like data packing, where multiple small examples are combined into larger batches, help improve throughput but add complexity to data handling routines.[106]

 5. Efficient Use of Resources: The financial and environmental costs of fine-tuning large models are significant. Large-scale fine-tuning involves not just the direct cost of computational resources but also the indirect costs associated with energy consumption and infrastructure maintenance.

Techniques such as mixed-precision training and gradient checkpointing can reduce these costs by optimising memory and computational efficiency.

The challenges in scaling the fine-tuning processes of LLMs are multifaceted and complex, involving significant computational, memory, and data handling constraints. Innovations in PEFT, data throughput optimisation, and resource-efficient training methods are critical for overcoming these challenges. As LLMs continue to grow in size and capability, addressing these challenges will be essential for making advanced AI accessible and practical for a wider range of applications.

12.1.2 Research Directions for Scalable Solutions

Advanced PEFT Techniques and Sparse Fine-Tuning

Recent advancements in PEFT techniques, like LoRA and its variant, Quantised LoRA, are revolutionising the scalability of LLMs. LoRA reduces the computational burden by updating only a low-rank approximation of the parameters, significantly lowering memory and processing requirements. Quantised LoRA further optimises resource usage by applying quantisation to these low-rank matrices, maintaining high model performance while minimising the need for extensive hardware. This has enabled efficient fine-tuning of massive models, such as in Meta's LLaMA project, where adapting a smaller set of influential parameters allowed the models to perform robustly across various tasks with less computational strain.

Sparse fine-tuning techniques, such as SpIEL [107] complement these efforts by selectively updating only the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the parameters, which it tracks with an index. The process includes updating the parameters, removing the least important ones, and adding new ones based on their gradients or estimated momentum using an efficient optimiser.

Data Efficient Fine-Tuning (DEFT)

To address the scalability challenges, recently the concept of DEFT has emerged. This novel approach introduces data pruning as a mechanism to optimise the fine-tuning process by focusing on the most critical data samples.

DEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by selectively pruning the training data to identify the most influential and representative samples. This method leverages few-shot learning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even exceeding performance levels achieved with full datasets [108].

Key Components of DEFT

High Accuracy Through Influence Score: DEFT introduces the concept of an influence score to evaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence score estimates how removing a specific sample would impact the overall performance of the model. This approach allows for the",chunk,,1
7b4e7c6cdeb25e0b3122ae6102b1614a7fa26e2774f47c5af2b63f9dc730d8943d2613840ad154927874d9f0f801fc7c1a53bbc2906b390534de2ba24ac577d1,". This method leverages few-shot learning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even exceeding performance levels achieved with full datasets [108].

Key Components of DEFT

High Accuracy Through Influence Score: DEFT introduces the concept of an influence score to evaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence score estimates how removing a specific sample would impact the overall performance of the model. This approach allows for the selection of a small subset of data that is highly representative and influential, thereby enabling the model to maintain high accuracy with significantly fewer samples.

High Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity of evaluating large datasets, DEFT employs a surrogate model-a smaller, computationally less intensive model-to approximate the influence scores. This surrogate model helps estimate the impact of each sample without the heavy computational burden associated with directly using the LLM. Additionally, DEFT introduces an effort score to identify and prioritise more challenging samples that may require special attention from the LLM. This dual-score system ensures that the fine-tuning process remains both efficient and effective.

Practical Implications and Use Cases

 · Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial for applications where models need to quickly adapt to new data with minimal samples. In scenarios such as

personalised recommendations or adapting to sudden changes in user behaviour, DEFT allows for rapid fine-tuning, maintaining high performance with a fraction of the data typically required.

 · Reducing Computational Costs in Large-Scale Deployments: By focusing on the most influential data samples and using surrogate models, DEFT significantly reduces the computational resources needed for fine-tuning. This makes it feasible to maintain high-performing LLMs even in large-scale deployments where data volumes are substantial.

Future Directions

The DEFT introduces a data pruning task for fine-tuning large language models (LLMs), setting the stage for new research into efficient LLM-based recommendation systems and presenting numerous opportunities for future exploration. Key areas for further investigation include:

 · Applying the proposed DEALRec[109] approach to a broader range of LLM-based recommender models across diverse cross-domain datasets, thereby enhancing fine-tuning performance within resource constraints.

 · Addressing the limited context window of LLMs by selectively focusing on the most informative items in user interaction sequences for fine-tuning purposes.

12.1.3 Hardware and Algorithm Co-Design

Co-designing hardware and algorithms tailored for LLMs can lead to significant improvements in the efficiency of fine-tuning processes. Custom hardware accelerators optimised for specific tasks or types of computation can drastically reduce the energy and time required for model training and fine-tuning.

 · Custom Accelerators: Developing hardware accelerators specifically for the sparse and lowprecision computations often used in LLM fine-tuning can enhance performance. These accelerators are designed to efficiently handle the unique requirements of LLMs, such as the high memory bandwidth and extensive matrix multiplications involved in transformer architectures.

 · Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation techniques, such as those that minimise data movement or leverage hardware-specific features (e.g., tensor cores for mixed-precision calculations), can further enhance the efficiency of fine-tuning processes.

 · Example: NVIDIA's TensorRT 3 is an example of hardware and algorithm co-design in action. It optimises deep learning models for inference by leveraging NVIDIA GPUs' capabilities, significantly speeding up the process while reducing the resource requirements. TensorRT's optimisations include support for mixed-precision and sparse tensor operations, making it highly suitable for finetuning large models.

As the scale of language models continues to grow, addressing the challenges of fine-tuning them efficiently becomes increasingly critical. Innovations in PEFT, sparse fine-tuning, data handling, and the integration of advanced hardware and algorithmic solutions present promising directions for future research. These scalable solutions are essential not only to make the deployment of LLMs feasible for a broader range of applications but also to push the boundaries of what these models can achieve.

12.2 Ethical Considerations in Fine-Tuning LLMs

12.2.1 Bias and Fairness

When fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets. However, these datasets may inherently carry biases that get transferred to the model during the finetuning process. Biases can arise from various sources, including historical data, imbalanced training samples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset primarily sourced from English-speaking countries might underperform or make biased predictions when

applied to text from other linguistic or cultural backgrounds. Google AI's Fairness Indicators tool 4 is a practical solution that allows developers to evaluate the fairness of their models by analysing performance metrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to monitor and address bias in real-time.

Addressing Bias and Fairness

 · Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse and representative of all user demographics can help mitigate bias.

 · Fairness Constraints: Incorporating fairness constraints, as suggested by the FairBERTa framework 5 , ensures that fine-tuned models maintain equitable performance across different groups.

 · Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing conditions might initially be trained on data from predominantly white patients. Such a model could produce less accurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning techniques, healthcare providers can develop models that perform more equitably across diverse patient populations.

12.2.2 Privacy Concerns

Fine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If not properly managed, fine-tuned models can inadvertently leak private information from their training data. This",chunk,,1
7f2cdfbadc7361def7f78cb21023b35e9292f699c3d61b4ec96db124e27cc04c0c42277c40e35f818f7e0fe4ea58e473ac3a99a72a575362b0f8c7513e8fbf7f," in diagnosing conditions might initially be trained on data from predominantly white patients. Such a model could produce less accurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning techniques, healthcare providers can develop models that perform more equitably across diverse patient populations.

12.2.2 Privacy Concerns

Fine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If not properly managed, fine-tuned models can inadvertently leak private information from their training data. This issue is especially critical in domains like healthcare or finance, where data confidentiality is paramount.

Ensuring Privacy During Fine-Tuning

 · Differential Privacy 6 : Implementing differential privacy techniques during fine-tuning can prevent models from leaking sensitive information.

 · Federated Learning 7 : Utilising federated learning frameworks allows models to be fine-tuned across decentralised data sources, which enhances privacy by keeping data localised.

 · Example Application: In customer service applications, companies might fine-tune LLMs using customer interaction data. Employing differential privacy ensures that the model learns from these interactions without memorising and potentially leaking personal information, thus maintaining customer confidentiality.

12.2.3 Security Risks

 · Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible to security vulnerabilities, particularly from adversarial attacks. These attacks involve inputs designed to exploit model weaknesses, causing them to produce erroneous or harmful outputs. Such vulnerabilities can be more pronounced in fine-tuned models due to their specialised training data, which may not cover all possible input scenarios.

 · Recent Research and Industry Practices: Microsoft's Adversarial ML Threat Matrix provides a comprehensive framework for identifying and mitigating adversarial threats during model development and fine-tuning. This matrix helps developers understand the potential attack vectors and implement defensive strategies accordingly.

· Enhancing Security in Fine-Tuning:

 - Adversarial Training: Exposing models to adversarial examples during fine-tuning can enhance their robustness against attacks.

 - Security Audits: Regularly conducting security audits on fine-tuned models can help identify and address potential vulnerabilities.

12.3 Accountability and Transparency

12.3.1 The Need for Accountability and Transparency

Fine-tuning can significantly alter an LLM's behaviour, making it crucial to document and understand the changes and their impacts. This transparency is essential for stakeholders to trust the model's outputs and for developers to be accountable for its performance and ethical implications.

12.3.2 Recent Research and Industry Practices

Meta's Responsible AI framework 8 underscores the importance of documenting the fine-tuning process and its effects on model behaviour. This includes maintaining detailed records of the data used, the changes made during fine-tuning, and the evaluation metrics applied.

12.3.3 Promoting Accountability and Transparency

 · Comprehensive Documentation: Creating detailed documentation of the fine-tuning process and its impact on model performance and behaviour.

 · Transparent Reporting: Utilising frameworks like Model Cards 9 to report on the ethical and operational characteristics of fine-tuned models.

 · Example Application: In content moderation systems, LLMs fine-tuned to identify and filter harmful content need clear documentation and reporting. This ensures that platform users and regulators understand how the model operates and can trust its moderation decisions.

12.3.4 Proposed frameworks/techniques for Ethical Fine-Tuning

Frameworks for Mitigating Bias

Bias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. FairBERTa, introduced by Facebook, is an example of such a framework that integrates fairness constraints directly into the model's objective function during fine-tuning. This approach ensures that the model's performance is balanced across different demographic groups.

Organisations can adopt fairness-aware frameworks to develop more equitable AI systems. For instance, social media platforms can use these frameworks to fine-tune models that detect and mitigate hate speech while ensuring fair treatment across various user demographics.

Techniques for Privacy Preservation

Differential privacy and federated learning are key techniques for preserving privacy during fine-tuning. TensorFlow Privacy 10 , developed by Google, provides built-in support for differential privacy, allowing developers to fine-tune models securely without compromising data confidentiality.

LLMs are highly effective but face challenges when applied in sensitive areas where data privacy is crucial. To address this, researchers focus on enhancing Small Language Models (SLMs) tailored to specific domains. Existing methods often use LLMs to generate additional data or transfer knowledge to SLMs, but these approaches struggle due to differences between LLM-generated data and private client data. In response, a new Federated Domain-specific Knowledge Transfer (FDKT)[110] framework is introduced. FDKT leverages LLMs to create synthetic samples that mimic clients' private data distribution using differential privacy. This approach significantly boosts SLMs' performance by approximately 5% while maintaining data privacy with a minimal privacy budget, outperforming traditional methods relying solely on local private data.

In healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data without transferring sensitive information. This approach ensures data privacy while enabling the development of robust, generalisable AI systems.

Frameworks for Enhancing Security

Adversarial training and robust security measures[111] are essential for protecting fine-tuned models against attacks. The adversarial training approach involves training models with adversarial examples to improve their resilience against malicious inputs. Microsoft Azure's adversarial training tools provide practical solutions for integrating these techniques into the fine-tuning process, helping developers create more secure and reliable models.

In cybersecurity, fine-tuned LLMs used for threat detection can benefit from adversarial training to enhance their ability to identify and respond to sophisticated attacks, thereby improving organisational security.

Frameworks for Ensuring Transparency

Transparency and accountability frameworks, such as Model Cards and AI FactSheets 11 , provide structured ways to document and report on the fine-tuning process and the resulting model behaviours. These frameworks promote understanding and trust among stakeholders",chunk,,1
5d6278cbaf2a36439f035ea425862efea07cc38768c641b00fa283a5281d96c369b23818f6ceb6ae43d2cb3996eeab02fc76f8dcbc02e63993735bfc544f9397,"uning process, helping developers create more secure and reliable models.

In cybersecurity, fine-tuned LLMs used for threat detection can benefit from adversarial training to enhance their ability to identify and respond to sophisticated attacks, thereby improving organisational security.

Frameworks for Ensuring Transparency

Transparency and accountability frameworks, such as Model Cards and AI FactSheets 11 , provide structured ways to document and report on the fine-tuning process and the resulting model behaviours. These frameworks promote understanding and trust among stakeholders by clearly outlining the model's capabilities, limitations, and ethical considerations.

In government applications, where AI systems might be used for decision-making or public services, maintaining transparent documentation through frameworks like AI FactSheets ensures that these systems are accountable and their decisions can be audited and trusted by the public.

Fine-tuning LLMs introduces several ethical challenges, including bias, privacy risks, security vulnerabilities, and accountability concerns. Addressing these requires a multifaceted approach that integrates fairness-aware frameworks, privacy-preserving techniques, robust security measures, and transparency and accountability mechanisms. By leveraging recent advancements in these areas, researchers and practitioners can develop and deploy LLMs that are not only powerful but also ethically sound and trustworthy.

12.4 Integration with Emerging Technologies

Integrating LLMs with emerging technologies such as IoT (Internet of Things) and edge computing presents numerous opportunities and challenges, reflecting advancements and insights from recent research and industry developments.

12.4.1 Opportunities

 · Enhanced Decision-Making and Automation: LLMs have the capability to analyse and derive insights from vast amounts of unstructured data generated by IoT devices. This data can range from sensor readings in manufacturing plants to environmental data in smart cities. By processing this data in real-time, LLMs can optimise decision-making processes and automate tasks that traditionally required human intervention. For example:

 - Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing sensor data to predict equipment failures before they occur, thereby reducing downtime and maintenance costs.

 - Smart Cities: LLMs can analyse traffic patterns and environmental data from IoT sensors to optimise city infrastructure and improve urban planning decisions.

 · Personalised User Experiences: Integration with edge computing allows LLMs to process data locally on devices rather than relying solely on cloud-based servers. This enables LLMs to deliver highly personalised services based on real-time data and user preferences, enhancing user experiences across various domains:

 - Healthcare: LLMs can provide personalised healthcare recommendations by analysing data from wearable devices and integrating it with medical records securely stored on edge devices.

 · Improved Natural Language Understanding: IoT data integration enriches LLMs' ability to understand context and respond more intelligently to natural language queries. This can significantly improve user interactions with smart environments:

 - Smart Homes: LLMs integrated with IoT devices can understand and respond to voice commands more accurately, adjusting smart home settings based on real-time sensor data (e.g., adjusting lighting and temperature based on occupancy and environmental conditions).

12.4.2 Challenges

 · Data Complexity and Integration: Integrating data from diverse IoT devices poses challenges related to data quality, interoperability, and scalability. LLMs need to effectively process and interpret this heterogeneous data to derive meaningful insights:

 - Data Integration: Ensuring seamless integration of data streams from different IoT platforms and devices without compromising data integrity or performance.

 - Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency and reliability before feeding it into LLMs for analysis.

 · Privacy and Security: Edge computing involves processing sensitive data locally on devices, raising concerns about data privacy and security:

 - Data Privacy: Implementing robust encryption techniques and access control mechanisms to protect sensitive data processed by LLMs on edge devices.

 - Secure Communication: Ensuring secure communication channels between IoT devices and LLMs to prevent data breaches or unauthorised access.

 · Real-Time Processing and Reliability: LLMs deployed in edge computing environments must operate with low latency and high reliability to support real-time applications:

 - Latency: Optimising algorithms and processing capabilities of LLMs to handle real-time data streams efficiently without delays.

 - Reliability: Ensuring the accuracy and consistency of insights generated by LLMs in dynamic and unpredictable IoT environments.

12.5 Future Research Areas

 · Federated Learning and Edge Computing: Exploring federated learning techniques where LLMs can be trained collaboratively across edge devices without centralised data aggregation. This approach addresses privacy concerns and reduces communication overhead.

 · Real-Time Decision Support Systems: Developing LLM-based systems capable of real-time decision-making by integrating with edge computing infrastructure. This includes optimising algorithms for low-latency processing and ensuring reliability under dynamic environmental conditions.

 · Ethical and Regulatory Implications: Investigating the ethical implications of integrating LLMs with IoT and edge computing, particularly regarding data ownership, transparency, and fairness. This area requires frameworks for ethical AI deployment and governance.

Glossary

<missing-text>

<missing-text>

 TREC Text REtrieval Conference - A benchmark that evaluates models on various text retrieval tasks, often focusing on information retrieval and document search.

 WMT Workshop on Machine Translation - A dataset and benchmark for evaluating the performance of machine translation systems across different language pairs.

 XNLI Cross-lingual Natural Language Inference - A dataset designed to evaluate a model's ability to understand and infer meaning across multiple languages.

PiQA Physical Interaction Question Answering - A dataset that measures a model's understanding of physical interactions and everyday tasks.

Winogrande A large-scale dataset aimed at evaluating a language model's ability to handle commonsense reasoning, typically through tasks that involve resolving ambiguous pronouns in sentences.

RLHF Reinforcement Learning from Human Feedback - A method where language models are finetuned based on human-provided feedback, often used to guide models towards preferred behaviours or outputs.

RAFT Retrieval-Augmented Fine-Tuning - A method combining retrieval techniques with fine-tuning",chunk,,1
ebbf96ba8e9fbd813f25a5ad28cc07f4cf7c6d47c142e0faf6a5ed39b6613ca6a9bd623e1978085ef714373def165bc64237ae8fa06018340a11237d67625384," measures a model's understanding of physical interactions and everyday tasks.

Winogrande A large-scale dataset aimed at evaluating a language model's ability to handle commonsense reasoning, typically through tasks that involve resolving ambiguous pronouns in sentences.

RLHF Reinforcement Learning from Human Feedback - A method where language models are finetuned based on human-provided feedback, often used to guide models towards preferred behaviours or outputs.

RAFT Retrieval-Augmented Fine-Tuning - A method combining retrieval techniques with fine-tuning to enhance the performance of language models by allowing them to access external information during training or inference.

Bibliography

 [1] N-gram language models. https://web.stanford.edu/ ~ jurafsky/slp3/3.pdf . [Accessed 01-072024].

 [2] Anis Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown, 04 2023.

 [3] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hullermeier. A survey of reinforcement learning from human feedback, 2024.

 [4] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuanyi Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, and Xingxu Xie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology , 15:1 - 45, 2023.

 [5] Ahtsham Zafar, Venkatesh Balavadhani Parthasarathy, Chan Le Van, Saad Shahid, Aafaq Iqbal Khan, and Arsalan Shahid. Building trust in conversational ai: A review and solution architecture using large language models and knowledge graphs. Big Data and Cognitive Computing , 8(6):70, 2024.

 [6] Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Min Yang, and Wenbin Zhang. History, development, and principles of large language models-an introductory survey, 2024.

 [7] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space, 2013.

 [8] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

 [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.

 [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.

 [11] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth'ee Lacroix, Baptiste Rozi'ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.

[12] The art of fine-tuning large language models, explained in depth - linkedin.com. https://www.linkedin.com/pulse/ art-fine-tuning-large-language-models-explained-depth-cherickal-giavc . [Accessed 01-07-2024].

<missing-text>

<missing-text>

<missing-text>

<missing-text>

<missing-text>

<missing-text>",chunk,,1
9241e4bd9bdbdadd0dbb7c30415c3cf8f82547f41b833fafd2c07eb75bfb048097720c9b0967c1c81f630f8680d4411e2029cb6037802659258ce762a4a5de0a,"Chain-of-Retrieval Augmented Generation

Liang Wang † Haonan Chen ‡ Nan Yang † Xiaolong Huang † Zhicheng Dou ‡ Furu Wei † † Microsoft Corporaion ‡ Renmin University of China

https://aka.ms/GeneralAI

Abstract

This paper introduces an approach for training o1-like RAG models that retrieve and reason over relevant information step by step before generating the final answer. Conventional RAG methods usually perform a single retrieval step before the generation process, which limits their effectiveness in addressing complex queries due to imperfect retrieval results. In contrast, our proposed method, CoRAG ( C haino fR etrieval A ugmented G eneration), allows the model to dynamically reformulate the query based on the evolving state. To train CoRAG effectively, we utilize rejection sampling to automatically generate intermediate retrieval chains, thereby augmenting existing RAG datasets that only provide the correct final answer. At test time, we propose various decoding strategies to scale the model's test-time compute by controlling the length and number of sampled retrieval chains. Experimental results across multiple benchmarks validate the efficacy of CoRAG, particularly in multi-hop question answering tasks, where we observe more than 10 points improvement in EM score compared to strong baselines. On the KILT benchmark, CoRAG establishes a new state-of-the-art performance across a diverse range of knowledge-intensive tasks. Furthermore, we offer comprehensive analyses to understand the scaling behavior of CoRAG, laying the groundwork for future research aimed at developing factual and grounded foundation models.

<missing-text>

 (a) Test-time scaling behavior of CoRAG.

 (b) An example of CoRAG in action.

1 Introduction

Retrieval-augmented generation (RAG) [18] is one of the core techniques in enterprise applications, necessitating the integration of large foundation models with proprietary data sources to produce responses that are both grounded and factual. Conventionally, foundation models are trained on large-scale datasets comprising trillions of tokens and remain frozen post-deployment. Nonetheless,

these models frequently struggle to memorize long-tail factual knowledge or may hallucinate false claims, resulting in unreliable responses in real-world scenarios. RAG mitigates this challenge by augmenting the generation process with retrieved information, thereby improving the trustworthiness of model-generated content and facilitating the incorporation of up-to-date information.

Contemporary RAG systems typically employ a sequential pipeline of retrieval and generation, wherein the retrieved information serves as additional input to the generative model. The effectiveness of RAG systems predominantly relies on the quality of the retrieved information. Retrieval models are engineered for efficiency to ensure scalability to large corpora. For instance, dense retrievers [16, 33] commonly utilize a bi-encoder architecture to compress documents and queries into fixed-size vector representations. This architectural choice permits the use of fast approximate nearest neighbor search algorithms but simultaneously constrains the expressive capacity of retrieval models to handle complex queries. Furthermore, in multi-hop reasoning tasks, it is often unclear what information should be retrieved initially; decisions must be made based on the progressively evolving state of the reasoning process.

To break the bottleneck of retrieval quality, we propose a framework that dynamically retrieves relevant information and plans subsequent retrieval steps based on the current state. By adjusting the number of retrieval steps at test time, our model can explore various aspects of the query and experiment with different query rewriting strategies when the retriever does not yield useful information. This paradigm mirrors the human problem solving process, where we iteratively seek information to address complex questions.

Rather than solely relying on the model's in-context learning capability [39] or distillation from proprietary models [1], we advocate for explicitly training language models to retrieve step by step. To this end, we utilize rejection sampling [40, 4] to augment existing RAG datasets with intermediate retrieval chains. Open-source language models are then fine-tuned on these augmented datasets using standard next-token prediction objectives. To examine the scaling behavior of our model, we propose various test-time decoding strategies, including greedy decoding, best-ofN sampling, and tree search. Diverse decoding strategies and hyperparameter configurations can be employed to control test-time token consumption and the frequency of retriever calls.

Our empirical evaluation demonstrates that CoRAG substantially surpasses strong baselines in QA tasks that require multi-hop reasoning, where retrievers frequently struggle to recall all necessary information in a single retrieval step. Across diverse decoding strategies, the Pareto frontier approximately adheres to a log-linear relationship between total token consumption and model performance, although the coefficients differ across datasets.

On the KILT benchmark [25], which encompasses a more diverse array of tasks, new state-of-the-art scores are achieves on the hidden test set for nearly all tasks. Additionally, we uncover that CoRAG exhibits varied scaling behaviors across different task types. For datasets such as NQ [17], where state-of-the-art retrievers already achieve high recall, the benefits of test-time scaling are often marginal. This suggests the potential for dynamically allocating test-time compute based on the complexity of the query and the quality of the retriever. Upon further analysis, we find that CoRAG can effectively decompose complex queries and perform flexible query reformulation to improve the quality of the generated responses. It also shows robustness against retrievers of varying quality. We posit that CoRAG represents a promising avenue for future research in the RAG domain, with the potential to mitigate hallucination in model-generated content.

2 Related Work

Retrieval-Augmented Generation (RAG) integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [18, 19]. By equipping LLMs with the ability to browse the web [24], RAG systems can access real-time data, thereby providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved",chunk,,1
3a23ecc85864576583d6dd223d2195562f7e27428e20a9e1bbd92e098c29f87cdaeaf5e7b8676dcc0d3a22bdcb2dea4c18258d7a42fc80cf93ae80ee2b2470cf," in the RAG domain, with the potential to mitigate hallucination in model-generated content.

2 Related Work

Retrieval-Augmented Generation (RAG) integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [18, 19]. By equipping LLMs with the ability to browse the web [24], RAG systems can access real-time data, thereby providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved information are pivotal for the efficacy of RAG systems. A substantial body of recent research has concentrated on developing better general-purpose text embeddings [16, 33]. Nevertheless, text embeddings frequently face limitations in addressing complex queries due to their reliance on fixed-size vector representations for efficiency purposes.

To mitigate this constraint, contemporary research has extended the conventional paradigm of a single retrieval step followed by generation, advancing to multi-step iterative retrieval and generation [5]. FLARE [12] prompts an LLM to actively determine when and what to retrieve during the generation process. ITER-RETGEN [28] proposes to interleave retrieval-augmented generation with generation-augmented retrieval, demonstrating enhancements in multi-hop QA tasks. Similarly, IRCoT [31] employs a chain-of-thought methodology, which recursively refines the reasoning thought for subsequent retrieval steps. Self-RAG [1] empowers LLMs to adaptively retrieve, generate, and critique through self-reflection, thus improving factual accuracy and citation precision in open-domain QA and long-form generation tasks. Auto-RAG [38] utilizes heuristic rules and exact answer matching to construct intermediate retrieval steps, yet its performance remains significantly below that of state-of-the-art models. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information.

Scaling Test-time Compute Instead of prompting LLMs to directly generate the final answer, Chainof-Thought (CoT) [34] demonstrates that letting the model to think step by step can drastically improve the performance on mathematical reasoning tasks. Tree-of-Thought (ToT) [37] extends the idea of CoT by adopting a tree structure, allowing the model to explore the search space more comprehensively. To further enhance the reasoning capabilities of LLMs, STaR [40] proposes to leverage bootstrapping techniques to generate intermediate states for training. OpenAI o1 [11] conducts large-scale reinforcement learning and exhibits promising test-time scaling behaviors on advanced reasoning datasets, but the technical details are not publicly available. A drawback of these methods is the increased token consumption, which consequently increases the response latency.

In the realm of RAG, test-time compute can be increased by retrieving more documents or performing additional retrieval steps. LongRAG [13] posits that RAG performance can be enhanced by integrating long-context LLMs with more retrieved documents. In contrast, IterDRAG [39] empirically examines the test-time scaling law through few-shot prompting and iterative retrieval for up to 5 Mtokens. A concurrent work Search-o1 [20] combines the open-source QwQ model [35] with active search from Bing, achieving competitive results on knowledge-intensive tasks. Our work extends the study of test-time scaling in RAG to a targeted fine-tuning paradigm under diverse decoding strategies.

3 Methodology

The CoRAG framework is illustrated in Figure 1. In this section, we describe the key components of CoRAG, including retrieval chain generation through rejection sampling, model training with augmented datasets, and strategies for scaling test-time compute.

3.1 Retrieval Chain Generation

Most RAG datasets only come with a query Q and the corresponding final answer A , without providing intermediate retrieval steps. We propose an automated method for generating retrieval chains through rejection sampling. Each sampled chain consists of a sequence of sub-queries Q 1: L = { Q 1 , Q 2 , . . . , Q L } and the corresponding sub-answers A 1: L , where L is a predetermined maximum chain length. The sub-query Q i = LLM( Q <i , A <i , Q ) is generated by sampling an LLM based on the query Q and the preceding sub-queries and sub-answers. To generate the sub-answer A i , we first retrieve the topk most relevant documents D ( i ) 1: k using a text retriever with Q i as the search query, and subsequently prompt an LLM to yield the answer A i = LLM( Q i , D ( i ) 1: k ). This procedure is iterated until the chain reaches the maximum length L or A i matches the correct answer A .

To assess the quality of a retrieval chain, we calculate the log-likelihood of the correct answer log P( A | Q,Q 1: L , A 1: L ) conditioned on the chain information. The retrieval chain with the highest log-likelihood score is selected to augment the original QA-only dataset.

3.2 Training

Each training instance in the augmented dataset is represented as a tuple ( Q,A,Q 1: L , A 1: L ) , accompanied by the corresponding topk retrieved documents for the query Q and each sub-query. We

<missing-text>

fine-tune an LLM on the augmented dataset using the standard next-token prediction objective within a multi-task learning framework.

The model is simultaneously trained on three tasks: next sub-query prediction, sub-answer prediction, and final answer prediction. We employ the same prompt templates as utilized in the retrieval chain generation process, with the exception that we also incorporate the top retrieved documents D 1: k for the original query Q as input for the final answer prediction task.

L sub\_query = -log P( Q i | Q,Q <i , A <i )",chunk,,1
abfaf87b2fdd52560c6a4a3c59178b5415b4aa963399370b4352953c76960f152cdb487476abf755ddd82f22eb2a70e4c6580f4857976fa3cc6541ea3c5738e1," next-token prediction objective within a multi-task learning framework.

The model is simultaneously trained on three tasks: next sub-query prediction, sub-answer prediction, and final answer prediction. We employ the same prompt templates as utilized in the retrieval chain generation process, with the exception that we also incorporate the top retrieved documents D 1: k for the original query Q as input for the final answer prediction task.

L sub\_query = -log P( Q i | Q,Q <i , A <i ) , i ∈ [1 , L ] L sub\_answer = -log P( A i | Q i , D ( i ) 1: k ) , i ∈ [1 , L ] L final\_answer = -log P( A | Q,Q 1: L , A 1: L , D 1: k )

The cross-entropy loss is computed only for the target output tokens. As we reuse the prompt templates for both data generation and model training, a fine-tuned model can be utilized for the next round of rejection sampling in an iterative manner.

3.3 Test-time Scaling

Given a trained CoRAG model, we propose several decoding strategies to control the trade-off between model performance and test-time compute. The test-time compute is measured by the total number of token consumptions, excluding the retrieval costs. Unlike previous approaches that consider only prompt tokens [39] or generated tokens [11], we account for both. To simplify further discussion, the prompt tokens are treated equally as the generated tokens, despite prompt tokens typically being less expensive due to prefix caching and computation parallelism of the prefilling stage.

Greedy Decoding This strategy utilizes greedy decoding to generate L sub-queries and their corresponding sub-answers sequentially. The final answer is generated using the same prompt template as employed during the training phase.

Best-ofN Sampling This method involves sampling N retrieval chains with a temperature 0 . 7 , subsequently selecting the best chain to generate the final answer. As the ground truth answer is not

available at test time, we instead calculate the conditional log-likelihood of 'No relevant information found' as a penalty score for each chain. The retrieval chain with the lowest penalty score is chosen.

Tree Search We implement a breadth-first search (BFS) variant with retrieval chain rollouts. At each step, the current state is expanded by sampling several sub-queries. For each expanded state, we perform multiple rollouts, and then compute the average penalty score of these rollouts. The state with the lowest average penalty score is retained for further expansion.

To control the test-time compute, the maximum length of the retrieval chain L can be adjusted across all decoding strategies. For best-ofN sampling, the number of sampled chains N offers an alternative option to scale the test-time compute. In tree search, the number of rollouts and expansion size are two additional hyperparameters.

4 Experiments

4.1 Setup

Data and Evaluation We evaluate CoRAG utilizing two sets of benchmarks: (1) a collection of multi-hop QA datasets, including 2WikiMultihopQA [7], HotpotQA [36], Bamboogle [26], and MuSiQue [30]; (2) the KILT benchmark [25], which encompasses a broad spectrum of knowledgeintensive tasks. The multi-hop QA datasets serve to evaluate the model's capacity to perform multi-hop reasoning, whereas the KILT benchmark assesses the framework's ability to generalize across more diverse tasks. For each training dataset, we prompt the open-source Llama-3.1-8BInstruct model to perform rejection sampling, unless specified otherwise. We utilize E5-large [32] as the text retriever for intermediate retrieval steps. The retrieval corpus is the English Wikipedia provided by KILT, comprising approximately 36 million passages [23]. The selected retrieval chains are employed to augment the original QA-only datasets for subsequent model training.

Regarding evaluation metrics, we report the exact match (EM) and F1 scores [27] for the multi-hop QA datasets. For the KILT benchmark, we submit the model's predictions to the official evaluation server and report the downstream metrics on the hidden test set . To adhere to the leaderboard submission policy, we report public validation set results when conducting ablation studies on the KILT benchmark.

Model Training We conduct full-parameter fine-tuning on the augmented datasets, initializing from the Llama-3.1-8B-Instruct checkpoint. Two separate models are trained: one for the multi-hop QA datasets and another for the KILT benchmark. The compiled multi-hop QA dataset comprises 125 k training instances, whereas the KILT benchmark includes 660 k instances after sub-sampling. The model is fine-tuned for 1 epoch with a maximum sequence length of 3 k tokens. For the KILT benchmark, we fine-tune an E5-Mistral retriever [33] and a RankLLaMA re-ranker [22] on the respective training set to boost the ranking quality.

Further implementation details are provided in Appendix A.

4.2 Main Results

Multi-hop QA In Table 1, we present a comparative analysis of CoRAG-8B against several models, including few-shot Llama-3.1-8B-Instruct [4], GPT-4o [9], Self-RAG-7B [1], ITER-RETGEN [28], DRAG, IterDRAG [39], and Search-o1-32B [20]. For a fair comparison, we also include a finetuned Llama-8B baseline utilizing the E5-large retriever, which is fine-tuned on the same datasets as CoRAG-8B but without retrieval chain augmentation. CoRAG-8B substantially surpasses all baselines, with the exception of the Bamboogle dataset, despite being based on a weaker LLM compared to Search-o1",chunk,,1
d471dc8504502fbf0acea9ef160d2222d31f12bd78b2c76c7552420916988ff21e91b8118d3fcfafc39aa460f27c3828548a206b523fc582c8f39595293818fe," IterDRAG [39], and Search-o1-32B [20]. For a fair comparison, we also include a finetuned Llama-8B baseline utilizing the E5-large retriever, which is fine-tuned on the same datasets as CoRAG-8B but without retrieval chain augmentation. CoRAG-8B substantially surpasses all baselines, with the exception of the Bamboogle dataset, despite being based on a weaker LLM compared to Search-o1-32B and IterDRAG. Conversely, we recognize that fine-tuning on multi-hop QA datasets creates an advantage for CoRAG-8B, compared to the few-shot setting for DRAG and IterDRAG.

The Bamboogle dataset comprises only 125 instances, resulting in considerable variance in performance across different runs. Certain questions within Bamboogle necessitate access to knowledge more recent than the Wikipedia dump used for retrieval. Systems like Search-o1-32B, which rely on commercial search engines, possess an advantage in this regard.

<missing-text>

<missing-text>

KILT Benchmark We present several strong systems on the KILT benchmark in Table 2, including KILT-RAG [25], SEAL [2], Atlas-11B [10], RA-DIT 65B [21], and FiD with RS [8]. For submission to the KILT leaderboard, we choose the best decoding configuration for each task based on the public validation set. The results of different decoding strategies are detailed in Appendix Table 5. Our CoRAG-8B model achieves a new state-of-the-art performance across all tasks, with the exception of FEVER, where it marginally trails behind a larger model with 11B parameters.

4.3 Scaling Test-Time Compute

In alignment with OpenAI o1 [11], our model allows for scaling test-time compute to potentially achieve better performance without updating model weights. There are multiple ways to control the test-time compute. In Figure 2, we concentrate on two factors: the retrieval chain length L and the number of sampled chains N for best-ofN sampling. Greedy decoding is a special instance of best-ofN sampling with N = 1 and the temperature set to 0 .

<missing-text>

We observe that increasing the retrieval chain length L results in substantial performance improvements when L is small, but the gains diminish as L increases. This observation aligns with the intuition that longer chains can encapsulate more reasoning steps and allows for trial-and-error exploration of various query rewriting strategies. Several examples are provided in Appendix Table 8. Conversely, increasing N for best-ofN sampling yields mixed effects depending on the dataset. For the most challenging dataset, MuSiQue, in terms of EM score, a larger N enhances performance, whereas for the less challenging dataset, 2WikiMultihopQA, a smaller N suffices. We defer the further exploration of tree search to future work, as it is considerably more computationally expensive than greedy decoding and best-ofN sampling.

The Pareto frontier between the EM score and token consumption approximately follows a log-linear trajectory for up to 128 k tokens, although the scaling behavior varies across different datasets. This observation assists practitioners in making informed decisions regarding the allocation of test-time compute based on the quality requirements. It is important to note that we make several simplifications in this scaling study, such as treating the prompt tokens equivalently to the generated tokens and ignoring the retrieval costs. A more rigorous analysis could take these factors into account.

5 Analysis

5.1 Iterative Rejection Sampling

Our framework facilitates self-improvement through iterative training, akin to the iterative rejection sampling employed in LLM post-training [4]. By utilizing the same prompt templates for both data generation and model training, a trained CoRAG model can generate new sets of retrieval chains. However, the results in Table 3 are mixed, showing performance improvements on the 2WikiMultihopQA dataset but slight declines on other datasets. This indicates that instruction-tuned LLMs already possess a strong ability to generate high-quality retrieval chains.

<missing-text>

5.2 Robustness and Generalization

Different Retrievers Wefurther investigate the influence of various text retrievers at test time. Instead of using the E5-large dense retriever, we substitute it with two weaker alternatives in a plug-and-play fashion: E5-base and BM25. Across all datasets, we observe consistent performance gains when investing more test-time compute, although stronger retrievers continue to outperform in terms of absolute performance. Orthogonal efforts to enhance the quality of text retrievers are likely to further boost the performance of CoRAG.

Weak-to-strong Generalization Due to the need of repeated sampling and autoregressive generation, the retrieval chain generation process costs more GPU hours than the model training. To mitigate this cost, one strategy is to employ weaker LLMs for retrieval chain generation and subsequently fine-tune stronger LLMs on the augmented datasets, similar to the weak-to-strong generalization setting [3].

The results in Table 3 demonstrate that utilizing Llama-3B achieves very close performance compared to the 8B model, whereas Llama-1B exhibits a noticeable performance drop. Manual inspection reveals that the 1 Bmodel frequently struggles to follow the given instructions, resulting in sub-optimal retrieval chains. Employing weaker LLMs also lowers the barrier to adopting more computationally expensive tree search strategies during data generation, which show great potential in mathematical reasoning tasks [6].

5.3 Does Chain-of-Retrieval Always Help?

<missing-text>

Multi-hop QA datasets are specifically designed to evaluate complex reasoning capabilities and are expected to benefit from the chain-of-retrieval mechanism. Table 1 presents empirical evidence supporting this assertion. In contrast, for tasks that a single retrieval step is typically sufficient, the advantage tends to be marginal, as demonstrated in Figure ",chunk,,1
cc8f93845808ad63b90f0780f543f7adebfa05922f4049f5c9362300b523d6096041a069c3d69085de79c0af4749b41fa8821cfe092b7e9183112ed7b0f73cc6," computationally expensive tree search strategies during data generation, which show great potential in mathematical reasoning tasks [6].

5.3 Does Chain-of-Retrieval Always Help?

<missing-text>

Multi-hop QA datasets are specifically designed to evaluate complex reasoning capabilities and are expected to benefit from the chain-of-retrieval mechanism. Table 1 presents empirical evidence supporting this assertion. In contrast, for tasks that a single retrieval step is typically sufficient, the advantage tends to be marginal, as demonstrated in Figure 3. Datasets such as NQ [17] and TriviaQA [15] are known for their (mostly) single-hop nature. This phenomenon implies that decoding strategies should be adaptive based on the complexity of the query. Additional results on the full KILT benchmark are listed in Appendix Table 5, where similar observations for other task types also hold.

5.4 Learning to Stop at Test Time

Instead of always performing L retrieval steps, we explore a model variant that learns to stop at test time. After each retrieval step, the model is prompted to predict whether the information gathered thus far suffices to answer the query. Note that this prompt itself also incurs token consumption and additional cost. The decoding space is constrained to two tokens: 'Yes' and 'No' . If the decoded output is ' Yes ', no further sub-queries are generated. By adjusting the logit bias of the ' Yes ' token, we can control the early stopping behavior.

During the training phase, an additional loss term is added for the stop prediction task. The target output is ' Yes ' if the current retrieval chain encompasses the prefix that maximizes the likelihood of the final answer, and ' No ' otherwise. The associated prompt template is in Appendix Section C.

In Figure 4, we illustrate how the performance varies along with the token consumption on the MuSiQue dataset. While early stopping can

<missing-text>

save some amount of token quota, it comes at the cost of performance degradation. The optimal configuration depends on the dataset characteristics and the quality expectations.

6 Conclusion

In this work, we introduce CoRAG, a framework that teaches LLMs to conduct iterative retrieval and reasoning to answer complex queries. The intermediate retrieval chains are automatically generated via rejection sampling, thereby alleviating the need for manual annotation. At test time, we offer multiple decoding strategies to manage the trade-off between performance and compute. Our experiments demonstrate that CoRAG-8B achieves state-of-the-art performance on both multihop QA datasets and the KILT benchmark, surpassing many baselines built with larger LLMs. A comprehensive analysis is conducted to understand its scaling behavior and generalization capability. In the future, we intend to extend CoRAG to more challenging and economically valuable RAG tasks, advancing towards building factual and trustworthy AI systems.

References

 [1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=hSyW5go0v8 .

 [2] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh,

editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper\_files/paper/2022/ hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-Conference.html .

 [3] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Fortyfirst International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=ghNRg2mEgN .

 [4] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint , abs/2407.21783, 2024. URL https://arxiv.org/abs/2407. 21783 .

 [5] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. ArXiv preprint , abs/2312.10997, 2023. URL https://arxiv. org/abs/2312.10997 .

",chunk,,1
72e2375ca7d66aaa6c39ab37430feb258354d7b1d3ee64e7a3eda9e9b4be120a94770ac485c14fdfa206ca9b99054be17371e78f94edbd2a8f5333f2dbb26af0," Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. ArXiv preprint , abs/2312.10997, 2023. URL https://arxiv. org/abs/2312.10997 .

 [6] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. ArXiv preprint , abs/2501.04519, 2025. URL https://arxiv.org/abs/2501.04519 .

 [7] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics , pages 6609-6625, Barcelona, Spain (Online), 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling-main.580 .

 [8] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. Multi-task retrievalaugmented text generation with relevance sampling. ArXiv preprint , abs/2207.03030, 2022. URL https://arxiv.org/abs/2207.03030 .

 [9] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. ArXiv preprint , abs/2410.21276, 2024. URL https://arxiv.org/abs/2410.21276 .

 [10] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research , 24(251): 1-43, 2023.

 [11] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. ArXiv preprint , abs/2412.16720, 2024. URL https://arxiv.org/abs/2412.16720 .

 [12] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7969-7992, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.495. URL https:// aclanthology.org/2023.emnlp-main.495 .

 [13] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented generation with long-context llms. ArXiv preprint , abs/2406.15319, 2024. URL https: //arxiv.org/abs/2406.15319 .

 [14] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient retrieval-augmented generation research. ArXiv preprint , abs/2405.13576, 2024. URL https://arxiv.org/abs/2405.13576 .

 [15] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and MinYen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1601-1611, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology. org/P17-1147 .

 [16] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main",chunk,,1
03171c273e4de49eb1774d9a44fbf107c1a295ccd0c3b00d33cf35ce838ee81ea1b157da34d3c9a553b8983b386901560b269213515b72122c79390d33522ed2," Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550 .

 [17] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452-466, 2019. doi: 10.1162/tacl\_a\_00276. URL https://aclanthology.org/Q19-1026 .

 [18] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual , 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html .

 [19] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: A survey on generative information retrieval. ArXiv preprint , abs/2404.14851, 2024. URL https://arxiv.org/abs/2404.14851 .

 [20] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. ArXiv preprint , abs/2501.05366, 2025. URL https://arxiv.org/abs/2501.05366 .

 [21] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. RA-DIT: retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024. URL https://openreview.net/forum?id=22OTbutug9 .

 [22] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024 , pages 2421-2425. ACM, 2024. doi: 10.1145/3626772.3657951. URL https://doi.org/10.1145/3626772.3657951 .

 [23] Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas Oguz, Veselin Stoyanov, and Gargi Ghosh. Multi-task retrieval for knowledge-intensive tasks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 1098-1111, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.89. URL https: //aclanthology.org/2021.acl-long.89 .

 [24] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. ArXiv preprint , abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332 .

 [25] Fabio Petroni, Aleksandra Piktus, Angela Fan",chunk,,1
e3e99574e793252609faec70e4f62b7b8bb9cba7c779d2d1f27d70c167d00d5293e2526309b16badd8f1775e2b46ba040c19bdb9d19bd3059aa1f43ccd7acbf1,"aji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. ArXiv preprint , abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332 .

 [25] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2523-2544, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https:// aclanthology.org/2021.naacl-main.200 .

 [26] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 5687-5711, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.findings-emnlp.378. URL https://aclanthology.org/2023.findings-emnlp. 378 .

 [27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383-2392, Austin, Texas, 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264 .

 [28] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 9248-9274, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.620. URL https: //aclanthology.org/2023.findings-emnlp.620 .

 [29] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint , abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530 .

 [30] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics , 10:539-554, 2022. doi: 10.1162/tacl\_a\_00475. URL https: //aclanthology.org/2022.tacl-1.31 .

 [31] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 10014-10037, Toronto, Canada, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.557. URL https://aclanthology.org/2023.acl-long.557 .

 [32] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. ArXiv preprint , abs/2212.03533, 2022. URL https://arxiv.org/abs/2212.03533 .

 [33] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. ArXiv preprint , abs/2401.00368, 2024. URL https://arxiv.org/abs",chunk,,1
58552434d5192fb25f449df3583bc8dc7a265b78f4bc06ce53d13ae7bf3cf27816b15fcbf0d5c5f2b68812da3a7d2d7fcfceeacb18e24f34eb0a801fa0239bbc,"-training. ArXiv preprint , abs/2212.03533, 2022. URL https://arxiv.org/abs/2212.03533 .

 [33] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. ArXiv preprint , abs/2401.00368, 2024. URL https://arxiv.org/abs/2401.00368 .

 [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper\_files/paper/2022/ hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html .

 [35] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. ArXiv preprint , abs/2412.15115, 2024. URL https://arxiv.org/abs/2412.15115 .

 [36] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2369-2380, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259 .

 [37] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023. URL http://papers.nips.cc/paper\_files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html .

 [38] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation for large language models. ArXiv preprint , abs/2411.19443, 2024. URL https://arxiv.org/ abs/2411.19443 .

 [39] Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. ArXiv preprint , abs/2410.04343, 2024. URL https://arxiv.org/ abs/2410.04343 .

 [40] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022. URL http://papers.nips.cc/paper\_files/paper/2022/ hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html .

A Implementation Details

Rejection Sampling For each training instance, we sample up to 16 retrieval chains, with the maximum length randomly selected from the interval [1 , 5] . The sampling temperature is set to 0 . 7 for sub-query generation and 0 for sub-answer generation. Chain generation is terminated if the sub-answer matches the correct answer or if the average conditional log-likelihood of the correct answer exceeds -0 . 05 . For each sub-query, we utilize the E5-large retriever 1 to retrieve the top5 most relevant documents from the KILT version of the Wikipedia corpus [23]. This corpus comprises 36 million passages.

<missing-text>

Multi-Hop QA Training Hyperparameters The training set is the union of the 2WikiMultihopQA, HotpotQA, and MuSiQue datasets, comprising a total of 125 k",chunk,,1
e64fad35273df21ea489806ba45031f18d35a4c1fb6c6f8d6b0074c185bec11819c293e4373c26205c8e6a0751a710b7550490be8a7659fff2c7f48350aeb319,"-likelihood of the correct answer exceeds -0 . 05 . For each sub-query, we utilize the E5-large retriever 1 to retrieve the top5 most relevant documents from the KILT version of the Wikipedia corpus [23]. This corpus comprises 36 million passages.

<missing-text>

Multi-Hop QA Training Hyperparameters The training set is the union of the 2WikiMultihopQA, HotpotQA, and MuSiQue datasets, comprising a total of 125 k samples. The Bamboogle dataset, consisting of only 125 questions, is reserved for evaluation only. Additional hyperparameters are detailed in Table 4. To balance the three loss terms in Section 3.2, we set a sample ratio of 0 . 2 for both the sub-query and sub-answer generation tasks; this ratio is also applied to the KILT training.

KILT Training Hyperparameters We utilize the official training set of the KILT benchmark, omitting the ELI5 and WoW datasets due to the lack of reliable evaluation metrics. To balance the task distribution, we only select 100 k samples for large datasets like T-REx and Zero-Shot RE. In accordance with the benchmark's guidelines, we also add 100 k samples from the BLINK dataset for entity linking.

Rather than using off-the-shelf retrievers, we fine-tune an E5-Mistral retriever following Wang et al., and a RankLLaMA re-ranker following Ma et al.. We adhere to the exact training hyperparameters outlined in the original papers, except that the training data is replaced with the KILT training set. For training the RankLLaMA re-ranker, the backbone is initialized with the Llama-3-8B-Base model, as opposed to Llama-2, to enhance performance. Retrieval and re-ranking scores are presented in Table 6.

All training jobs are conducted using 8 A100 GPUs. The multi-hop QA task requires less than 6 hours of training, whereas the KILT training takes approximately 30 hours. When submitting to the KILT leaderboard, we select the optimal decoding strategy for each task based on validation set performance.

Decoding Strategies In the context of best-ofN sampling, the temperature is set to 0 . 7 for sub-query generation. For sub-answer generation and final answer prediction, the temperature is always set to 0 across all decoding strategies. Regarding tree search, we set the expansion size to 4 and the number of rollouts to 2 . Given that tree search incurs a significantly higher token consumption compared to other decoding strategies, we limit the rollouts to a maximum of 2 steps for each expansion. To avoid the model from generating repetitive sub-queries endlessly, any generated sub-query identical to previous ones is discarded.

Evaluation For multi-hop QA tasks, we evaluate the performance using the exact match (EM) and F1 scores [16]. For Self-RAG-7B, we reproduce the results utilizing the FlashRAG [14] toolkit with the official checkpoint released by the authors.

For the KILT benchmark, we employ the official evaluation scripts provided by the organizers. For Open QA tasks, the main evaluation metric is the EM score, while other task types are evaluated using accuracy scores. The KILT benchmark also offers a variant of the evaluation protocol that requires the model not only to generate the correct answer but also to provide the correct supporting evidence. However, our method spreads the evidence documents across the retrieval chain, rendering it challenging to conform to such an evaluation protocol.

B Additional Results

<missing-text>

Different Decoding Strategies on the KILT Benchmark In Table 5, we present the results of various decoding strategies applied to the validation set of the KILT benchmark. Given that most tasks within the KILT benchmark are much easier for strong dense retrievers compared to multi-hop QA, the disparity in performance across different decoding strategies is less pronounced. This observation

<missing-text>

underscores the necessity of developing a system capable of adaptively selecting the optimal decoding strategy to effectively balance the trade-off between performance and test-time compute.

<missing-text>

Scaling Compute for Training Data Generation Within our proposed framework, rather than investing more compute at test time, we can scale the compute for retrieval chain generation during rejection sampling. By increasing the number of sampled chains, we may identify better chains that contribute to higher-quality training data. However, as illustrated in Figure 5, no definitive trend emerges indicating that increasing the number of sampled chains always leads to better performance. Conversely, the training loss consistently decreases as we scale up rejection sampling, suggesting that the training data becomes less noisy and easier to fit. We hypothesize that the majority of sampled chains are already of high quality and that LM fine-tuning exhibits considerable robustness to noisy training data.

<missing-text>

Effects of Sampling Temperature In best-ofN sampling, the sampling temperature controls the diversity and quality trade-off in the generated retrieval chains. A higher temperature results in more diverse chains, albeit with the potential introduction of increased noise. Figure 6 illustrates the lack of a consistent conclusion regarding the impact of sampling temperature on performance. For the MuSiQue and HotpotQA datasets, a lower temperature generally yields superior results, whereas for the 2WikiMultihopQA dataset, a medium temperature leads to the best performance. As a result, we stick to a temperature of 0 . 7 for both rejection sampling and test-time decoding for simplicity.

Case Analysis Table 8 presents several model predictions on the validation set of the HotpotQA dataset. We compare the performance of RAG without chain-of-retrieval against CoRAG. CoRAG effectively decompose the complex multi-hop queries into a sequences of simpler sub-queries and dynamically conducts query reformulation when the retrieved information proves unhelpful. In

the fourth example, the model initially hallucinates some incorrect information but subsequently",chunk,,1
7c1d28a644e921d0256ef0bca4670768335790db8ad5d5a35824f6fe69d71afd941205678e86db056b1d0ae86c8dcd7e9056230b1a687dbb27e1c48322797c90," for both rejection sampling and test-time decoding for simplicity.

Case Analysis Table 8 presents several model predictions on the validation set of the HotpotQA dataset. We compare the performance of RAG without chain-of-retrieval against CoRAG. CoRAG effectively decompose the complex multi-hop queries into a sequences of simpler sub-queries and dynamically conducts query reformulation when the retrieved information proves unhelpful. In

the fourth example, the model initially hallucinates some incorrect information but subsequently self-corrects by verifying the poet's name and country of origin through additional retrieval steps.

C Prompts

<missing-text>

All prompts are listed in this section. Texts highlighted in blue denote placeholders to be filled in. The ' task description ' field is dataset-specific, with corresponding values detailed in Table 7.

Prompt: Sub-query Generation

You are using a search engine to answer the main query by iteratively searching the web. Given the following intermediate queries and answers, generate a new simple follow-up question that can help answer the main query. You may rephrase or decompose the main query when previous answers are not helpful. Ask simple follow-up questions only as the search engine may not understand complex questions.

## Previous intermediate queries and answers {intermediate queries and answers}

## Task description {task description}

## Main query to answer

{query}

Respond with a simple follow-up question that will help answer the main query, do not explain yourself or output anything else.

Prompt: Intermediate Answer Generation

Given the following documents, generate an appropriate answer for the query. DO NOT hallucinate any information, only use the provided documents to generate the answer. Respond 'No relevant information found' if the documents do not contain useful information.

## Documents

{retrieved documents}

## Query

{sub-query}

Respond with a concise answer only, do not explain yourself or output anything else.

Prompt: Final Answer Generation

Given the following intermediate queries and answers, generate a final answer for the main query by combining relevant information. Note that intermediate answers are generated by an LLM and may not always be accurate.

## Documents

{retrieved documents}

## Intermediate queries and answers {intermediate queries and answers}

## Task description

{task description}

## Main query

{query}

Respond with an appropriate answer only, do not explain yourself or output anything else.

Prompt: Learning to Stop

Given the following intermediate queries and answers, judge whether you have enough information to answer the main query. If you believe you have enough information, respond with 'Yes', otherwise respond with 'No'.

## Intermediate queries and answers

{intermediate queries and answers}

## Main query

{query}

Respond with 'Yes' or 'No' only, do not explain yourself or output anything else.

Table 8: Examples from the validation set of the HotpotQA dataset. For conciseness, all retrieved documents at each step are omitted. Correct answers are highlighted in blue, while incorrect answers are highlighted in red.

Query: What wrestling team is Mathew Thomas Rehwoldt a part of?

RAG without Chain-of-Retrieval

Final Answer: WWE

✗

CoRAG (Ours)

Sub-query 1: What is Mathew Thomas Rehwoldt's profession?

Sub-answer 1: No relevant information found.

Sub-query 2: What is Mathew Thomas Rehwoldt's name in the wrestling industry?

Sub-answer 2: Aiden English

Sub-query 3: What wrestling team is Aiden English a part of?

Sub-answer 3: The Vaudevillains

Final Answer: The Vaudevillains

✓

Query: How many months apart are Johan Mjällby and Neil Lennon in age?

RAG without Chain-of-Retrieval

Final Answer: two months

✗

CoRAG (Ours)

Sub-query 1: What is Johan Mjällby's birthdate?

Sub-answer 1: 9 February 1971

Sub-query 2: What is Neil Lennon's birthdate?

Sub-answer 2: 25 June 1971

Sub-query 3: What is the difference in months between 9 February 1971 and 25 June 1971?

Sub-answer 3: 4 months

Final Answer:

4

✓

Query: Thomas Parker Sanborn was a model for the protagonist in the only novel by a poet originally from

what country?

RAG without Chain-of-Retrieval

Final Answer: Italian

✗

CoRAG (Ours)

Sub-query 1: What country is the poet originally from?

Sub-answer 1: Hungary, Romania, Switzerland, China, Ukraine, France, Turkey.

Sub-query 2: What is the name of the poet who wrote the only novel?

Sub-answer 2: Emily Brontë

Sub-query 3: What country is the poet who wrote the only novel originally from?

Sub-answer 3: United States

Sub-query 4: What is the name of the poet who wrote the only novel that Thomas Parker Sanborn was a model

for?

Sub-answer 4: George Santayana

Sub-query 5: What poet wrote the only novel that Thomas Parker Sanborn was a model for?

Sub-answer 5: George Santayana

Sub-query 6: What country is the poet George Santayana originally from?

Sub-answer 6: Spain.

Final Answer:

Spain ✓

Query: Which university is further North, University of New Hampshire or Stony Brook University?

RAG without Chain-of-Retrieval

Final Answer: University of New Hampshire ✗

CoRAG (Ours)

Sub-query 1: What is the location of the University of New Hampshire?

Sub-answer 1: Durham, New Hampshire.

Sub-query 2: What is the location of",chunk,,1
87e8b9a944ef8406dbfa946a3542dfc282a0e593c0808d6fd6ee8dfd16ce55d5cf550b5a118ca34cb418c67861eb9ea20fe46724d93c9cc2310638b6200a012d," Santayana originally from?

Sub-answer 6: Spain.

Final Answer:

Spain ✓

Query: Which university is further North, University of New Hampshire or Stony Brook University?

RAG without Chain-of-Retrieval

Final Answer: University of New Hampshire ✗

CoRAG (Ours)

Sub-query 1: What is the location of the University of New Hampshire?

Sub-answer 1: Durham, New Hampshire.

Sub-query 2: What is the location of Stony Brook University?

Sub-answer 2: Stony Brook, NY.

Sub-query 3: What is the latitude of Durham, New Hampshire?

Sub-answer 3: No relevant information found.

Sub-query 4: What is the latitude of Stony Brook, NY?

Sub-answer 4: No relevant information found.

Final Answer: University of New Hampshire ✗",chunk,,1
a8b765d9d64c173b527b19399d8071b2476bc8aa345827e0da4c7b7ddf7996876fa52fecaefaede23d41d6a43cbcde7efce40b55791eea8db599e6dd21253d73,2408.13296v3 (1).txt,document,,1
f7ed019e71d299d7b11947cc8c588531072e3b2514bc473c4e1e6238ba392412c3a36c967222f35ce63231c7e21eebe88f6f2707129c2ff4f311575dd18046a3,2501.14342v1.txt,document,,1
7578b6d1-fe94-4a24-9fd0-378ac5fea605,Large Language Models and Their Ecosystem,community,,1
33e212e3-7a71-4553-b0c3-83c3773ff157,Retrieval-Augmented Generation Community,community,,1
135e77cd-fa90-4a06-9c2a-e3d15e64445f,Machine Learning Hyperparameters and Optimization Techniques,community,,1
e76f4822-22b5-41e4-9e40-bb4653aa6957,Retrieval-Augmented Generation Community,community,,1
74ef4bbb-d889-48be-8c54-6e4be7711bab,HuggingFace and AI Model Ecosystem,community,,1
355c3e49-59ef-4a32-b55a-3a67ec378589,Large Language Models and Their Ecosystem,community,,1
92b39b2c-8591-4fd6-bac4-21cfb2c7e707,Reinforcement Learning Community: PPO and DPO,community,,1
7e4f62b0-9dda-4e4a-8e1e-8068fa6179fe,Data Preprocessing and Imbalanced Learning Techniques,community,,1
8b22b757-00ed-46b4-9082-c7c7c9cf0c05,DecodingTrust and AI Safety Frameworks,community,,1
0c569f6b-751a-4ac7-a7e4-fe85c0ad2243,Multimodal Large Language Model Community,community,,1
1861fada-52fd-4d88-b8fd-c7d63b452b49,CoRAG and Multi-Hop Question Answering Community,community,,1
b2c6acad-90cb-4cd3-a7e4-08e0b5b093d9,KILT Benchmark and Retrieval-Augmented Generation Community,community,,1
8a32bac6-2634-4f02-be12-371cbd9b2180,FinGPT and Financial Data Ecosystem,community,,1
3a1b72c2-d218-4a72-87d0-987ed727d11a,Natural Questions and Generative Information Retrieval Community,community,,1
efcd7edb-e764-4242-9510-5c33da689dd3,Language Model Optimization Community,community,,1
94602c5f-573d-43af-93eb-8da5b8e4c8ba,Advanced Language Models Community,community,,1
6b9b40ef-993d-43c6-abe4-c51820b94e2e,Hugging Face and Parameter-Efficient Fine-Tuning Community,community,,1
d3cee6ee-2225-42b8-820e-5edb3bd0064c,HuggingFace Fine-Tuning Community,community,,1
9fcbe74d-bda4-418f-a4b7-2938ec7117aa,NVIDIA NeMo and AI Model Development,community,,1
a62b0d2e-5556-4aac-8526-5f804adbf220,Large Language Model Ecosystem,community,,1
6c9c1211-2f9f-436f-9234-19f153a4f320,Large Language Models and Their Ecosystem,community,,1
32432a3c-8507-4581-83c1-3cec0b8c61bd,Adversarial Training and Security Frameworks,community,,1
06795090-5cef-408c-9c28-384add4a608c,Distributed Training and Parallelism Techniques,community,,1
06c0857b-278f-4282-ba55-b79b04dfe920,Throughput and Data Volume in LLM Optimization,community,,1
6c7eb6ba-d714-4675-b4a3-038f73768e5f,Self-RAG Community and Key Researchers,community,,1
88d9ff06-b599-4cd5-9c90-f31647fb4260,Fairness and Privacy in Machine Learning,community,,1
51321a84-91eb-4bbb-94fe-0e6964409a12,GPU and TPU in Training Environments,community,,1
7a9463c2-893e-4f07-9888-ce94fc254731,Language Model Enhancement Community,community,,1
88fc1a72-2be5-4cd3-be75-bf55024deb72,Retrieval-Augmented Generation Community,community,,1
739fe532-6974-440f-b2d9-038ed8124a04,Jiawei Sun and Flashrag Community,community,,1
2fba1013-c461-405d-b66b-3a2f96e5c4a5,Mixture-of-Agents Framework and Its Components,community,,1
41a695f8-37f6-4481-9a53-12994c9441ed,Machine Learning Optimization Techniques,community,,1
b6baef5e-33fd-4e63-93df-7a8e3b80130b,Overfitting and Model Performance Community,community,,1
17971b00-408e-4436-96ca-ffd7b3d7a17f,Hyperparameter Tuning Community,community,,1
c388327c-1afc-42fd-9d53-60447b066a5b,Machine Learning Hyperparameters Community,community,,1
db88c352-ed8d-473c-a5dc-f614112b2da3,Hyperparameters and Model Training Community,community,,1
ba2674b2-3e97-4aeb-a9bd-3c94cb9322bd,Retrieval-Augmented Generation Community,community,,1
35145f10-117e-44f1-9c45-f65a11632a5b,Data Indexing and Vector Database Community,community,,1
608056f0-48f1-41af-b725-8d3d1626e995,Natural Language Processing Research Community,community,,1
42116ed4-2905-4875-ab37-07633c9520b4,HuggingFace and Its Machine Learning Ecosystem,community,,1
546f5325-da19-45e9-a556-9c11795b1ed8,Optimum and Model Optimization Techniques,community,,1
77a6b081-4434-409a-8104-07a46f885d7b,AI Model Deployment Community: OpenAI and Amazon Services,community,,1
99f9b9a8-f57a-43cb-93f2-ae83d970eb92,Deep Learning Frameworks Community: PyTorch and TensorFlow,community,,1
ebc2751e-68af-4044-8863-5e04bc69b82f,Reinforcement Learning Optimization Community,community,,1
ad89b805-51f1-4c8e-b552-9f954e9f2e8c,Reinforcement Learning from Human Feedback and Reward Model,community,,1
fcc6b541-2791-4926-8bd7-3df748a0ae75,DEFT and Fine-Tuning Innovations,community,,1
e63dbfb2-0b7b-4da2-b729-e74b50652736,Large Language Models and Their Ecosystem,community,,1
f3698108-c79d-4513-b389-218588a2818e,Natural Language Processing and DoRA Community,community,,1
0514da31-cc8c-4a78-91be-503321df3f86,Neural Language Models and Word2Vec,community,,1
2166be15-b688-4c1c-9cc1-2a40c346aaa7,Proximal Policy Optimisation and HuggingFace Framework,community,,1
f472aea0-c1fb-4889-913b-cea237413222,Reinforcement Learning and Pruning Techniques,community,,1
a32040ad-32e9-4af2-b1ef-63933f202f90,Evaluation Metrics in Machine Learning,community,,1
07a6e5b6-bd8b-4d0b-b951-8397e3cac55a,DPO and Preference Dataset in Reinforcement Learning,community,,1
31809e64-993b-4672-a26e-19dbf4f36536,Data Processing Techniques Community,community,,1
0c25ea96-4812-4a2a-99bb-05233408b3ac,DecodingTrust and AI Safety Frameworks,community,,1
f703ea7b-3543-4cce-9501-b1b4479fcccb,Fairness and Bias in AI Models,community,,1
6026e32e-b7aa-4aaf-9abd-b53e54c9062c,Llama Guard Content Moderation Community,community,,1
2099cab0-1a68-4552-83da-de19e2efb557,Multimodal Large Language Model Community,community,,1
56f069b8-b429-4df8-881b-f03f9c030a68,Efficient Attention Skipping and MemVP,community,,1
d9b682c4-c51a-4d0f-858f-f7d7603e6809,Multi-Hop Question Answering Community,community,,1
416ca5ba-762a-44bc-95ca-338cde6cde12,Retrieval Chain and Information Retrieval Entities,community,,1
e840b0fe-0902-44cb-b727-ad5bf79cc3e2,HotpotQA Research Community,community,,1
6406c157-e399-4350-b36c-6a7400c397b2,CoRAG and Multi-Hop Question Answering Community,community,,1
3289c8e2-27ab-4555-b053-ba7ad743d621,Retrieval-Augmented Generation Community,community,,1
00d35ce7-01fd-4ae8-907d-895f3b97e1aa,CoRAG-8B and Multi-Hop Question Answering Community,community,,1
c30c61ca-45f1-4df2-aee9-f7aa06dd9ea1,KILT Benchmark Community,community,,1
b649ed11-3c9e-45f4-8dcd-9c9dcdb234e3,KILT Benchmark and Retrieval-Augmented Generation Models,community,,1
7853db66-fa15-444c-a149-616636660712,Multi-Hop QA and Evaluation Metrics,community,,1
680260e7-80aa-49dd-a5a4-20dae22c62d4,Association for Computational Linguistics and Related Conferences,community,,1
60b42121-4968-4994-993d-c6e9c0bf5718,Natural Language Processing Research Community,community,,1
3b04f70a-d9bf-4227-86e8-ad147f986e4d,Multi-Task Retrieval Community,community,,1
d7a81940-d8e5-40ae-87f8-d8cfd94ca6c4,Natural Questions and Generative Information Retrieval Community,community,,1
c1fc0014-c265-4a80-9aa3-5afe06fe3dbb,PharmaGPT and Fine-Tuning Techniques,community,,1
38e89abf-253d-4123-8ffe-761752540990,Memory Optimization Techniques in AI Models,community,,1
530a6464-fc1d-4a6a-9ec8-721ee2c67b9a,Half Fine-Tuning and LLAMA 2-7B Community,community,,1
aaa587b2-64ae-40cb-b29b-9cd5c5112e56,Language Model Optimization Community,community,,1
6a92ddbf-aaba-47c5-8b0c-6cf1c8108108,Audio Processing Models Community,community,,1
4cca9436-8115-4601-b553-d948b810e5b0,GPT-4 and Fine-Tuning Research Community,community,,1
37e7c594-24e9-4994-b7b0-8b96e55fa3f7,Mixtral 8X7B and Related Language Models,community,,1
33a3b6f6-357b-47c9-b2da-068a90691b39,WebLLM and Its Applications,community,,1
57c73b37-ce53-43b5-bf6a-ca5a27414661,QLORA and Model Optimization Community,community,,1
aa327b01-9a52-458b-a25a-5f905da33692,MiniGPT-V2 and Multimodal Applications,community,,1
2e3d3052-bc54-44ae-adf0-f1ec4b7b3a2a,Parameter-Efficient Fine-Tuning Techniques,community,,1
d155cd36-6ce3-4ce9-8283-211817dd9a7b,Hugging Face and Distributed Machine Learning Frameworks,community,,1
afc6ead8-7bc5-4750-9bef-1c3ef281cf52,vLLM and PagedAttention Community,community,,1
95bc4ed9-bd29-4cc6-b062-53cdf66cb9a9,Fine-Tuning Techniques in Machine Learning,community,,1
5ff4ef3d-c9d0-4ee6-a90a-2db80785e391,LAWGPT and Chinese Legal Applications,community,,1
efa0dfb9-ce0a-43f7-b08d-957c284ba92e,Med-PaLM 2 and Medical Datasets Community,community,,1
aa7c26b3-b5f6-4fdf-98ab-e2b4a8cb3215,Multi-Task Adapter and PEFT Library Community,community,,1
fef2f40e-486b-4e49-bfa8-63cf3bf3be37,Synthetic Data and Adversarial Examples,community,,1
805dfe41-2731-4e1e-af5a-b90e46be4e21,Autotrain and Fine-Tuning Community,community,,1
17ea94bf-15d2-47b6-b62d-952ff33a45d3,Pre-Trained Language Models and NLP Techniques,community,,1
71fcee5d-a584-4fa9-8b7a-ae53947795b4,LLAMA 2 and NeMo Collections Community,community,,1
641861c7-1a64-4538-8f92-71b9a204c324,NVIDIA NeMo Customizer and Enterprise-Ready Models,community,,1
9c9f77d7-8f02-437f-84b3-a10fd29d7d6d,"Cloud Computing Platforms: Google Cloud, Microsoft Azure, and Amazon SageMaker",community,,1
15212d33-d364-4556-8ea8-f89fadc83db4,NVIDIA NeMo AI Deployment Community,community,,1
7f65212b-144f-4af4-b4dd-6b65b92a2246,NVIDIA and AI Model Compliance,community,,1
df89f260-5bf1-4d1e-9151-0d27de16de29,NVIDIA NeMo and AI Development Tools,community,,1
6f08ce75-ad05-48fa-9a09-bd7e7f516596,OpenAI and Azure Integration Community,community,,1
a0f493b1-aae3-41d4-8ebd-12cd1a077c67,Retraining Methods and Their Impact on LLMs,community,,1
e26ae3f6-1ddb-4383-a99a-8cb16f0e516e,Continuous Learning and LLM Update Strategies,community,,1
eade0802-8640-485a-8f45-3826f1ceeaf5,Large Language Model Ecosystem,community,,1
ae9d36be-5e34-4ff6-bf8b-f35a94bc16af,LLM Monitoring and Evaluation Community,community,,1
ee164508-0f45-48b0-81c9-ed3f1cd58df0,Retrieval-Augmented Generation Community,community,,1
97c3dd7b-3e17-4002-8913-808b795a37b6,Retrieval-Augmented Generation Community,community,,1
c01185f3-be5b-41c6-9bf6-ea272a3b47f3,Regularization Techniques in Machine Learning,community,,1
1a15615f-4704-4c20-bc13-0c689c801364,Weight Decay and Regularisation Techniques,community,,1
0e1ee30d-510a-4efc-bf9d-f1e6b573a628,Overfitting and Model Performance Community,community,,1
bde3c2a2-d419-4965-8b84-02ee8a8a9fb8,Model Performance and Data Quality in Machine Learning,community,,1
deaf3acb-ac6e-4e39-b6cf-2680acd7090e,Direct Preference Optimisation Community,community,,1
5e448d68-2be3-4d57-a2b7-7d2f82abaa9a,Proximal Policy Optimization and Related Techniques,community,,1
8a8cba05-96cd-4eaa-b4e8-6aad53715bed,Large Language Models and Their Ecosystem,community,,1
32f74ebe-674a-484e-9a6c-103177820212,Mixture of Experts and Mixture of Agents Community,community,,1
b7a075f0-1bcf-444d-9bad-d185e02dce44,Retrieval Chain and Information Querying,community,,1
9aecc2ab-8ee7-454a-86f5-650960af99bf,Supervised Fine-Tuning and Log-Likelihood Community,community,,1
09be0531-3786-4985-83e6-8cf16246de67,CoRAG Development Community,community,,1
5f308afd-f6b8-4778-829d-16844e45f875,Decoding Strategies and Temperature in CoRAG,community,,1
2152a8c2-9d68-4f91-9b36-6a56c12a7f20,Rejection Sampling and Training Loss,community,,1
305f0fd0-a38d-4427-8285-7524a99ffcdd,CoRAG and Decoding Strategies,community,,1
966db57a-b896-4f64-a785-0c1f00b64e4b,Searching and Ranking Algorithms Community,community,,1
4b530151-c4b8-46cb-995c-e11530247409,RAG and Its Enhancements in Information Retrieval,community,,1
ae8dd7a4-bb89-49a3-87e4-721bf9cc9008,Search Models Community: E5 and IterDRAG,community,,1
038dd2a9-7fa8-462e-b936-f4b169819da7,LLAMA 2 and BERT-LARGE AI Models Community,community,,1
b0370068-f350-4bb0-9f49-150aa126ca6c,GPT-3 and LLMXPLORER Community,community,,1
ede1560d-ae43-45ed-8613-405d66c268e6,Multi-Step Generation and Iterative Generation,community,,1
a2ff4490-de46-418a-beae-952702613886,NeMo Collections and Pre-trained Models,community,,1
87254bd2-3a5c-41eb-beeb-f922e0223dfe,Alerting Mechanisms and Performance Monitoring,community,,1
ddb01f68-4a07-4d90-a033-91d5e7c3d4b6,Monitoring Programme for LLM Applications,community,,1
8226a3bd-1142-4093-8a32-9b753b63babb,User Interface and Role-Based Access Control,community,,1
f8de056b-8c55-4a66-963c-5c7da40a04a6,Response Monitoring and Embedding Distance Metrics,community,,1
19730e8c-511e-4d61-b482-b7ae7bdb40eb,Prompt Monitoring and Evaluative LLMs,community,,1
