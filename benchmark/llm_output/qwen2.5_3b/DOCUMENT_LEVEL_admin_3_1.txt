The performance of different large language models (LLMs) like CORA-G and Atlas-11B can vary significantly across different datasets, tasks, and evaluation metrics. This variation could be due to several factors:

1. **Data Characteristics**: The dataset used for training the model (in this case, FEVER for fact verification) might have unique characteristics that are particularly challenging for one model over another. CORA-G may have been trained on a broader range of tasks or datasets, but these specific characteristics might make it less effective at certain tasks like factual verification.

2. **Model Architecture and Training**: The architecture of the models and how they were trained can affect their performance. Models may be optimized for different types of tasksâ€”some are better at generative tasks while others specialize in understanding and answering questions accurately.

3. **Evaluation Metrics**: Different datasets use varying metrics to evaluate model performance, which can lead to differences in reported results even when the underlying technology is similar.

4. **Task-Specific Limitations**: It's possible that FEVER, as a fact verification task, imposes unique limitations on models like CORA-G and Atlas-11B. This could indicate that there are inherent challenges in verifying facts or detecting misinformation specific to this domain that general-purpose language models struggle with.

5. **Fine-Tuning Strategies**: The way models are fine-tuned for the FEVER dataset might also play a role. Some models may have better fine-tuning strategies, leading to improved performance on specific fact verification tasks.

6. **Evaluation Robustness and Contextual Issues**: Sometimes evaluations of AI systems can be subjective or sensitive to contextual issues that aren't fully addressed by standard metrics. Fact verification is inherently context-dependent, which might highlight some limitations in the current evaluation methods for such tasks.

Given these points, it suggests that there could indeed be task-specific limitations when verifying facts and detecting misinformation using fact-checking datasets like FEVER. This could imply a need for more specialized models or techniques tailored to this particular type of fact verification work.

In summary, while both CORA-G and Atlas-11B show superior performance in other tasks, the results on specific fact verification datasets might point towards limitations related to how well these models handle the nuances of factual accuracy detection.