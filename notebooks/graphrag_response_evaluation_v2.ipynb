{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import openai\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from questions.apple import quiz_questions as apple_questions\n",
    "from questions.cs2 import quiz_questions as cs2_questions\n",
    "\n",
    "apple_mapper = {v['question']: int(k) for k,v in apple_questions.items()}\n",
    "cs2_mapper = {v['question']: int(k) + len(apple_mapper) + 1 for k,v in cs2_questions.items()}\n",
    "\n",
    "# Combine them\n",
    "q_mapper = {**apple_mapper, **cs2_mapper}\n",
    "q_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/results_0902025.csv')\n",
    "df['qid'] = df['question'].map(q_mapper)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = df['qid'].unique()\n",
    "n_models = df['model'].unique()\n",
    "n_access = df['access'].unique()\n",
    "n_users = df['user'].unique()\n",
    "\n",
    "all_dfs = []\n",
    "for id in n_ids:\n",
    "    for access in n_access:\n",
    "        for user in n_users:\n",
    "            try:\n",
    "                filtered_df = df.loc[\n",
    "                    (df['qid'].isin([id])) &\n",
    "                    # (df['model'].isin([model])) &\n",
    "                    (df['access'].isin([access])) &\n",
    "                    (df['user'].isin([user]))\n",
    "                ].copy()\n",
    "\n",
    "                # Create Prompt for Evaluation\n",
    "                prompt = f\"\"\"You are an expert evaluator of LLM responses. Please analyze and rank the following responses to the same question.\n",
    "                Question: {filtered_df['question'].values[0]}\n",
    "                Correct Answer: {filtered_df['answer'].values[0]}\n",
    "                Reference for Correct Answer in Original Text: {filtered_df['reference'].values[0]}\n",
    "\n",
    "                Responses to evaluate:\n",
    "                {'-' * 50}\n",
    "                \"\"\"\n",
    "\n",
    "                for idx, row in filtered_df.iterrows():\n",
    "                    prompt += f\"\\n{row['model']}:\\n{row['response']}\\n{'-' * 50}\\n\"\n",
    "\n",
    "                prompt += \"\"\"\\nPlease evaluate each response based on:\n",
    "                1. Accuracy and factual correctness\n",
    "                2. Completeness of the answer\n",
    "                3. Clarity and coherence\n",
    "                4. Relevance to the question\n",
    "                5. Proper use of available context\n",
    "\n",
    "                Provide a scoring from best to worst, with scores (0-10) and detailed explanations.\n",
    "                Format your response in a structured way that can be parsed into the following JSON schema:\n",
    "                {\n",
    "                    \"evaluations\": [\n",
    "                        {\n",
    "                            \"model_name\": \"model name\",\n",
    "                            \"score\": score (0-10),\n",
    "                            \"reasoning\": \"detailed explanation\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"meta_analysis\": \"overall analysis of patterns and differences between responses\"\n",
    "                }\n",
    "\n",
    "                Check the response json format before returning it.\n",
    "                \"\"\"\n",
    "\n",
    "                # Get evaluation from GPT-4\n",
    "                response = openai.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert evaluator of LLM responses.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    response_format={ \"type\": \"json_object\" }\n",
    "                )\n",
    "\n",
    "                # Parse response\n",
    "                try:\n",
    "                    eval_dict = json.loads(response.choices[0].message.content)\n",
    "                    eval_df = pd.DataFrame(eval_dict['evaluations'])\n",
    "\n",
    "                    # Create mappings from eval_df using model names as keys\n",
    "                    score_mapping = dict(zip(eval_df['model_name'], eval_df['score']))\n",
    "                    reasoning_mapping = dict(zip(eval_df['model_name'], eval_df['reasoning']))\n",
    "\n",
    "                    # Map values to filtered_df with default values for missing models\n",
    "                    filtered_df['score'] = filtered_df['model'].map(score_mapping).fillna(-1)\n",
    "                    filtered_df['reasoning'] = filtered_df['model'].map(reasoning_mapping).fillna('Model evaluation failed')\n",
    "                    filtered_df['meta_analysis'] = eval_dict['meta_analysis']\n",
    "\n",
    "                    \n",
    "                    # filtered_df['score'] = eval_df['score'].values\n",
    "                    # filtered_df['reasoning'] = eval_df['reasoning'].values\n",
    "                    # filtered_df['meta_analysis'] = eval_dict['meta_analysis']\n",
    "\n",
    "                    all_dfs.append(filtered_df)\n",
    "                    # evaluations.append(eval_dict)\n",
    "                    # print(eval_dict)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error for {id} {access} {user}\")\n",
    "                    print(f\"Error parsing JSON: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error for {id} {access} {user}\")\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "            # break\n",
    "    #     break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all at once after the loop\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# final_df.to_csv('./results/results_0902025_with_scores.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval with Opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('./results/results_0902025_with_scores.csv')\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.evaluation.metrics import ContextPrecision, ContextRecall, Usefulness, AnswerRelevance, Hallucination, LevenshteinRatio\n",
    "import opik\n",
    "\n",
    "opik.configure(use_local=False)\n",
    "\n",
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate various metrics for each row in the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame containing columns 'question', 'response', 'reference', 'answer'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional metric columns\n",
    "    \"\"\"\n",
    "    # Create copy to avoid modifying original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Initialize metric columns\n",
    "    metric_columns = ['levenshtein_ratio', 'levenshtein_ratio_reason',\n",
    "                     'hallucination_score', 'hallucination_score_reason', \n",
    "                     'answer_relevance', 'answer_relevance_reason',\n",
    "                     'usefulness', 'usefulness_reason',\n",
    "                     'context_precision', 'context_precision_reason',\n",
    "                     'context_recall', 'context_recall_reason']\n",
    "    for col in metric_columns:\n",
    "        if col.split('_')[-1] == 'reason':\n",
    "            result_df[col] = pd.Series(dtype='object', data = 'N/A')\n",
    "        else:\n",
    "            result_df[col] = pd.Series(dtype='float64', data = -1.0)\n",
    "        \n",
    "    # Calculate metrics for each row\n",
    "    for idx, row in tqdm(result_df.iterrows(), total=len(result_df)):\n",
    "        try:\n",
    "            # LevenshteinRatio\n",
    "            metric = LevenshteinRatio()\n",
    "            score = metric.score(output=row['response'], reference=row['reference'])\n",
    "            result_df.at[idx, 'levenshtein_ratio'] = score\n",
    "            result_df.at[idx, 'levenshtein_ratio_reason'] = score.reason\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            # Hallucination\n",
    "            metric = Hallucination(model=\"gpt-4o-mini\")\n",
    "            score = metric.score(\n",
    "                input=row['question'],\n",
    "                output=row['response'],\n",
    "                context=[row['reference']]\n",
    "            )\n",
    "            result_df.at[idx, 'hallucination_score'] = score.value\n",
    "            result_df.at[idx, 'hallucination_score_reason'] = score.reason\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            # AnswerRelevance\n",
    "            metric = AnswerRelevance(model=\"gpt-4o-mini\")\n",
    "            score = metric.score(\n",
    "                input=row['question'],\n",
    "                output=row['response'],\n",
    "                context=[row['reference']]\n",
    "            )\n",
    "            result_df.at[idx, 'answer_relevance'] = score.value\n",
    "            result_df.at[idx, 'answer_relevance_reason'] = score.reason\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            # Usefulness\n",
    "            metric = Usefulness(model=\"gpt-4o-mini\")\n",
    "            score = metric.score(\n",
    "                input=row['question'],\n",
    "                output=row['response']\n",
    "            )\n",
    "            result_df.at[idx, 'usefulness'] = score.value\n",
    "            result_df.at[idx, 'usefulness_reason'] = score.reason\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            # ContextPrecision\n",
    "            metric = ContextPrecision(model=\"gpt-4o-mini\")\n",
    "            score = metric.score(\n",
    "                input=row['question'],\n",
    "                output=row['response'],\n",
    "                expected_output=row['answer'],\n",
    "                context=[row['reference']]\n",
    "            )\n",
    "            result_df.at[idx, 'context_precision'] = score.value\n",
    "            result_df.at[idx, 'context_precision_reason'] = score.reason\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            # ContextRecall\n",
    "            metric = ContextRecall(model=\"gpt-4o-mini\")\n",
    "            score = metric.score(\n",
    "                input=row['question'],\n",
    "                output=row['response'],\n",
    "                expected_output=row['answer'],\n",
    "                context=[row['reference']]\n",
    "            )\n",
    "            result_df.at[idx, 'context_recall'] = score.value\n",
    "            result_df.at[idx, 'context_recall_reason'] = score.reason\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "final_df_opik = calculate_metrics(final_df)\n",
    "# final_df_opik.to_csv('./results/results_0902025_with_scores_and_metrics.csv', index = False)\n",
    "final_df_opik.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_opik.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import ollama\n",
    "list(ollama.list())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_old = pd.read_csv('./results/results_with_scores_and_metrics.csv')\n",
    "# # results_old = results_old[results_old['access'] != 'DOCUMENT_LEVEL'].copy()\n",
    "# results_new = pd.read_csv('./results/results_0902025_with_scores_and_metrics.csv')\n",
    "\n",
    "# # First keep the filtered results_old\n",
    "# results_old = results_old[results_old['access'] != 'DOCUMENT_LEVEL'].copy()\n",
    "\n",
    "# # Filter results_new for DOCUMENT_ONLY entries\n",
    "# document_only_new = results_new[results_new['access'] == 'DOCUMENT_LEVEL'].copy()\n",
    "\n",
    "# # Combine the dataframes\n",
    "# combined_df = pd.concat([results_old, document_only_new], ignore_index=True)\n",
    "\n",
    "# # Verify the combination\n",
    "# print(\"Access types in combined dataset:\", combined_df['access'].unique())\n",
    "# print(\"\\nNumber of entries in results_old:\", len(results_old))\n",
    "# print(\"Number of DOCUMENT_ONLY entries added:\", len(document_only_new))\n",
    "# print(\"Total entries in combined dataset:\", len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create complete model mapping dictionary\n",
    "# model_mapping = {\n",
    "#     # Direct matches (keep results_new version)\n",
    "#     'deepseek-r1:1.5b': 'deepseek-r1:1.5b',\n",
    "#     'mistral-small:24b': 'mistral-small:24b',\n",
    "#     'llama3.2:1b': 'llama3.2:1b',\n",
    "#     'qwen2.5:7b': 'qwen2.5:7b',\n",
    "#     'qwen2.5:3b': 'qwen2.5:3b',\n",
    "#     'qwen2.5:1.5b': 'qwen2.5:1.5b',\n",
    "#     'qwen2.5:0.5b': 'qwen2.5:0.5b',\n",
    "#     'qwen2.5:32b': 'qwen2.5:32b',\n",
    "#     'qwen2.5:14b': 'qwen2.5:14b',\n",
    "#     'phi3.5:3.8b': 'phi3.5:3.8b',\n",
    "#     'phi4:14b': 'phi4:14b',\n",
    "    \n",
    "#     # Similar model mappings (old -> new)\n",
    "#     'deepseek-r1:32b': 'deepseek-r1:8b',  # Map to closest available size\n",
    "#     'llama3.2:latest': 'llama3.2:3b',     # Map latest to specific version\n",
    "#     'phi3.5:latest': 'phi3.5:3.8b',       # Map latest to specific version\n",
    "#     'phi4:latest': 'phi4:14b',            # Map latest to specific version\n",
    "#     'smollm:135m': 'smollm2:135m',        # Map to v2 versions\n",
    "#     'smollm:360m': 'smollm2:360m',\n",
    "#     'smollm:1.7b': 'smollm2:1.7b'\n",
    "# }\n",
    "\n",
    "# # Print unique models before mapping\n",
    "# print(\"Unique models before mapping:\", combined_df['model'].unique())\n",
    "\n",
    "# # Map models and handle unmapped cases\n",
    "# def map_model(model):\n",
    "#     if pd.isna(model):\n",
    "#         return model\n",
    "#     return model_mapping.get(model, model)  # Return original if no mapping exists\n",
    "\n",
    "# combined_df['model'] = combined_df['model'].apply(map_model)\n",
    "\n",
    "# # Save combined dataset\n",
    "# combined_df.to_csv('./results/combined_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access_model_df = combined_df[combined_df['access'] == 'DOCUMENT_LEVEL'].groupby(['model', 'user'])[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean().reset_index()\n",
    "# access_model_df.sort_values(by='response_time', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate means for each user and model combination\n",
    "# user1_df = access_model_df[access_model_df['user'] == 'user1'].groupby('model')[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean()\n",
    "# admin_df = access_model_df[access_model_df['user'] == 'admin'].groupby('model')[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean()\n",
    "\n",
    "# # Calculate percentage difference\n",
    "# # Formula: ((user1 - admin) / admin) * 100\n",
    "# percent_diff = ((admin_df - user1_df) / user1_df * 100).round(2)\n",
    "\n",
    "# # Add a column for average difference across all metrics (excluding response_time)\n",
    "# metric_cols = ['score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']\n",
    "# percent_diff['avg_difference'] = percent_diff[metric_cols].mean(axis=1).round(2)\n",
    "\n",
    "# # Sort by average difference\n",
    "# percent_diff_sorted = percent_diff.sort_values('avg_difference', ascending=False)\n",
    "\n",
    "# print(\"Percentage difference (user1 compared to admin):\")\n",
    "# print(\"\\nPositive values mean user1's values are higher\")\n",
    "# print(\"Negative values mean admin's values are higher\")\n",
    "# # print(\"\\n\", percent_diff_sorted)\n",
    "\n",
    "# # Optional: Save to CSV\n",
    "# percent_diff_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_df = combined_df.groupby(['model'])[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean().reset_index() #.to_csv('./results/model_metrics.csv', index = False)\n",
    "# metric_df.sort_values(by='context_recall', ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def config_seaborn():\n",
    "    palette = [\"#fa00e1\", \"#0019fa\", \"#00fa15\", \"#00faee\", \"#faaf00\", \"#9200fa\", \"#edfa00\", \"#c805e6\", \"#3214f5\", \"#0046cc\", \"#00cd43\", \"#49d7b8\", \"#03e5b3\", \"#32ebbe\", \"#c8be30\", \"#e58c32\", \"#a723c8\"]\n",
    "    sns_palette = sns.color_palette(palette, len(palette))\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.set_context('talk')\n",
    "\n",
    "    return sns_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = config_seaborn()\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.barplot(\n",
    "    data=access_model_df,\n",
    "    x='model',\n",
    "    y='score',\n",
    "    hue='user',\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "# plt.title('Statistical Measures Across Embedding Models', pad=20, size=14)\n",
    "plt.xlabel('LLM Model', size=25)\n",
    "plt.ylabel('Score (0-10)', size=25)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=15, fontsize=22)\n",
    "plt.yticks(fontsize=22)\n",
    "\n",
    "# Adjust legend\n",
    "plt.legend(\n",
    "    title='Input Type',\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    loc='center',\n",
    "    ncol=4,\n",
    "    frameon=True,\n",
    "    fontsize=24,\n",
    "    title_fontsize=28\n",
    ")\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('./plots/model_performance.png', dpi=300)\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = config_seaborn()\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(25, 13))\n",
    "\n",
    "# Create grouped bar plot\n",
    "\n",
    "models_to_drop = ['qwen2.5:32b', 'qwen2.5:14b']\n",
    "_df = combined_df[~combined_df['model'].isin(models_to_drop)].copy()\n",
    "\n",
    "# Create new access types based on user and access combination\n",
    "def modify_access(row):\n",
    "    if row['access'] == 'DOCUMENT_LEVEL':\n",
    "        return f'DOCUMENT_LEVEL_{row[\"user\"].upper()}'\n",
    "    return row['access']\n",
    "\n",
    "# Apply the modification to the DataFrame\n",
    "_df['access'] = _df.apply(modify_access, axis=1)\n",
    "\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=_df,\n",
    "    x='model',\n",
    "    y='score',\n",
    "    hue='access',\n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "# plt.title('Statistical Measures Across Embedding Models', pad=20, size=14)\n",
    "plt.xlabel('LLM Model', size=25)\n",
    "plt.ylabel('Score (0-10)', size=25)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, fontsize=28)\n",
    "plt.yticks(fontsize=28)\n",
    "\n",
    "# Adjust legend\n",
    "plt.legend(\n",
    "    title='Input Type',\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    loc='center',\n",
    "    ncol=5,\n",
    "    frameon=True,\n",
    "    fontsize=26,\n",
    "    title_fontsize=33\n",
    ")\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./plots/model_performance.png', dpi=300)\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.groupby('access')[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean().reset_index().sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df[_df['user'] == 'user1'].groupby('access')[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean().reset_index().sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = config_seaborn()\n",
    "\n",
    "scatter_df = final_df_opik.groupby(['model', 'access', 'user'])[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean().reset_index()\n",
    "def min_max_scaling(column):\n",
    "    min_val = column.min()\n",
    "    max_val = column.max()\n",
    "    return (column - min_val) / (max_val - min_val)\n",
    "\n",
    "# Apply to a single column\n",
    "scatter_df['normalized_response_time'] = min_max_scaling(scatter_df['response_time'])\n",
    "# Create figure\n",
    "plt.figure(figsize=(33, 12))\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.scatterplot(\n",
    "    x=\"answer_relevance\", \n",
    "    y=\"usefulness\",\n",
    "    hue=\"model\", \n",
    "    size=\"response_time\",\n",
    "    palette=palette,\n",
    "    sizes=(100, 600), \n",
    "    linewidth=0,\n",
    "    data=scatter_df, \n",
    ")\n",
    "\n",
    "# Customize plot\n",
    "# plt.title('Statistical Measures Across Embedding Models', pad=20, size=14)\n",
    "plt.xlabel('Answer Relevance', size=38)\n",
    "plt.ylabel('Usefullness', size=38)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "legend = plt.legend(\n",
    "    ncol=5,\n",
    "    fontsize=32,\n",
    ")\n",
    "legend.get_texts()[0].set_text('MODEL')\n",
    "legend.get_texts()[17].set_text('RESPONSE TIME (s)')\n",
    "\n",
    "plt.tick_params(axis='both', labelsize=32)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./plots/model_relevance_usefulness_performance.png', dpi=300)\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "palette = config_seaborn()\n",
    "\n",
    "scatter_df = final_df_opik.groupby(['model', 'access', 'user'])[['response_time', 'score', 'hallucination_score', 'answer_relevance', 'usefulness', 'context_precision', 'context_recall']].mean().reset_index()\n",
    "# def min_max_scaling(column):\n",
    "#     min_val = column.min()\n",
    "#     max_val = column.max()\n",
    "#     return (column - min_val) / (max_val - min_val)\n",
    "\n",
    "# Apply to a single column\n",
    "# scatter_df['normalized_response_time'] = min_max_scaling(scatter_df['response_time'])\n",
    "# Create figure\n",
    "plt.figure(figsize=(30,6))\n",
    "\n",
    "# Create grouped bar plot\n",
    "ax = sns.scatterplot(\n",
    "    x=\"context_precision\", \n",
    "    y=\"context_recall\",\n",
    "    hue=\"model\", \n",
    "    size=\"response_time\",\n",
    "    palette=palette,\n",
    "    sizes=(100, 600), \n",
    "    linewidth=0,\n",
    "    data=scatter_df, \n",
    ")\n",
    "# rect = Rectangle((7, 0.5), 3, 0.5, color='gray', alpha=0.5, transform=ax.transData)\n",
    "# ax.add_patch(rect)\n",
    "# ax.text(8.5, 0.75, 'better performance', color='black', fontsize=10, ha='center', va='center')\n",
    "\n",
    "# Customize plot\n",
    "# plt.title('Statistical Measures Across Embedding Models', pad=20, size=14)\n",
    "plt.xlabel('Precision', size=38)\n",
    "plt.ylabel('Recall', size=38)\n",
    "plt.legend('', frameon=False)\n",
    "plt.tick_params(axis='both', labelsize=32)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('./plots/model_precision_recall_performance.png', dpi=300)\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
