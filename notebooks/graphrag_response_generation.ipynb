{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
    "from graphrag.query.llm.oai.embedding import OpenAIEmbedding\n",
    "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
    "from graphrag.query.question_gen.local_gen import LocalQuestionGen\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../benchmark/output/\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
    "ENTITY_TABLE = \"create_final_nodes\"\n",
    "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
    "RELATIONSHIP_TABLE = \"create_final_relationships\"\n",
    "COVARIATE_TABLE = \"create_final_covariates\"\n",
    "TEXT_UNIT_TABLE = \"create_final_text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
    "\n",
    "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)\n",
    "\n",
    "# load description embeddings to an in-memory lancedb vectorstore\n",
    "# to connect to a remote db, specify url and port values.\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "print(f\"Entity count: {len(entity_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)\n",
    "\n",
    "print(f\"Relationship count: {len(relationship_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
    "\n",
    "print(f\"Report records: {len(report_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_unit_df = text_unit_df.dropna(how='any')\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "print(f\"Text unit records: {len(text_unit_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = os.environ[\"GRAPHRAG_API_KEY\"]\n",
    "llm_model = \"gpt-4o-mini\" # os.environ[\"GRAPHRAG_LLM_MODEL\"]\n",
    "embedding_model = 'text-embedding-3-small' # os.environ[\"GRAPHRAG_EMBEDDING_MODEL\"]\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    # api_key=api_key,\n",
    "    model=llm_model,\n",
    "    api_type=OpenaiApiType.OpenAI,  # OpenaiApiType.OpenAI or OpenaiApiType.AzureOpenAI\n",
    "    max_retries=20,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "text_embedder = OpenAIEmbedding(\n",
    "    # api_key=api_key,\n",
    "    api_base=None,\n",
    "    api_type=OpenaiApiType.OpenAI,\n",
    "    model=embedding_model,\n",
    "    deployment_name=embedding_model,\n",
    "    max_retries=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    # if you did not run covariates during indexing, set this to None\n",
    "    covariates=None,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,\n",
    "    \"top_k_relationships\": 10,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
    "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "}\n",
    "\n",
    "llm_params = {\n",
    "    \"max_tokens\": 2_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500)\n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine = LocalSearch(\n",
    "    llm=llm,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    llm_params=llm_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_document_mapping = {}\n",
    "chunks_document_mapping_r = {}\n",
    "entity_document_mapping = {}\n",
    "entity_document_mapping_r = {}\n",
    "relationship_document_mapping = {}\n",
    "relationship_document_mapping_r = {}\n",
    "reports_documents_mapping = {}\n",
    "\n",
    "entities_df = pd.read_parquet('../benchmark/output/create_final_entities.parquet')\n",
    "entity_mapper = {}\n",
    "for idx, row in entity_df.iterrows():\n",
    "    entity_mapper[row['id']] = row['human_readable_id']\n",
    "\n",
    "relationship_mapper = {}\n",
    "for idx, row in relationship_df.iterrows():\n",
    "    relationship_mapper[row['id']] = row['human_readable_id']\n",
    "\n",
    "\n",
    "for idx, row in text_unit_df.iterrows():\n",
    "    chunks_document_mapping[row['human_readable_id']] = row['document_ids'][0]\n",
    "    for e in row['entity_ids']:\n",
    "        entity_document_mapping[entity_mapper[e]] = row['document_ids'][0]\n",
    "    for r in row['relationship_ids']:\n",
    "        relationship_document_mapping[relationship_mapper[r]] = row['document_ids'][0]\n",
    "\n",
    "# for idx, row in report_df.iterrows():\n",
    "#     reports_documents_mapping[row['id']] = row[]\n",
    "\n",
    "print(chunks_document_mapping[1])\n",
    "print(entity_document_mapping[641])\n",
    "print(relationship_document_mapping[638])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('../benchmark/results-benchmark.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create id to title mapping\n",
    "id_to_title = {item['id']: item['title'] for item in data}\n",
    "id_to_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Responses\n",
    "\n",
    "## Set Access Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_access_mapping():\n",
    "    \"\"\"Create a mock document access mapping with fictional user and document IDs.\"\"\"\n",
    "    return {\n",
    "        \"2a5249a0a6455998a6380194c2e0396894f20be186335de35b3a05dd1ee3aa0ffccb7acaf5cae36971bf8e41403e369e5cd8727aaceb6d6739c0a809fe513cf6\": [\"user1\", \"admin\"],\n",
    "        \"60dd3abebec49fd661cf23fa4abbe211d9cdac2d404e1d3c1c6033abdac768f7bce7f45c8ddca2fcb1a5f30328d32329ba3f945fb5b64c01db8b8f98e7def4a6\": [\"user1\", \"admin\"],\n",
    "        \"8b197120432108f3a0b055d6623358e7472338fcacbcf2116628b36091940f4880948496d9554442010bc02f25250af299e30399b9eaa857db5b552e215ca48b\": [\"admin\"],\n",
    "        \"913c8b875eac2a1e19c34de9f136ec6d79577f71af2a37a7094e0d6312833c3a48904ac32ab9d79d6b68aa9cd281ecd5b2ab0c3b03beaef417db7f50d8854be2\": [\"user1\", \"admin\"],\n",
    "        \"9685dd41c9837a9f1b5561eaa29359a3e911d202232ed51665ddcd062d7900928e0705d17f5f932ea81fd89d5251c29984038fcf61598dd25e2a0635f21e6415\": [\"user1\", \"admin\"],\n",
    "        \"b4bfb7c4d56b91f3c1901805f5c3ff3b9d80a26d20023b3614af41bde57839e2db37e303d51946bf91738bdfee19014ed55eef91bf079b941dc3d791eb20a392\": [\"user1\", \"admin\"],\n",
    "        \"bec5c29288289e3d4be72eaf2d10228c199a545cb363344bb35c4e8f0024a3243e4f8abdf196ed3288832e72a93cc9e042a53eb35c862c123681f0239ecbb0b4\": [\"user1\", \"admin\"],\n",
    "        \"c6b30efb0aededdf34730d9958cf585ac6c304fb5290881d28588b4f26b9197fd0da9877829af4a11386f23eb79ba3e4afb2aa5053ccfc8261579267ddfe5a4f\": [\"admin\"],\n",
    "        \"e8429b1a8c6a7f5151f887658dea4e957abb4366e169e55bd9b4c1ead6f37fc6eb9f9aca80d8cf5764c4fad3f745644edc5fd5a01d7bbcdb32f69dcaa3ca55f2\": [\"user1\", \"admin\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Filter accoridng to access control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def filter_response_by_access(response_dict, access_level, user_id=None):\n",
    "    \"\"\"\n",
    "    Filter the response dictionary based on access level and user permissions.\n",
    "    \n",
    "    Args:\n",
    "        response_dict (dict): Original response containing all information\n",
    "        access_level (str): One of 'KG_ONLY', 'CHUNKS', 'FULL', 'DOCUMENT_LEVEL'\n",
    "        user_id (str): Required for DOCUMENT_LEVEL access checking\n",
    "    \n",
    "    Returns:\n",
    "        dict: Filtered response based on access level\n",
    "    \"\"\"\n",
    "    if access_level == \"KG_ONLY\":\n",
    "        return {\n",
    "            \"relationships\": response_dict[\"relationships\"],\n",
    "            \"entities\": response_dict[\"entities\"]\n",
    "        }\n",
    "    \n",
    "    elif access_level == \"CHUNKS\":\n",
    "        return {\n",
    "            \"relationships\": response_dict[\"relationships\"],\n",
    "            \"entities\": response_dict[\"entities\"],\n",
    "            \"sources\": response_dict[\"sources\"]\n",
    "        }\n",
    "    \n",
    "    elif access_level == \"FULL\":\n",
    "        return response_dict\n",
    "    \n",
    "    elif access_level == \"DOCUMENT_LEVEL\":\n",
    "        if not user_id:\n",
    "            raise ValueError(\"user_id is required for DOCUMENT_LEVEL access\")\n",
    "        \n",
    "        doc_access = create_document_access_mapping()\n",
    "        accessible_docs = {doc_id for doc_id, users in doc_access.items() \n",
    "                         if user_id in users}\n",
    "        # print(accessible_docs)\n",
    "        \n",
    "        filtered_response = {}\n",
    "        \n",
    "        # Filter entities\n",
    "        if \"entities\" in response_dict:\n",
    "            filtered_response[\"entities\"] = [\n",
    "                entity for _, entity in response_dict[\"entities\"].iterrows()\n",
    "                if entity_document_mapping.get(int(entity[\"id\"])) in accessible_docs\n",
    "            ]\n",
    "        \n",
    "        # Filter relationships\n",
    "        if \"relationships\" in response_dict:\n",
    "            filtered_response[\"relationships\"] = [\n",
    "                rel for _, rel in response_dict[\"relationships\"].iterrows()\n",
    "                if relationship_document_mapping.get(int(rel[\"id\"])) in accessible_docs\n",
    "            ]\n",
    "        \n",
    "        # Filter sources\n",
    "        if \"sources\" in response_dict:\n",
    "            filtered_response[\"sources\"] = [\n",
    "                source for _, source in response_dict[\"sources\"].iterrows()\n",
    "                if chunks_document_mapping.get(int(source[\"id\"])) in accessible_docs\n",
    "            ]\n",
    "        \n",
    "        # Include reports if present and user has access\n",
    "        if \"reports\" in response_dict:\n",
    "            reports_ = []\n",
    "            ids_ = response_dict[\"reports\"]['id'].unique()\n",
    "            for id in ids_:\n",
    "                docs_ = list(set([entity_document_mapping[x] for x in entity_df[entity_df['community'] == int(id)]['human_readable_id'].values]))\n",
    "                # print(docs_)\n",
    "                # print(accessible_docs)\n",
    "                # include = True\n",
    "                for doc in docs_:\n",
    "                    # print(doc in accessible_docs)\n",
    "                    if doc in accessible_docs:\n",
    "                        report_string = ''\n",
    "                        # for row in response_dict['reports'].loc[response_dict['reports']['id'] == str(id)].values:\n",
    "                        report_string += response_dict['reports'].loc[response_dict['reports']['id'] == str(id)]['title'].values\n",
    "                        report_string += response_dict['reports'].loc[response_dict['reports']['id'] == str(id)]['content'].values\n",
    "                        # print(f\"Not allowed {doc}\")\n",
    "                        # include = False\n",
    "                \n",
    "                # if include:\n",
    "                #     for row in response_dict['reports'].values:\n",
    "                #         report_string += row[2]\n",
    "\n",
    "                    # report_string = f\"{response_dict['reports'].loc[response_dict['reports']['id'] == str(id)]['title']} - {response_dict['reports'].loc[response_dict['reports']['id'] == str(id)]['content']}\"\n",
    "                        reports_.append(report_string)\n",
    "\n",
    "            filtered_response['reports'] = reports_\n",
    "                    \n",
    "            # filtered_response['reports'] = []\n",
    "            # for e in entities_:\n",
    "            #     doc = entity_document_mapping[entity_df[entity_df['community'] == e]['human_readable_id'].values[0]]\n",
    "            #     if doc not in accessible_docs:\n",
    "            #         include = False\n",
    "            #     filtered_response['reports'].append()\n",
    "            #     filtered_response['reports'] = [\n",
    "            #         source for _, source in response_dict[\"reports\"].iterrows()\n",
    "            #         if chunks_document_mapping.get(int(source[\"id\"])) in accessible_docs\n",
    "            #     ]\n",
    "            #     # filtered_response[\"reports\"] = response_dict[\"reports\"]\n",
    "        \n",
    "        return filtered_response\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid access level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Control Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from questions.apple import quiz_questions as apple_questions\n",
    "from questions.cs2 import quiz_questions as cs2_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_questions['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_engine.search(apple_questions['1']['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example calls:\n",
    "kg_only_result = filter_response_by_access(result.context_data, \"KG_ONLY\")\n",
    "chunks_result = filter_response_by_access(result.context_data, \"CHUNKS\")\n",
    "full_result = filter_response_by_access(result.context_data, \"FULL\")\n",
    "print(result.context_data.keys())\n",
    "print()\n",
    "\n",
    "for u in [\"user1\", \"admin\"]:\n",
    "    document_level_result = filter_response_by_access(result.context_data, \"DOCUMENT_LEVEL\", user_id=u)\n",
    "    print(f\"User {u}:\")\n",
    "    print(len(document_level_result['entities']))\n",
    "    print(len(document_level_result['relationships']))\n",
    "    print(len(document_level_result['sources']))\n",
    "    print()\n",
    "    print('#'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_level_result = filter_response_by_access(result.context_data, \"DOCUMENT_LEVEL\", user_id='user1')\n",
    "document_level_result['entities'][:2]\n",
    "\n",
    "for e in document_level_result['entities'][:2]:\n",
    "    print(f\"Entity: {e['id']} belongs to document: {id_to_title[entity_document_mapping.get(int(e['id']))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ollama\n",
    "\n",
    "def generate_search_prompt(context_data: str, response_type: str = \"multiple paragraphs\") -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt using the local search system prompt template\n",
    "    \n",
    "    Args:\n",
    "        context_data: The context data to include in the prompt\n",
    "        response_type: The desired response format/length\n",
    "    \n",
    "    Returns:\n",
    "        str: The formatted prompt\n",
    "    \"\"\"\n",
    "    # Read template file\n",
    "    template_path = Path(\"../benchmark/prompts/local_search_system_prompt.txt\")\n",
    "    with open(template_path, \"r\") as f:\n",
    "        template = f.read()\n",
    "    \n",
    "    # Replace variables\n",
    "    prompt = template.replace(\"{response_type}\", response_type)\n",
    "    # content_str = context_data['reports']\n",
    "    prompt = prompt.replace(\"{context_data}\", context_data)\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def create_llm_response(model, prompt, question):\n",
    "    # Use Ollama to test the prompt\n",
    "    response = ollama.chat(\n",
    "        model=model,  # or your preferred model\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt\n",
    "        }, {\n",
    "            'role': 'user',\n",
    "            'content': question\n",
    "        }],\n",
    "        options={'timeout': 1000}\n",
    "    )\n",
    "    return response # response['message']['content']\n",
    "\n",
    "def save_llm_response(model: str, access: str, question_num: int, response: str):\n",
    "    \"\"\"Save LLM response to a file in the specified folder structure.\"\"\"\n",
    "    # Create base directory if it doesn't exist\n",
    "    base_dir = \"../benchmark/llm_output\"\n",
    "    model_dir = f\"{base_dir}/{model.replace(':', '_')}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Create filename with access type and question number\n",
    "    filename = f\"{access}_{question_num}_2.txt\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    \n",
    "    # Save response to file\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = dict(dict(ollama.list())['models'][7])['model']\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_level_result = filter_response_by_access(result.context_data, \"DOCUMENT_LEVEL\", user_id='admin')\n",
    "# Generate prompt\n",
    "prompt = generate_search_prompt(\n",
    "    context_data=str(document_level_result['reports']),\n",
    "    response_type=\"multiple paragraphs\"\n",
    ")\n",
    "# response = create_llm_response(model_1, prompt, apple_questions['1']['question'])\n",
    "# print(prompt)\n",
    "response = create_llm_response(model_1, prompt, apple_questions['1']['question'])\n",
    "# print()\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time taken: {round(response['total_duration'] / 1000000000,2)} seconds\")\n",
    "print(response['message']['content'])\n",
    "print(response['eval_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_models = ollama.list()\n",
    "ollama_models = [dict(x)['model'] for x in dict(ollama.list())['models']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = {\n",
    "    str(i+1): value \n",
    "    for i, value in enumerate(list(apple_questions.values()) + list(cs2_questions.values()))\n",
    "}\n",
    "merged_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected structure\n",
    "expected_keys = {'question', 'answer', 'reference'}\n",
    "\n",
    "# Function to check and fix dictionary structure\n",
    "def validate_and_fix_dict(input_dict):\n",
    "    fixed_dict = {}\n",
    "    \n",
    "    for key, value in input_dict.items():\n",
    "        # Create a new entry for this item\n",
    "        fixed_entry = {}\n",
    "        \n",
    "        # Check if any key needs to be fixed\n",
    "        for k, v in value.items():\n",
    "            # If 'ion' is found, change it to 'question'\n",
    "            if k == 'ion':\n",
    "                fixed_entry['question'] = v\n",
    "            else:\n",
    "                fixed_entry[k] = v\n",
    "        \n",
    "        # Check if all expected keys are present\n",
    "        for expected_key in expected_keys:\n",
    "            if expected_key not in fixed_entry:\n",
    "                print(f\"Warning: Missing key '{expected_key}' in entry {key}\")\n",
    "                fixed_entry[expected_key] = ''  # Add empty string for missing keys\n",
    "        \n",
    "        fixed_dict[int(key)] = fixed_entry\n",
    "    \n",
    "    return fixed_dict\n",
    "\n",
    "# Use the function\n",
    "fixed_dict = validate_and_fix_dict(merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_responses = {}\n",
    "\n",
    "for idx, q in fixed_dict.items():\n",
    "    kg_responses[int(idx)] = search_engine.search(q['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ollama_models) * 4*2*len(fixed_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    \n",
    "    # Set the signal handler and a timeout\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Disable the alarm\n",
    "        signal.alarm(0)\n",
    "\n",
    "n_model = []\n",
    "n_access = []\n",
    "n_user = []\n",
    "n_question = []\n",
    "n_response = []\n",
    "n_answer = []\n",
    "n_reference = []\n",
    "n_response_time = []\n",
    "n_reponse_length = []\n",
    "n_response_tokens = []\n",
    "\n",
    "counter = 0\n",
    "# Update the loop to save responses\n",
    "for m in ollama_models:\n",
    "    for access in ['DOCUMENT_LEVEL']: # [\"KG_ONLY\", \"CHUNKS\", \"FULL\", \"DOCUMENT_LEVEL\"]:\n",
    "        for u in [\"user1\", \"admin\"]:\n",
    "            for idx, q in fixed_dict.items():\n",
    "                try:\n",
    "                    with timeout(60):  # Set 60 second timeout\n",
    "                        # Query KG\n",
    "                        document_level_result = filter_response_by_access(kg_responses[idx].context_data, access, user_id=u)\n",
    "                        # Generate prompt\n",
    "                        prompt = generate_search_prompt(\n",
    "                            context_data=str(document_level_result['reports']),\n",
    "                            response_type=\"multiple paragraphs\"\n",
    "                        )\n",
    "                        response = ollama.chat(\n",
    "                            model=m,\n",
    "                            messages=[{\n",
    "                                'role': 'user',\n",
    "                                'content': prompt\n",
    "                            }, {\n",
    "                                'role': 'user',\n",
    "                                'content': q['question']\n",
    "                            }]\n",
    "                        )\n",
    "                        \n",
    "                        n_model.append(m)\n",
    "                        n_access.append(access)\n",
    "                        n_user.append(u)\n",
    "                        n_question.append(q['question'])\n",
    "                        n_response.append(response['message']['content'])\n",
    "                        n_answer.append(q['answer'])\n",
    "                        n_reference.append(q['reference'])\n",
    "                        n_response_time.append(round(response['total_duration'] / 1000000000,2))\n",
    "                        n_reponse_length.append(len(response['message']['content']))\n",
    "                        n_response_tokens.append(response['eval_count'])\n",
    "                        \n",
    "                        print(f\"{m} - {access} - {u} - {idx}\")\n",
    "\n",
    "                except TimeoutException:\n",
    "                    print(f\"Timeout occurred for {m} - {access} - {u} - {idx}\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if counter % 100 == 0:\n",
    "                    print(f\"Progress: {round(counter / 2240, 2)} %\")\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'model': n_model,\n",
    "    'access': n_access, \n",
    "    'user': n_user,\n",
    "    'question': n_question,\n",
    "    'response': n_response,\n",
    "    'answer': n_answer,\n",
    "    'reference': n_reference,\n",
    "    'response_time': n_response_time,\n",
    "    'response_length': n_reponse_length,\n",
    "    'response_tokens': n_response_tokens\n",
    "}).to_csv('./results/results_0902025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/results_0902025.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
